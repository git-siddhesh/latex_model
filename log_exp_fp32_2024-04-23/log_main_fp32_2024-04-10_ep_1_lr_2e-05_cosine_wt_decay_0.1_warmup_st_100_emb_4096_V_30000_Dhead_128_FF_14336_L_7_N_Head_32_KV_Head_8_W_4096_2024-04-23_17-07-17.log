2024-04-24 08:33:30,579 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
2024-04-24 08:33:30,790 urllib3.connectionpool DEBUG    https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0
2024-04-24 08:34:59,673 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
2024-04-24 08:34:59,896 urllib3.connectionpool DEBUG    https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0
2024-04-24 08:35:13,318 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
2024-04-24 08:35:13,542 urllib3.connectionpool DEBUG    https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0
2024-04-24 08:37:40,050 lightning.pytorch.core INFO     

______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

2024-04-24 08:37:40,051 lightning.pytorch.core INFO     test:True,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:0,  max_grad_norm:0.9,  start_month_index:7,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:37:40,054 lightning.pytorch.core INFO     
+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+
2024-04-24 08:37:40,187 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
2024-04-24 08:37:40,396 urllib3.connectionpool DEBUG    https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0
2024-04-24 08:37:53,295 lightning.pytorch.core INFO     

______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

2024-04-24 08:37:53,295 lightning.pytorch.core INFO     test:True,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:0,  max_grad_norm:0.9,  start_month_index:7,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:37:53,298 lightning.pytorch.core INFO     
+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+
2024-04-24 08:37:53,300 lightning.pytorch.core INFO     
+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |   10   |
|        SAVE STEPS       |   5    |
|      LOGGING STEPS      |   5    |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
2024-04-24 08:37:53,300 lightning.pytorch.core INFO     MODEL PATH: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:37:53,393 lightning.pytorch.core INFO     Tokenizer loaded__________________________________________________________
2024-04-24 08:37:53,393 lightning.pytorch.core INFO     ____________________________________________________________________________________________________
Loading the model
2024-04-24 08:37:55,426 lightning.pytorch.core INFO     
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
2024-04-24 08:37:55,426 lightning.pytorch.core INFO     MISTRAL model size: 1772.5M parameters
2024-04-24 08:37:55,426 lightning.pytorch.core INFO     Total Trainable Params: 1772.5481M
2024-04-24 08:37:55,426 lightning.pytorch.core INFO     Total Trainable Params in one layer: 218.1120M
2024-04-24 08:37:55,426 lightning.pytorch.core INFO     Original Model type:torch.float32
2024-04-24 08:37:55,766 lightning.pytorch.core INFO     GPU memory usage before cleaning cache: 440.88 MB
2024-04-24 08:37:55,766 lightning.pytorch.core INFO     GPU memory usage after cleaning cache: 440.88 MB
2024-04-24 08:37:55,767 lightning.pytorch.core INFO     GPU memory occupied from nvmlInit: 1061 MB.
2024-04-24 08:37:55,767 lightning.pytorch.core INFO     MODEL LOADED took __________ 0.03956359624862671 minutes_________________
2024-04-24 08:37:55,767 lightning.pytorch.core INFO     ______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:37:57,067 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
2024-04-24 08:37:57,287 urllib3.connectionpool DEBUG    https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0
2024-04-24 08:38:09,579 lightning.pytorch.core INFO     

______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

2024-04-24 08:38:09,580 lightning.pytorch.core INFO     test:True,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:0,  max_grad_norm:0.9,  start_month_index:7,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:38:09,583 lightning.pytorch.core INFO     
+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+
2024-04-24 08:38:09,584 lightning.pytorch.core INFO     
+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |   10   |
|        SAVE STEPS       |   5    |
|      LOGGING STEPS      |   5    |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
2024-04-24 08:38:09,584 lightning.pytorch.core INFO     MODEL PATH: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:38:09,717 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
2024-04-24 08:38:09,930 urllib3.connectionpool DEBUG    https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0
2024-04-24 08:51:50,504 lightning.pytorch.core INFO     

______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

2024-04-24 08:51:50,505 lightning.pytorch.core INFO     test:True,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:0,  max_grad_norm:0.9,  start_month_index:7,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:51:50,508 lightning.pytorch.core INFO     
+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+
2024-04-24 08:51:50,509 lightning.pytorch.core INFO     
+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |   10   |
|        SAVE STEPS       |   5    |
|      LOGGING STEPS      |   5    |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
2024-04-24 08:51:50,510 lightning.pytorch.core INFO     MODEL PATH: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:51:50,650 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
2024-04-24 08:51:51,023 urllib3.connectionpool DEBUG    https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0
2024-04-24 08:52:17,937 lightning.pytorch.core INFO     

______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

2024-04-24 08:52:17,937 lightning.pytorch.core INFO     test:True,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:0,  max_grad_norm:0.9,  start_month_index:7,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:52:17,941 lightning.pytorch.core INFO     
+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+
2024-04-24 08:52:17,942 lightning.pytorch.core INFO     
+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |   10   |
|        SAVE STEPS       |   5    |
|      LOGGING STEPS      |   5    |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
2024-04-24 08:52:17,942 lightning.pytorch.core INFO     MODEL PATH: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:52:18,036 lightning.pytorch.core INFO     Tokenizer loaded__________________________________________________________
2024-04-24 08:52:18,037 lightning.pytorch.core INFO     ____________________________________________________________________________________________________
Loading the model
2024-04-24 08:52:20,460 lightning.pytorch.core INFO     
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
2024-04-24 08:52:20,460 lightning.pytorch.core INFO     MISTRAL model size: 1772.5M parameters
2024-04-24 08:52:20,460 lightning.pytorch.core INFO     Total Trainable Params: 1772.5481M
2024-04-24 08:52:20,460 lightning.pytorch.core INFO     Total Trainable Params in one layer: 218.1120M
2024-04-24 08:52:20,460 lightning.pytorch.core INFO     Original Model type:torch.float32
2024-04-24 08:52:20,775 lightning.pytorch.core INFO     GPU memory usage before cleaning cache: 440.88 MB
2024-04-24 08:52:20,776 lightning.pytorch.core INFO     GPU memory usage after cleaning cache: 440.88 MB
2024-04-24 08:52:20,776 lightning.pytorch.core INFO     GPU memory occupied from nvmlInit: 1061 MB.
2024-04-24 08:52:20,776 lightning.pytorch.core INFO     MODEL LOADED took __________ 0.0456559419631958 minutes_________________
2024-04-24 08:52:20,776 lightning.pytorch.core INFO     ______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:52:22,558 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
2024-04-24 08:52:22,783 urllib3.connectionpool DEBUG    https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0
2024-04-24 08:53:19,677 lightning.pytorch.core INFO     

______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

2024-04-24 08:53:19,678 lightning.pytorch.core INFO     test:True,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:0,  max_grad_norm:0.9,  start_month_index:7,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:53:19,681 lightning.pytorch.core INFO     
+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+
2024-04-24 08:53:19,682 lightning.pytorch.core INFO     
+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |   10   |
|        SAVE STEPS       |   5    |
|      LOGGING STEPS      |   5    |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
2024-04-24 08:53:19,683 lightning.pytorch.core INFO     MODEL PATH: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:53:19,776 lightning.pytorch.core INFO     Tokenizer loaded__________________________________________________________
2024-04-24 08:53:19,777 lightning.pytorch.core INFO     ____________________________________________________________________________________________________
Loading the model
2024-04-24 08:53:21,924 lightning.pytorch.core INFO     
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
2024-04-24 08:53:21,925 lightning.pytorch.core INFO     MISTRAL model size: 1772.5M parameters
2024-04-24 08:53:21,925 lightning.pytorch.core INFO     Total Trainable Params: 1772.5481M
2024-04-24 08:53:21,925 lightning.pytorch.core INFO     Total Trainable Params in one layer: 218.1120M
2024-04-24 08:53:21,925 lightning.pytorch.core INFO     Original Model type:torch.float32
2024-04-24 08:53:22,243 lightning.pytorch.core INFO     GPU memory usage before cleaning cache: 440.88 MB
2024-04-24 08:53:22,243 lightning.pytorch.core INFO     GPU memory usage after cleaning cache: 440.88 MB
2024-04-24 08:53:22,245 lightning.pytorch.core INFO     GPU memory occupied from nvmlInit: 1061 MB.
2024-04-24 08:53:22,245 lightning.pytorch.core INFO     MODEL LOADED took __________ 0.04113383293151855 minutes_________________
2024-04-24 08:53:22,245 lightning.pytorch.core INFO     ______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:53:23,785 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
2024-04-24 08:53:24,004 urllib3.connectionpool DEBUG    https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0
2024-04-24 08:54:16,463 lightning.pytorch.core INFO     

______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

2024-04-24 08:54:16,464 lightning.pytorch.core INFO     test:True,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:0,  max_grad_norm:0.9,  start_month_index:7,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:54:16,467 lightning.pytorch.core INFO     
+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+
2024-04-24 08:54:16,469 lightning.pytorch.core INFO     
+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |   10   |
|        SAVE STEPS       |   5    |
|      LOGGING STEPS      |   5    |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
2024-04-24 08:54:16,469 lightning.pytorch.core INFO     MODEL PATH: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:54:16,562 lightning.pytorch.core INFO     Tokenizer loaded__________________________________________________________
2024-04-24 08:54:16,562 lightning.pytorch.core INFO     ____________________________________________________________________________________________________
Loading the model
2024-04-24 08:54:18,810 lightning.pytorch.core INFO     
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
2024-04-24 08:54:18,811 lightning.pytorch.core INFO     MISTRAL model size: 1772.5M parameters
2024-04-24 08:54:18,811 lightning.pytorch.core INFO     Total Trainable Params: 1772.5481M
2024-04-24 08:54:18,811 lightning.pytorch.core INFO     Total Trainable Params in one layer: 218.1120M
2024-04-24 08:54:18,811 lightning.pytorch.core INFO     Original Model type:torch.float32
2024-04-24 08:54:19,122 lightning.pytorch.core INFO     GPU memory usage before cleaning cache: 440.88 MB
2024-04-24 08:54:19,123 lightning.pytorch.core INFO     GPU memory usage after cleaning cache: 440.88 MB
2024-04-24 08:54:19,123 lightning.pytorch.core INFO     GPU memory occupied from nvmlInit: 1061 MB.
2024-04-24 08:54:19,123 lightning.pytorch.core INFO     MODEL LOADED took __________ 0.04267771641413371 minutes_________________
2024-04-24 08:54:19,123 lightning.pytorch.core INFO     ______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:54:20,829 lightning.pytorch.core INFO     
+------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|         Training Argument          |                                                                                                Value                                                                                                |
+------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|             output_dir             | /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 |
|        overwrite_output_dir        |                                                                                                 True                                                                                                |
|              do_train              |                                                                                                 True                                                                                                |
|              do_eval               |                                                                                                 True                                                                                                |
|             do_predict             |                                                                                                False                                                                                                |
|        evaluation_strategy         |                                                                                        IntervalStrategy.STEPS                                                                                       |
|        prediction_loss_only        |                                                                                                False                                                                                                |
|    per_device_train_batch_size     |                                                                                                  1                                                                                                  |
|     per_device_eval_batch_size     |                                                                                                  1                                                                                                  |
|      per_gpu_train_batch_size      |                                                                                                 None                                                                                                |
|      per_gpu_eval_batch_size       |                                                                                                 None                                                                                                |
|    gradient_accumulation_steps     |                                                                                                  32                                                                                                 |
|      eval_accumulation_steps       |                                                                                                  32                                                                                                 |
|             eval_delay             |                                                                                                  0                                                                                                  |
|           learning_rate            |                                                                                                2e-05                                                                                                |
|            weight_decay            |                                                                                                 0.1                                                                                                 |
|             adam_beta1             |                                                                                                 0.9                                                                                                 |
|             adam_beta2             |                                                                                                0.999                                                                                                |
|            adam_epsilon            |                                                                                                1e-08                                                                                                |
|           max_grad_norm            |                                                                                                 0.9                                                                                                 |
|          num_train_epochs          |                                                                                                  1                                                                                                  |
|             max_steps              |                                                                                                  -1                                                                                                 |
|         lr_scheduler_type          |                                                                                         SchedulerType.COSINE                                                                                        |
|        lr_scheduler_kwargs         |                                                                                                  {}                                                                                                 |
|            warmup_ratio            |                                                                                                 0.0                                                                                                 |
|            warmup_steps            |                                                                                                 100                                                                                                 |
|             log_level              |                                                                                                debug                                                                                                |
|         log_level_replica          |                                                                                               warning                                                                                               |
|          log_on_each_node          |                                                                                                 True                                                                                                |
|            logging_dir             |                                                                                                ./logs                                                                                               |
|          logging_strategy          |                                                                                        IntervalStrategy.STEPS                                                                                       |
|         logging_first_step         |                                                                                                False                                                                                                |
|           logging_steps            |                                                                                                  5                                                                                                  |
|       logging_nan_inf_filter       |                                                                                                 True                                                                                                |
|           save_strategy            |                                                                                        IntervalStrategy.STEPS                                                                                       |
|             save_steps             |                                                                                                  5                                                                                                  |
|          save_total_limit          |                                                                                                  3                                                                                                  |
|          save_safetensors          |                                                                                                 True                                                                                                |
|         save_on_each_node          |                                                                                                False                                                                                                |
|          save_only_model           |                                                                                                False                                                                                                |
|              no_cuda               |                                                                                                False                                                                                                |
|              use_cpu               |                                                                                                False                                                                                                |
|           use_mps_device           |                                                                                                False                                                                                                |
|                seed                |                                                                                                  42                                                                                                 |
|             data_seed              |                                                                                                 None                                                                                                |
|           jit_mode_eval            |                                                                                                False                                                                                                |
|              use_ipex              |                                                                                                False                                                                                                |
|                bf16                |                                                                                                False                                                                                                |
|                fp16                |                                                                                                False                                                                                                |
|           fp16_opt_level           |                                                                                                  O1                                                                                                 |
|       half_precision_backend       |                                                                                                 auto                                                                                                |
|           bf16_full_eval           |                                                                                                False                                                                                                |
|           fp16_full_eval           |                                                                                                False                                                                                                |
|                tf32                |                                                                                                 None                                                                                                |
|             local_rank             |                                                                                                  0                                                                                                  |
|            ddp_backend             |                                                                                                 None                                                                                                |
|           tpu_num_cores            |                                                                                                 None                                                                                                |
|         tpu_metrics_debug          |                                                                                                False                                                                                                |
|               debug                |                                                                                                  []                                                                                                 |
|        dataloader_drop_last        |                                                                                                False                                                                                                |
|             eval_steps             |                                                                                                  10                                                                                                 |
|       dataloader_num_workers       |                                                                                                  0                                                                                                  |
|     dataloader_prefetch_factor     |                                                                                                 None                                                                                                |
|             past_index             |                                                                                                  -1                                                                                                 |
|              run_name              |                                                                  run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-07-17_test=False                                                                  |
|            disable_tqdm            |                                                                                                False                                                                                                |
|       remove_unused_columns        |                                                                                                 True                                                                                                |
|            label_names             |                                                                                                 None                                                                                                |
|       load_best_model_at_end       |                                                                                                False                                                                                                |
|       metric_for_best_model        |                                                                                                 None                                                                                                |
|         greater_is_better          |                                                                                                 None                                                                                                |
|          ignore_data_skip          |                                                                                                False                                                                                                |
|                fsdp                |                                                                                                  []                                                                                                 |
|        fsdp_min_num_params         |                                                                                                  0                                                                                                  |
|            fsdp_config             |                                                        {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}                                                       |
| fsdp_transformer_layer_cls_to_wrap |                                                                                                 None                                                                                                |
|         accelerator_config         |                                             AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True)                                             |
|             deepspeed              |                                                                                                 None                                                                                                |
|       label_smoothing_factor       |                                                                                                 0.0                                                                                                 |
|               optim                |                                                                                      OptimizerNames.ADAMW_TORCH                                                                                     |
|             optim_args             |                                                                                                 None                                                                                                |
|             adafactor              |                                                                                                False                                                                                                |
|          group_by_length           |                                                                                                False                                                                                                |
|         length_column_name         |                                                                                                length                                                                                               |
|             report_to              |                                                                                              ['wandb']                                                                                              |
|     ddp_find_unused_parameters     |                                                                                                 None                                                                                                |
|         ddp_bucket_cap_mb          |                                                                                                 None                                                                                                |
|       ddp_broadcast_buffers        |                                                                                                 None                                                                                                |
|       dataloader_pin_memory        |                                                                                                 True                                                                                                |
|   dataloader_persistent_workers    |                                                                                                False                                                                                                |
|        skip_memory_metrics         |                                                                                                 True                                                                                                |
|     use_legacy_prediction_loop     |                                                                                                False                                                                                                |
|            push_to_hub             |                                                                                                False                                                                                                |
|       resume_from_checkpoint       |                                                                                                 None                                                                                                |
|            hub_model_id            |                                                                                                 None                                                                                                |
|            hub_strategy            |                                                                                        HubStrategy.EVERY_SAVE                                                                                       |
|             hub_token              |                                                                                                 None                                                                                                |
|          hub_private_repo          |                                                                                                False                                                                                                |
|          hub_always_push           |                                                                                                False                                                                                                |
|       gradient_checkpointing       |                                                                                                False                                                                                                |
|   gradient_checkpointing_kwargs    |                                                                                                 None                                                                                                |
|     include_inputs_for_metrics     |                                                                                                False                                                                                                |
|            fp16_backend            |                                                                                                 auto                                                                                                |
|        push_to_hub_model_id        |                                                                                                 None                                                                                                |
|      push_to_hub_organization      |                                                                                                 None                                                                                                |
|         push_to_hub_token          |                                                                                                 None                                                                                                |
|           mp_parameters            |                                                                                                                                                                                                     |
|        auto_find_batch_size        |                                                                                                False                                                                                                |
|          full_determinism          |                                                                                                False                                                                                                |
|            torchdynamo             |                                                                                                 None                                                                                                |
|             ray_scope              |                                                                                                 last                                                                                                |
|            ddp_timeout             |                                                                                                 1800                                                                                                |
|           torch_compile            |                                                                                                False                                                                                                |
|       torch_compile_backend        |                                                                                                 None                                                                                                |
|         torch_compile_mode         |                                                                                                 None                                                                                                |
|          dispatch_batches          |                                                                                                 None                                                                                                |
|           split_batches            |                                                                                                 None                                                                                                |
|     include_tokens_per_second      |                                                                                                False                                                                                                |
|   include_num_input_tokens_seen    |                                                                                                False                                                                                                |
|        neftune_noise_alpha         |                                                                                                 None                                                                                                |
|         distributed_state          |                                                                             Distributed environment: DistributedType.NO                                                                             |
|                                    |                                                                                           Num processes: 1                                                                                          |
|                                    |                                                                                           Process index: 0                                                                                          |
|                                    |                                                                                        Local process index: 0                                                                                       |
|                                    |                                                                                             Device: cuda                                                                                            |
|                                    |                                                                                                                                                                                                     |
|               _n_gpu               |                                                                                                  1                                                                                                  |
|      __cached__setup_devices       |                                                                                                cuda:0                                                                                               |
|          deepspeed_plugin          |                                                                                                 None                                                                                                |
+------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
2024-04-24 08:54:20,829 lightning.pytorch.core INFO     ______________________________________________________________________________________________________________________________________________________
Training for 0-7
______________________________________________________________________________________________________________________________________________________
2024-04-24 08:54:20,829 lightning.pytorch.core INFO     val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
2024-04-24 08:54:33,531 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
2024-04-24 08:54:33,752 urllib3.connectionpool DEBUG    https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0
2024-04-24 08:54:52,667 lightning.pytorch.core INFO     

______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

2024-04-24 08:54:52,668 lightning.pytorch.core INFO     test:True,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:0,  max_grad_norm:0.9,  start_month_index:8,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:54:52,671 lightning.pytorch.core INFO     
+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+
2024-04-24 08:54:52,672 lightning.pytorch.core INFO     
+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |   10   |
|        SAVE STEPS       |   5    |
|      LOGGING STEPS      |   5    |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
2024-04-24 08:54:52,673 lightning.pytorch.core INFO     MODEL PATH: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:54:52,766 lightning.pytorch.core INFO     Tokenizer loaded__________________________________________________________
2024-04-24 08:54:52,767 lightning.pytorch.core INFO     ____________________________________________________________________________________________________
Loading the model
2024-04-24 08:54:55,044 lightning.pytorch.core INFO     
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
2024-04-24 08:54:55,045 lightning.pytorch.core INFO     MISTRAL model size: 1772.5M parameters
2024-04-24 08:54:55,045 lightning.pytorch.core INFO     Total Trainable Params: 1772.5481M
2024-04-24 08:54:55,045 lightning.pytorch.core INFO     Total Trainable Params in one layer: 218.1120M
2024-04-24 08:54:55,045 lightning.pytorch.core INFO     Original Model type:torch.float32
2024-04-24 08:54:55,366 lightning.pytorch.core INFO     GPU memory usage before cleaning cache: 440.88 MB
2024-04-24 08:54:55,366 lightning.pytorch.core INFO     GPU memory usage after cleaning cache: 440.88 MB
2024-04-24 08:54:55,366 lightning.pytorch.core INFO     GPU memory occupied from nvmlInit: 1061 MB.
2024-04-24 08:54:55,366 lightning.pytorch.core INFO     MODEL LOADED took __________ 0.04333097537358602 minutes_________________
2024-04-24 08:54:55,366 lightning.pytorch.core INFO     ______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-04-24 08:54:56,977 lightning.pytorch.core INFO     
+------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|         Training Argument          |                                                                                                Value                                                                                                |
+------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|             output_dir             | /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 |
|        overwrite_output_dir        |                                                                                                 True                                                                                                |
|              do_train              |                                                                                                 True                                                                                                |
|              do_eval               |                                                                                                 True                                                                                                |
|             do_predict             |                                                                                                False                                                                                                |
|        evaluation_strategy         |                                                                                        IntervalStrategy.STEPS                                                                                       |
|        prediction_loss_only        |                                                                                                False                                                                                                |
|    per_device_train_batch_size     |                                                                                                  1                                                                                                  |
|     per_device_eval_batch_size     |                                                                                                  1                                                                                                  |
|      per_gpu_train_batch_size      |                                                                                                 None                                                                                                |
|      per_gpu_eval_batch_size       |                                                                                                 None                                                                                                |
|    gradient_accumulation_steps     |                                                                                                  32                                                                                                 |
|      eval_accumulation_steps       |                                                                                                  32                                                                                                 |
|             eval_delay             |                                                                                                  0                                                                                                  |
|           learning_rate            |                                                                                                2e-05                                                                                                |
|            weight_decay            |                                                                                                 0.1                                                                                                 |
|             adam_beta1             |                                                                                                 0.9                                                                                                 |
|             adam_beta2             |                                                                                                0.999                                                                                                |
|            adam_epsilon            |                                                                                                1e-08                                                                                                |
|           max_grad_norm            |                                                                                                 0.9                                                                                                 |
|          num_train_epochs          |                                                                                                  1                                                                                                  |
|             max_steps              |                                                                                                  -1                                                                                                 |
|         lr_scheduler_type          |                                                                                         SchedulerType.COSINE                                                                                        |
|        lr_scheduler_kwargs         |                                                                                                  {}                                                                                                 |
|            warmup_ratio            |                                                                                                 0.0                                                                                                 |
|            warmup_steps            |                                                                                                 100                                                                                                 |
|             log_level              |                                                                                                debug                                                                                                |
|         log_level_replica          |                                                                                               warning                                                                                               |
|          log_on_each_node          |                                                                                                 True                                                                                                |
|            logging_dir             |                                                                                                ./logs                                                                                               |
|          logging_strategy          |                                                                                        IntervalStrategy.STEPS                                                                                       |
|         logging_first_step         |                                                                                                False                                                                                                |
|           logging_steps            |                                                                                                  5                                                                                                  |
|       logging_nan_inf_filter       |                                                                                                 True                                                                                                |
|           save_strategy            |                                                                                        IntervalStrategy.STEPS                                                                                       |
|             save_steps             |                                                                                                  5                                                                                                  |
|          save_total_limit          |                                                                                                  3                                                                                                  |
|          save_safetensors          |                                                                                                 True                                                                                                |
|         save_on_each_node          |                                                                                                False                                                                                                |
|          save_only_model           |                                                                                                False                                                                                                |
|              no_cuda               |                                                                                                False                                                                                                |
|              use_cpu               |                                                                                                False                                                                                                |
|           use_mps_device           |                                                                                                False                                                                                                |
|                seed                |                                                                                                  42                                                                                                 |
|             data_seed              |                                                                                                 None                                                                                                |
|           jit_mode_eval            |                                                                                                False                                                                                                |
|              use_ipex              |                                                                                                False                                                                                                |
|                bf16                |                                                                                                False                                                                                                |
|                fp16                |                                                                                                False                                                                                                |
|           fp16_opt_level           |                                                                                                  O1                                                                                                 |
|       half_precision_backend       |                                                                                                 auto                                                                                                |
|           bf16_full_eval           |                                                                                                False                                                                                                |
|           fp16_full_eval           |                                                                                                False                                                                                                |
|                tf32                |                                                                                                 None                                                                                                |
|             local_rank             |                                                                                                  0                                                                                                  |
|            ddp_backend             |                                                                                                 None                                                                                                |
|           tpu_num_cores            |                                                                                                 None                                                                                                |
|         tpu_metrics_debug          |                                                                                                False                                                                                                |
|               debug                |                                                                                                  []                                                                                                 |
|        dataloader_drop_last        |                                                                                                False                                                                                                |
|             eval_steps             |                                                                                                  10                                                                                                 |
|       dataloader_num_workers       |                                                                                                  0                                                                                                  |
|     dataloader_prefetch_factor     |                                                                                                 None                                                                                                |
|             past_index             |                                                                                                  -1                                                                                                 |
|              run_name              |                                                                  run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-07-17_test=False                                                                  |
|            disable_tqdm            |                                                                                                False                                                                                                |
|       remove_unused_columns        |                                                                                                 True                                                                                                |
|            label_names             |                                                                                                 None                                                                                                |
|       load_best_model_at_end       |                                                                                                False                                                                                                |
|       metric_for_best_model        |                                                                                                 None                                                                                                |
|         greater_is_better          |                                                                                                 None                                                                                                |
|          ignore_data_skip          |                                                                                                False                                                                                                |
|                fsdp                |                                                                                                  []                                                                                                 |
|        fsdp_min_num_params         |                                                                                                  0                                                                                                  |
|            fsdp_config             |                                                        {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}                                                       |
| fsdp_transformer_layer_cls_to_wrap |                                                                                                 None                                                                                                |
|         accelerator_config         |                                             AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True)                                             |
|             deepspeed              |                                                                                                 None                                                                                                |
|       label_smoothing_factor       |                                                                                                 0.0                                                                                                 |
|               optim                |                                                                                      OptimizerNames.ADAMW_TORCH                                                                                     |
|             optim_args             |                                                                                                 None                                                                                                |
|             adafactor              |                                                                                                False                                                                                                |
|          group_by_length           |                                                                                                False                                                                                                |
|         length_column_name         |                                                                                                length                                                                                               |
|             report_to              |                                                                                              ['wandb']                                                                                              |
|     ddp_find_unused_parameters     |                                                                                                 None                                                                                                |
|         ddp_bucket_cap_mb          |                                                                                                 None                                                                                                |
|       ddp_broadcast_buffers        |                                                                                                 None                                                                                                |
|       dataloader_pin_memory        |                                                                                                 True                                                                                                |
|   dataloader_persistent_workers    |                                                                                                False                                                                                                |
|        skip_memory_metrics         |                                                                                                 True                                                                                                |
|     use_legacy_prediction_loop     |                                                                                                False                                                                                                |
|            push_to_hub             |                                                                                                False                                                                                                |
|       resume_from_checkpoint       |                                                                                                 None                                                                                                |
|            hub_model_id            |                                                                                                 None                                                                                                |
|            hub_strategy            |                                                                                        HubStrategy.EVERY_SAVE                                                                                       |
|             hub_token              |                                                                                                 None                                                                                                |
|          hub_private_repo          |                                                                                                False                                                                                                |
|          hub_always_push           |                                                                                                False                                                                                                |
|       gradient_checkpointing       |                                                                                                False                                                                                                |
|   gradient_checkpointing_kwargs    |                                                                                                 None                                                                                                |
|     include_inputs_for_metrics     |                                                                                                False                                                                                                |
|            fp16_backend            |                                                                                                 auto                                                                                                |
|        push_to_hub_model_id        |                                                                                                 None                                                                                                |
|      push_to_hub_organization      |                                                                                                 None                                                                                                |
|         push_to_hub_token          |                                                                                                 None                                                                                                |
|           mp_parameters            |                                                                                                                                                                                                     |
|        auto_find_batch_size        |                                                                                                False                                                                                                |
|          full_determinism          |                                                                                                False                                                                                                |
|            torchdynamo             |                                                                                                 None                                                                                                |
|             ray_scope              |                                                                                                 last                                                                                                |
|            ddp_timeout             |                                                                                                 1800                                                                                                |
|           torch_compile            |                                                                                                False                                                                                                |
|       torch_compile_backend        |                                                                                                 None                                                                                                |
|         torch_compile_mode         |                                                                                                 None                                                                                                |
|          dispatch_batches          |                                                                                                 None                                                                                                |
|           split_batches            |                                                                                                 None                                                                                                |
|     include_tokens_per_second      |                                                                                                False                                                                                                |
|   include_num_input_tokens_seen    |                                                                                                False                                                                                                |
|        neftune_noise_alpha         |                                                                                                 None                                                                                                |
|         distributed_state          |                                                                             Distributed environment: DistributedType.NO                                                                             |
|                                    |                                                                                           Num processes: 1                                                                                          |
|                                    |                                                                                           Process index: 0                                                                                          |
|                                    |                                                                                        Local process index: 0                                                                                       |
|                                    |                                                                                             Device: cuda                                                                                            |
|                                    |                                                                                                                                                                                                     |
|               _n_gpu               |                                                                                                  1                                                                                                  |
|      __cached__setup_devices       |                                                                                                cuda:0                                                                                               |
|          deepspeed_plugin          |                                                                                                 None                                                                                                |
+------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
2024-04-24 08:54:56,978 lightning.pytorch.core INFO     ______________________________________________________________________________________________________________________________________________________
Training for 0-8
______________________________________________________________________________________________________________________________________________________
2024-04-24 08:54:56,978 lightning.pytorch.core INFO     val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_8_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_8_datasets.pkl
2024-04-24 08:55:33,919 lightning.pytorch.core INFO     trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
}) 
 Took0.615695075194041 minutes
2024-04-24 08:55:34,056 git.cmd    DEBUG    Popen(['git', 'cat-file', '--batch-check'], cwd=/home/iitgn_cse/latex_model, stdin=<valid stream>, shell=False, universal_newlines=False)
2024-04-24 09:03:18,992 lightning.pytorch.core INFO     ____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=10, training_loss=3.561591911315918, metrics={'train_runtime': 464.9475, 'train_samples_per_second': 0.688, 'train_steps_per_second': 0.022, 'train_loss': 3.561591911315918, 'epoch': 1.0})
 Took7.751200600465139 minutes
2024-04-24 09:03:18,992 lightning.pytorch.core INFO     GPU memory usage before cleaning cache: 35522.88 MB
2024-04-24 09:03:19,043 lightning.pytorch.core INFO     GPU memory usage after cleaning cache: 21918.88 MB
2024-04-24 09:03:19,043 lightning.pytorch.core INFO     GPU memory occupied from nvmlInit: 22539 MB.
2024-04-24 09:03:22,824 lightning.pytorch.core INFO     ----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 4.530398368835449, 'eval_runtime': 3.7536, 'eval_samples_per_second': 2.664, 'eval_steps_per_second': 2.664, 'epoch': 1.0}
 Took0.06259162425994873 minutes
2024-04-24 09:03:22,824 lightning.pytorch.core INFO     ----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 3.5602, 'grad_norm': 1.9239450693130493, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.5, 'step': 5}, {'loss': 3.5629, 'grad_norm': 2.0373435020446777, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.0, 'step': 10}, {'eval_loss': 4.530398368835449, 'eval_runtime': 3.7295, 'eval_samples_per_second': 2.681, 'eval_steps_per_second': 2.681, 'epoch': 1.0, 'step': 10}, {'train_runtime': 464.9475, 'train_samples_per_second': 0.688, 'train_steps_per_second': 0.022, 'total_flos': 6168369658503168.0, 'train_loss': 3.561591911315918, 'epoch': 1.0, 'step': 10}, {'eval_loss': 4.530398368835449, 'eval_runtime': 3.7536, 'eval_samples_per_second': 2.664, 'eval_steps_per_second': 2.664, 'epoch': 1.0, 'step': 10}]
2024-04-24 09:03:22,825 lightning.pytorch.core INFO     Model STATE saved successfully 
 Took5.622704823811849e-06 minutes
2024-04-24 09:03:31,211 lightning.pytorch.core INFO     Model saved successfully @ /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-24/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.13975923061370848 minutes
2024-04-24 09:03:31,211 lightning.pytorch.core INFO     GPU memory usage before cleaning cache: 22618.88 MB
2024-04-24 09:03:31,221 lightning.pytorch.core INFO     GPU memory usage after cleaning cache: 21918.88 MB
2024-04-24 09:03:31,221 lightning.pytorch.core INFO     GPU memory occupied from nvmlInit: 22539 MB.
2024-04-24 09:03:31,221 lightning.pytorch.core INFO     ______________________________________________________________________________________________________________________________________________________
Training for 0-8 completed
______________________________________________________________________________________________________________________________________________________
2024-04-24 09:03:31,221 lightning.pytorch.core INFO     ______________________________________________________________________________________________________________________________________________________
Training for 0-9
______________________________________________________________________________________________________________________________________________________
2024-04-24 09:03:31,221 lightning.pytorch.core INFO     val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_9_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_9_datasets.pkl
2024-04-24 09:06:33,906 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
2024-04-24 09:06:34,118 urllib3.connectionpool DEBUG    https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0
2024-04-24 09:08:11,164 - cmd.py:1057 - git.cmd    DEBUG    Popen(['git', 'cat-file', '--batch-check'], cwd=/home/iitgn_cse/latex_model, stdin=<valid stream>, shell=False, universal_newlines=False)
2024-04-24 11:15:12,531 - connectionpool.py:1055 - urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
2024-04-24 11:15:12,756 - connectionpool.py:549 - urllib3.connectionpool DEBUG    https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0
