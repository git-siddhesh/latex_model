2024-05-12 20:28:07,073 - v4_training_fp32.py:149 - root       INFO     

______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

2024-05-12 20:28:07,073 - v4_training_fp32.py:150 - root       INFO     test:False,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:3,  max_grad_norm:0.9,  start_month_index:1,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-05-12 20:28:07,098 - training_utils.py:83 - root       INFO     
+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+
2024-05-12 20:28:07,098 - training_utils.py:122 - root       INFO     
+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |  6000  |
|        SAVE STEPS       |  2000  |
|      LOGGING STEPS      |  500   |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
2024-05-12 20:28:07,098 - v4_training_fp32.py:153 - root       INFO     MODEL PATH: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-05-12 20:28:07,207 - v4_training_fp32.py:157 - root       INFO     Tokenizer loaded__________________________________________________________
2024-05-12 20:28:07,207 - training_utils.py:260 - root       INFO     ____________________________________________________________________________________________________
Loading the model
2024-05-12 20:28:40,603 - training_utils.py:290 - root       INFO     
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
2024-05-12 20:28:40,603 - training_utils.py:291 - root       INFO     MISTRAL model size: 1772.5M parameters
2024-05-12 20:28:40,603 - training_utils.py:292 - root       INFO     Total Trainable Params: 1772.5481M
2024-05-12 20:28:40,603 - training_utils.py:293 - root       INFO     Total Trainable Params in one layer: 218.1120M
2024-05-12 20:28:40,604 - training_utils.py:294 - root       INFO     Original Model type:torch.float32
2024-05-12 20:28:42,120 - training_utils.py:31 - root       INFO     GPU memory usage before cleaning cache: 440.88 MB
2024-05-12 20:28:42,121 - training_utils.py:35 - root       INFO     GPU memory usage after cleaning cache: 440.88 MB
2024-05-12 20:28:42,121 - training_utils.py:27 - root       INFO     GPU memory occupied from nvmlInit: 1061 MB.
2024-05-12 20:28:42,121 - v4_training_fp32.py:161 - root       INFO     MODEL LOADED took __________ 0.5818966746330261 minutes_________________
2024-05-12 20:28:42,121 - v4_training_fp32.py:162 - root       INFO     ______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
2024-05-12 20:28:47,265 - v4_training_fp32.py:212 - root       INFO     
+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|         Training Argument          |                                                    Value                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|             output_dir             |                              /home/iitgn_cse/latex_model/model_main_fp32_2024-                              |
|                                    |                                 04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-                                |
|                                    |                              05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30                             |
|                                    |                              000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4                             |
|                                    |                                                     096                                                     |
|        overwrite_output_dir        |                                                     True                                                    |
|              do_train              |                                                     True                                                    |
|              do_eval               |                                                     True                                                    |
|             do_predict             |                                                    False                                                    |
|        evaluation_strategy         |                                                    steps                                                    |
|        prediction_loss_only        |                                                    False                                                    |
|    per_device_train_batch_size     |                                                      1                                                      |
|     per_device_eval_batch_size     |                                                      1                                                      |
|      per_gpu_train_batch_size      |                                                     None                                                    |
|      per_gpu_eval_batch_size       |                                                     None                                                    |
|    gradient_accumulation_steps     |                                                      32                                                     |
|      eval_accumulation_steps       |                                                      32                                                     |
|             eval_delay             |                                                      0                                                      |
|           learning_rate            |                                                    2e-05                                                    |
|            weight_decay            |                                                     0.1                                                     |
|             adam_beta1             |                                                     0.9                                                     |
|             adam_beta2             |                                                    0.999                                                    |
|            adam_epsilon            |                                                    1e-08                                                    |
|           max_grad_norm            |                                                     0.9                                                     |
|          num_train_epochs          |                                                      1                                                      |
|             max_steps              |                                                      -1                                                     |
|         lr_scheduler_type          |                                                    cosine                                                   |
|        lr_scheduler_kwargs         |                                                      {}                                                     |
|            warmup_ratio            |                                                     0.0                                                     |
|            warmup_steps            |                                                     100                                                     |
|             log_level              |                                                    debug                                                    |
|         log_level_replica          |                                                   warning                                                   |
|          log_on_each_node          |                                                     True                                                    |
|            logging_dir             |                                                    ./logs                                                   |
|          logging_strategy          |                                                    steps                                                    |
|         logging_first_step         |                                                    False                                                    |
|           logging_steps            |                                                     500                                                     |
|       logging_nan_inf_filter       |                                                     True                                                    |
|           save_strategy            |                                                    steps                                                    |
|             save_steps             |                                                     2000                                                    |
|          save_total_limit          |                                                      3                                                      |
|          save_safetensors          |                                                     True                                                    |
|         save_on_each_node          |                                                    False                                                    |
|          save_only_model           |                                                    False                                                    |
|              no_cuda               |                                                    False                                                    |
|              use_cpu               |                                                    False                                                    |
|           use_mps_device           |                                                    False                                                    |
|                seed                |                                                      42                                                     |
|             data_seed              |                                                     None                                                    |
|           jit_mode_eval            |                                                    False                                                    |
|              use_ipex              |                                                    False                                                    |
|                bf16                |                                                    False                                                    |
|                fp16                |                                                    False                                                    |
|           fp16_opt_level           |                                                      O1                                                     |
|       half_precision_backend       |                                                     auto                                                    |
|           bf16_full_eval           |                                                    False                                                    |
|           fp16_full_eval           |                                                    False                                                    |
|                tf32                |                                                     None                                                    |
|             local_rank             |                                                      0                                                      |
|            ddp_backend             |                                                     None                                                    |
|           tpu_num_cores            |                                                     None                                                    |
|         tpu_metrics_debug          |                                                    False                                                    |
|               debug                |                                                      []                                                     |
|        dataloader_drop_last        |                                                    False                                                    |
|             eval_steps             |                                                     6000                                                    |
|       dataloader_num_workers       |                                                      0                                                      |
|     dataloader_prefetch_factor     |                                                     None                                                    |
|             past_index             |                                                      -1                                                     |
|              run_name              |                              run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-                              |
|                                    |                                               07-17_test_False                                              |
|            disable_tqdm            |                                                    False                                                    |
|       remove_unused_columns        |                                                     True                                                    |
|            label_names             |                                                     None                                                    |
|       load_best_model_at_end       |                                                    False                                                    |
|       metric_for_best_model        |                                                     None                                                    |
|         greater_is_better          |                                                     None                                                    |
|          ignore_data_skip          |                                                    False                                                    |
|                fsdp                |                                                      []                                                     |
|        fsdp_min_num_params         |                                                      0                                                      |
|            fsdp_config             |            {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}           |
| fsdp_transformer_layer_cls_to_wrap |                                                     None                                                    |
|         accelerator_config         | AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True) |
|             deepspeed              |                                                     None                                                    |
|       label_smoothing_factor       |                                                     0.0                                                     |
|               optim                |                                                 adamw_torch                                                 |
|             optim_args             |                                                     None                                                    |
|             adafactor              |                                                    False                                                    |
|          group_by_length           |                                                    False                                                    |
|         length_column_name         |                                                    length                                                   |
|             report_to              |                                                  ['wandb']                                                  |
|     ddp_find_unused_parameters     |                                                     None                                                    |
|         ddp_bucket_cap_mb          |                                                     None                                                    |
|       ddp_broadcast_buffers        |                                                     None                                                    |
|       dataloader_pin_memory        |                                                     True                                                    |
|   dataloader_persistent_workers    |                                                    False                                                    |
|        skip_memory_metrics         |                                                     True                                                    |
|     use_legacy_prediction_loop     |                                                    False                                                    |
|            push_to_hub             |                                                    False                                                    |
|       resume_from_checkpoint       |                                                     None                                                    |
|            hub_model_id            |                                                     None                                                    |
|            hub_strategy            |                                                  every_save                                                 |
|             hub_token              |                                                     None                                                    |
|          hub_private_repo          |                                                    False                                                    |
|          hub_always_push           |                                                    False                                                    |
|       gradient_checkpointing       |                                                    False                                                    |
|   gradient_checkpointing_kwargs    |                                                     None                                                    |
|     include_inputs_for_metrics     |                                                    False                                                    |
|            fp16_backend            |                                                     auto                                                    |
|        push_to_hub_model_id        |                                                     None                                                    |
|      push_to_hub_organization      |                                                     None                                                    |
|         push_to_hub_token          |                                                     None                                                    |
|           mp_parameters            |                                                                                                             |
|        auto_find_batch_size        |                                                    False                                                    |
|          full_determinism          |                                                    False                                                    |
|            torchdynamo             |                                                     None                                                    |
|             ray_scope              |                                                     last                                                    |
|            ddp_timeout             |                                                     1800                                                    |
|           torch_compile            |                                                    False                                                    |
|       torch_compile_backend        |                                                     None                                                    |
|         torch_compile_mode         |                                                     None                                                    |
|          dispatch_batches          |                                                     None                                                    |
|           split_batches            |                                                     None                                                    |
|     include_tokens_per_second      |                                                    False                                                    |
|   include_num_input_tokens_seen    |                                                    False                                                    |
|        neftune_noise_alpha         |                                                     None                                                    |
|         distributed_state          |                                 Distributed environment: DistributedType.NO                                 |
|                                    |                                               Num processes: 1                                              |
|                                    |                                               Process index: 0                                              |
|                                    |                                            Local process index: 0                                           |
|                                    |                                                 Device: cuda                                                |
|                                    |                                                                                                             |
|               _n_gpu               |                                                      1                                                      |
|      __cached__setup_devices       |                                                    cuda:0                                                   |
|          deepspeed_plugin          |                                                     None                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+

2024-05-12 20:28:47,266 - v4_training_fp32.py:216 - root       INFO     ______________________________________________________________________________________________________________________________________________________
Training for 2-10
______________________________________________________________________________________________________________________________________________________

2024-05-12 20:28:47,266 - v4_training_fp32.py:219 - root       INFO     val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_2_10_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_2_10_datasets.pkl
2024-05-12 20:29:23,399 - v4_training_fp32.py:228 - root       INFO     trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 182170
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 48129
}) 
 Took0.6022147258122762 minutes
2024-05-13 06:17:10,615 - v4_training_fp32.py:235 - root       INFO     ____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5692, training_loss=2.0207515778209824, metrics={'train_runtime': 35265.6857, 'train_samples_per_second': 5.166, 'train_steps_per_second': 0.161, 'train_loss': 2.0207515778209824, 'epoch': 1.0})
 Took587.7868955691655 minutes
2024-05-13 06:17:10,616 - training_utils.py:31 - root       INFO     GPU memory usage before cleaning cache: 35776.81 MB
2024-05-13 06:17:10,667 - training_utils.py:35 - root       INFO     GPU memory usage after cleaning cache: 21796.81 MB
2024-05-13 06:17:10,668 - training_utils.py:27 - root       INFO     GPU memory occupied from nvmlInit: 22417 MB.
2024-05-13 06:51:58,049 - v4_training_fp32.py:246 - root       INFO     ----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2086.3195, 'eval_samples_per_second': 23.069, 'eval_steps_per_second': 23.069, 'epoch': 1.0}
 Took34.789665536085764 minutes
2024-05-13 06:51:58,049 - v4_training_fp32.py:250 - root       INFO     ----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.039, 'grad_norm': 1.9965236186981201, 'learning_rate': 1.9748564262071798e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.0655, 'grad_norm': 2.521886110305786, 'learning_rate': 1.8748737957725903e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.051, 'grad_norm': 2.437591552734375, 'learning_rate': 1.7063118280355656e-05, 'epoch': 0.26, 'step': 1500}, {'loss': 2.0476, 'grad_norm': 2.4510231018066406, 'learning_rate': 1.4823837065405865e-05, 'epoch': 0.35, 'step': 2000}, {'loss': 2.027, 'grad_norm': 2.0815916061401367, 'learning_rate': 1.2206426398186534e-05, 'epoch': 0.44, 'step': 2500}, {'loss': 2.0278, 'grad_norm': 2.0987651348114014, 'learning_rate': 9.416059058901605e-06, 'epoch': 0.53, 'step': 3000}, {'loss': 2.0103, 'grad_norm': 2.3071131706237793, 'learning_rate': 6.671465502027106e-06, 'epoch': 0.61, 'step': 3500}, {'loss': 2.0049, 'grad_norm': 2.082608222961426, 'learning_rate': 4.18778808089959e-06, 'epoch': 0.7, 'step': 4000}, {'loss': 1.9972, 'grad_norm': 2.4265785217285156, 'learning_rate': 2.159716535270063e-06, 'epoch': 0.79, 'step': 4500}, {'loss': 1.9954, 'grad_norm': 2.2130792140960693, 'learning_rate': 7.462267120243905e-07, 'epoch': 0.88, 'step': 5000}, {'loss': 1.9815, 'grad_norm': 2.2182326316833496, 'learning_rate': 5.811881552384768e-08, 'epoch': 0.97, 'step': 5500}, {'train_runtime': 35265.6857, 'train_samples_per_second': 5.166, 'train_steps_per_second': 0.161, 'total_flos': 4.3329903415627776e+17, 'train_loss': 2.0207515778209824, 'epoch': 1.0, 'step': 5692}, {'eval_loss': nan, 'eval_runtime': 2086.3195, 'eval_samples_per_second': 23.069, 'eval_steps_per_second': 23.069, 'epoch': 1.0, 'step': 5692}]
2024-05-13 06:51:58,050 - v4_training_fp32.py:255 - root       INFO     Model STATE saved successfully 
 Took1.0720888773600261e-05 minutes
2024-05-13 06:53:00,496 - v4_training_fp32.py:262 - root       INFO     Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.0407648921012878 minutes
2024-05-13 06:53:00,496 - training_utils.py:31 - root       INFO     GPU memory usage before cleaning cache: 22534.81 MB
2024-05-13 06:53:00,500 - training_utils.py:35 - root       INFO     GPU memory usage after cleaning cache: 21796.81 MB
2024-05-13 06:53:00,501 - training_utils.py:27 - root       INFO     GPU memory occupied from nvmlInit: 22417 MB.
2024-05-13 06:53:00,501 - v4_training_fp32.py:267 - root       INFO     ______________________________________________________________________________________________________________________________________________________
Training for 2-10 completed
######################################################################################################################################################
2024-05-13 06:53:00,501 - v4_training_fp32.py:216 - root       INFO     ______________________________________________________________________________________________________________________________________________________
Training for 2-11
______________________________________________________________________________________________________________________________________________________

2024-05-13 06:53:00,501 - v4_training_fp32.py:219 - root       INFO     val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_2_11_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_2_11_datasets.pkl
2024-05-13 06:53:48,652 - v4_training_fp32.py:228 - root       INFO     trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180576
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47847
}) 
 Took0.8025217572848002 minutes
2024-05-13 16:06:30,758 - v4_training_fp32.py:235 - root       INFO     ____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5643, training_loss=2.0109281970870296, metrics={'train_runtime': 33160.239, 'train_samples_per_second': 5.446, 'train_steps_per_second': 0.17, 'train_loss': 2.0109281970870296, 'epoch': 1.0})
 Took552.701746114095 minutes
2024-05-13 16:06:30,759 - training_utils.py:31 - root       INFO     GPU memory usage before cleaning cache: 35616.81 MB
2024-05-13 16:06:30,809 - training_utils.py:35 - root       INFO     GPU memory usage after cleaning cache: 21796.81 MB
2024-05-13 16:06:30,810 - training_utils.py:27 - root       INFO     GPU memory occupied from nvmlInit: 22417 MB.
2024-05-13 16:39:09,484 - v4_training_fp32.py:246 - root       INFO     ----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 1.8838684558868408, 'eval_runtime': 1958.4886, 'eval_samples_per_second': 24.431, 'eval_steps_per_second': 24.431, 'epoch': 1.0}
 Took32.64456741015116 minutes
2024-05-13 16:39:09,485 - v4_training_fp32.py:250 - root       INFO     ----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.0291, 'grad_norm': 2.084932804107666, 'learning_rate': 1.974411837986949e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.0409, 'grad_norm': 2.203181505203247, 'learning_rate': 1.8727001701698057e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.0511, 'grad_norm': 2.175938129425049, 'learning_rate': 1.7013728775055817e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.0444, 'grad_norm': 2.4087376594543457, 'learning_rate': 1.4740968033614076e-05, 'epoch': 0.35, 'step': 2000}, {'loss': 2.019, 'grad_norm': 2.303009510040283, 'learning_rate': 1.2090018467050574e-05, 'epoch': 0.44, 'step': 2500}, {'loss': 2.0193, 'grad_norm': 2.19380784034729, 'learning_rate': 9.272347331614969e-06, 'epoch': 0.53, 'step': 3000}, {'loss': 2.0084, 'grad_norm': 2.4074506759643555, 'learning_rate': 6.512721326583556e-06, 'epoch': 0.62, 'step': 3500}, {'loss': 1.9904, 'grad_norm': 1.9798035621643066, 'learning_rate': 4.0312768690801426e-06, 'epoch': 0.71, 'step': 4000}, {'loss': 1.99, 'grad_norm': 2.396077871322632, 'learning_rate': 2.025959728267134e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 1.9699, 'grad_norm': 1.9312386512756348, 'learning_rate': 6.567348159734044e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 1.9674, 'grad_norm': 2.1695384979248047, 'learning_rate': 3.282572477253165e-08, 'epoch': 0.97, 'step': 5500}, {'train_runtime': 33160.239, 'train_samples_per_second': 5.446, 'train_steps_per_second': 0.17, 'total_flos': 4.015305244028682e+17, 'train_loss': 2.0109281970870296, 'epoch': 1.0, 'step': 5643}, {'eval_loss': 1.8838684558868408, 'eval_runtime': 1958.4886, 'eval_samples_per_second': 24.431, 'eval_steps_per_second': 24.431, 'epoch': 1.0, 'step': 5643}]
2024-05-13 16:39:09,486 - v4_training_fp32.py:255 - root       INFO     Model STATE saved successfully 
 Took1.0526180267333984e-05 minutes
2024-05-13 16:40:03,304 - v4_training_fp32.py:262 - root       INFO     Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8969751318295797 minutes
2024-05-13 16:40:03,305 - training_utils.py:31 - root       INFO     GPU memory usage before cleaning cache: 22534.81 MB
2024-05-13 16:40:03,309 - training_utils.py:35 - root       INFO     GPU memory usage after cleaning cache: 21796.81 MB
2024-05-13 16:40:03,309 - training_utils.py:27 - root       INFO     GPU memory occupied from nvmlInit: 22417 MB.
2024-05-13 16:40:03,309 - v4_training_fp32.py:267 - root       INFO     ______________________________________________________________________________________________________________________________________________________
Training for 2-11 completed
######################################################################################################################################################
2024-05-13 16:40:03,309 - v4_training_fp32.py:216 - root       INFO     ______________________________________________________________________________________________________________________________________________________
Training for 2-12
______________________________________________________________________________________________________________________________________________________

2024-05-13 16:40:03,309 - v4_training_fp32.py:219 - root       INFO     val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_2_12_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_2_12_datasets.pkl
2024-05-13 16:40:50,779 - v4_training_fp32.py:228 - root       INFO     trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180432
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47826
}) 
 Took0.7911599159240723 minutes
2024-05-14 01:50:58,867 - v4_training_fp32.py:235 - root       INFO     ____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5638, training_loss=1.9961408849894133, metrics={'train_runtime': 33005.8023, 'train_samples_per_second': 5.467, 'train_steps_per_second': 0.171, 'train_loss': 1.9961408849894133, 'epoch': 1.0})
 Took550.1347915172577 minutes
2024-05-14 01:50:58,868 - training_utils.py:31 - root       INFO     GPU memory usage before cleaning cache: 35636.81 MB
2024-05-14 01:50:58,916 - training_utils.py:35 - root       INFO     GPU memory usage after cleaning cache: 21796.81 MB
2024-05-14 01:50:58,916 - training_utils.py:27 - root       INFO     GPU memory occupied from nvmlInit: 22417 MB.
2024-05-14 02:23:29,727 - v4_training_fp32.py:246 - root       INFO     ----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 1.8767949342727661, 'eval_runtime': 1950.6202, 'eval_samples_per_second': 24.518, 'eval_steps_per_second': 24.518, 'epoch': 1.0}
 Took32.513497936725614 minutes
2024-05-14 02:23:29,727 - v4_training_fp32.py:250 - root       INFO     ----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.007, 'grad_norm': 2.0673742294311523, 'learning_rate': 1.9743658109419642e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.0363, 'grad_norm': 2.29445219039917, 'learning_rate': 1.8724752174301643e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.0364, 'grad_norm': 2.1350817680358887, 'learning_rate': 1.7008620580080197e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.0236, 'grad_norm': 2.113255023956299, 'learning_rate': 1.4732405443572511e-05, 'epoch': 0.35, 'step': 2000}, {'loss': 2.0031, 'grad_norm': 2.2684285640716553, 'learning_rate': 1.2078007129064846e-05, 'epoch': 0.44, 'step': 2500}, {'loss': 1.9996, 'grad_norm': 2.2452192306518555, 'learning_rate': 9.257547949877105e-06, 'epoch': 0.53, 'step': 3000}, {'loss': 1.9969, 'grad_norm': 2.1913270950317383, 'learning_rate': 6.496420727592898e-06, 'epoch': 0.62, 'step': 3500}, {'loss': 1.982, 'grad_norm': 2.3338124752044678, 'learning_rate': 4.015276858259427e-06, 'epoch': 0.71, 'step': 4000}, {'loss': 1.9672, 'grad_norm': 2.2658581733703613, 'learning_rate': 2.0123932825927795e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 1.9568, 'grad_norm': 2.2558069229125977, 'learning_rate': 6.47827477671199e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 1.9594, 'grad_norm': 2.3653030395507812, 'learning_rate': 3.062670050776429e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 33005.8023, 'train_samples_per_second': 5.467, 'train_steps_per_second': 0.171, 'total_flos': 3.997440229369774e+17, 'train_loss': 1.9961408849894133, 'epoch': 1.0, 'step': 5638}, {'eval_loss': 1.8767949342727661, 'eval_runtime': 1950.6202, 'eval_samples_per_second': 24.518, 'eval_steps_per_second': 24.518, 'epoch': 1.0, 'step': 5638}]
2024-05-14 02:23:29,728 - v4_training_fp32.py:255 - root       INFO     Model STATE saved successfully 
 Took1.1161963144938152e-05 minutes
2024-05-14 02:24:24,670 - v4_training_fp32.py:262 - root       INFO     Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.9156962354977926 minutes
2024-05-14 02:24:24,671 - training_utils.py:31 - root       INFO     GPU memory usage before cleaning cache: 22534.81 MB
2024-05-14 02:24:24,675 - training_utils.py:35 - root       INFO     GPU memory usage after cleaning cache: 21796.81 MB
2024-05-14 02:24:24,675 - training_utils.py:27 - root       INFO     GPU memory occupied from nvmlInit: 22417 MB.
2024-05-14 02:24:24,675 - v4_training_fp32.py:267 - root       INFO     ______________________________________________________________________________________________________________________________________________________
Training for 2-12 completed
######################################################################################################################################################
2024-05-14 02:24:24,675 - v4_training_fp32.py:216 - root       INFO     ______________________________________________________________________________________________________________________________________________________
Training for 3-1
______________________________________________________________________________________________________________________________________________________

2024-05-14 02:24:24,675 - v4_training_fp32.py:219 - root       INFO     val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_3_1_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_3_1_datasets.pkl
2024-05-14 02:25:11,925 - v4_training_fp32.py:228 - root       INFO     trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 179752
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47677
}) 
 Took0.7874942342440288 minutes
2024-05-14 11:22:07,423 - v4_training_fp32.py:235 - root       INFO     ____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5617, training_loss=1.9838477799067702, metrics={'train_runtime': 32213.8607, 'train_samples_per_second': 5.58, 'train_steps_per_second': 0.174, 'train_loss': 1.9838477799067702, 'epoch': 1.0})
 Took536.9249483625094 minutes
2024-05-14 11:22:07,423 - training_utils.py:31 - root       INFO     GPU memory usage before cleaning cache: 35636.81 MB
2024-05-14 11:22:07,470 - training_utils.py:35 - root       INFO     GPU memory usage after cleaning cache: 21796.81 MB
2024-05-14 11:22:07,471 - training_utils.py:27 - root       INFO     GPU memory occupied from nvmlInit: 22417 MB.
2024-05-14 11:53:41,590 - v4_training_fp32.py:246 - root       INFO     ----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1892.5837, 'eval_samples_per_second': 25.191, 'eval_steps_per_second': 25.191, 'epoch': 1.0}
 Took31.568642020225525 minutes
2024-05-14 11:53:41,591 - v4_training_fp32.py:250 - root       INFO     ----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 1.9931, 'grad_norm': 2.3240373134613037, 'learning_rate': 1.9741711367723004e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.033, 'grad_norm': 2.2729310989379883, 'learning_rate': 1.8715239259515184e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.0189, 'grad_norm': 2.189875602722168, 'learning_rate': 1.6987025469968482e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.011, 'grad_norm': 2.1139750480651855, 'learning_rate': 1.4696223912621815e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 1.9959, 'grad_norm': 2.022477388381958, 'learning_rate': 1.2027287462330119e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 1.9878, 'grad_norm': 2.087688684463501, 'learning_rate': 9.195116008384388e-06, 'epoch': 0.53, 'step': 3000}, {'loss': 1.9856, 'grad_norm': 2.631833076477051, 'learning_rate': 6.427752946812096e-06, 'epoch': 0.62, 'step': 3500}, {'loss': 1.9717, 'grad_norm': 2.3147528171539307, 'learning_rate': 3.948023371667049e-06, 'epoch': 0.71, 'step': 4000}, {'loss': 1.9466, 'grad_norm': 2.4221904277801514, 'learning_rate': 1.9555924383754934e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 1.9442, 'grad_norm': 2.194664716720581, 'learning_rate': 6.108885404038622e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 1.9403, 'grad_norm': 2.0818276405334473, 'learning_rate': 2.2185787752672106e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 32213.8607, 'train_samples_per_second': 5.58, 'train_steps_per_second': 0.174, 'total_flos': 3.8774176702377984e+17, 'train_loss': 1.9838477799067702, 'epoch': 1.0, 'step': 5617}, {'eval_loss': nan, 'eval_runtime': 1892.5837, 'eval_samples_per_second': 25.191, 'eval_steps_per_second': 25.191, 'epoch': 1.0, 'step': 5617}]
2024-05-14 11:53:41,592 - v4_training_fp32.py:255 - root       INFO     Model STATE saved successfully 
 Took1.0569890340169271e-05 minutes
2024-05-14 11:54:35,516 - v4_training_fp32.py:262 - root       INFO     Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8987465620040893 minutes
2024-05-14 11:54:35,517 - training_utils.py:31 - root       INFO     GPU memory usage before cleaning cache: 22534.81 MB
2024-05-14 11:54:35,521 - training_utils.py:35 - root       INFO     GPU memory usage after cleaning cache: 21796.81 MB
2024-05-14 11:54:35,521 - training_utils.py:27 - root       INFO     GPU memory occupied from nvmlInit: 22417 MB.
2024-05-14 11:54:35,521 - v4_training_fp32.py:267 - root       INFO     ______________________________________________________________________________________________________________________________________________________
Training for 3-1 completed
######################################################################################################################################################
2024-05-14 11:54:35,521 - v4_training_fp32.py:216 - root       INFO     ______________________________________________________________________________________________________________________________________________________
Training for 3-2
______________________________________________________________________________________________________________________________________________________

2024-05-14 11:54:35,521 - v4_training_fp32.py:219 - root       INFO     val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_3_2_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_3_2_datasets.pkl
2024-05-14 11:55:24,478 - v4_training_fp32.py:228 - root       INFO     trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178446
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47565
}) 
 Took0.8159356315930685 minutes
2024-05-14 20:23:12,834 - v4_training_fp32.py:235 - root       INFO     ____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5576, training_loss=1.9708248614580766, metrics={'train_runtime': 30466.5179, 'train_samples_per_second': 5.857, 'train_steps_per_second': 0.183, 'train_loss': 1.9708248614580766, 'epoch': 1.0})
 Took507.8059255798658 minutes
2024-05-14 20:23:12,835 - training_utils.py:31 - root       INFO     GPU memory usage before cleaning cache: 35616.81 MB
2024-05-14 20:23:12,883 - training_utils.py:35 - root       INFO     GPU memory usage after cleaning cache: 21796.81 MB
2024-05-14 20:23:12,884 - training_utils.py:27 - root       INFO     GPU memory occupied from nvmlInit: 22417 MB.
2024-05-14 20:54:01,913 - v4_training_fp32.py:246 - root       INFO     ----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1848.8334, 'eval_samples_per_second': 25.727, 'eval_steps_per_second': 25.727, 'epoch': 1.0}
 Took30.81714117527008 minutes
2024-05-14 20:54:01,914 - v4_training_fp32.py:250 - root       INFO     ----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 1.9771, 'grad_norm': 2.296298027038574, 'learning_rate': 1.9737846213609392e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.0105, 'grad_norm': 2.429800033569336, 'learning_rate': 1.869635952065721e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 1.9999, 'grad_norm': 2.1759934425354004, 'learning_rate': 1.694419891068761e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.0083, 'grad_norm': 2.1026558876037598, 'learning_rate': 1.462455244313774e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 1.9942, 'grad_norm': 2.3038036823272705, 'learning_rate': 1.1926983605726049e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 1.9753, 'grad_norm': 2.3759849071502686, 'learning_rate': 9.07194002456018e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 1.963, 'grad_norm': 2.0201549530029297, 'learning_rate': 6.292738296115124e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 1.955, 'grad_norm': 2.2729926109313965, 'learning_rate': 3.816497155977188e-06, 'epoch': 0.72, 'step': 4000}, {'loss': 1.9367, 'grad_norm': 2.133025646209717, 'learning_rate': 1.845577142065873e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 1.9361, 'grad_norm': 2.4326491355895996, 'learning_rate': 5.410435188849228e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 1.9286, 'grad_norm': 2.429476261138916, 'learning_rate': 9.503886737061151e-09, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 30466.5179, 'train_samples_per_second': 5.857, 'train_steps_per_second': 0.183, 'total_flos': 3.615051354873938e+17, 'train_loss': 1.9708248614580766, 'epoch': 1.0, 'step': 5576}, {'eval_loss': nan, 'eval_runtime': 1848.8334, 'eval_samples_per_second': 25.727, 'eval_steps_per_second': 25.727, 'epoch': 1.0, 'step': 5576}]
2024-05-14 20:54:01,914 - v4_training_fp32.py:255 - root       INFO     Model STATE saved successfully 
 Took1.095136006673177e-05 minutes
2024-05-14 20:55:01,407 - v4_training_fp32.py:262 - root       INFO     Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.9915353735287984 minutes
2024-05-14 20:55:01,407 - training_utils.py:31 - root       INFO     GPU memory usage before cleaning cache: 22534.81 MB
2024-05-14 20:55:01,411 - training_utils.py:35 - root       INFO     GPU memory usage after cleaning cache: 21796.81 MB
2024-05-14 20:55:01,411 - training_utils.py:27 - root       INFO     GPU memory occupied from nvmlInit: 22417 MB.
2024-05-14 20:55:01,412 - v4_training_fp32.py:267 - root       INFO     ______________________________________________________________________________________________________________________________________________________
Training for 3-2 completed
######################################################################################################################################################
2024-05-14 20:55:01,412 - v4_training_fp32.py:216 - root       INFO     ______________________________________________________________________________________________________________________________________________________
Training for 3-3
______________________________________________________________________________________________________________________________________________________

2024-05-14 20:55:01,412 - v4_training_fp32.py:219 - root       INFO     val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_3_3_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_3_3_datasets.pkl
2024-05-14 20:55:53,306 - v4_training_fp32.py:228 - root       INFO     trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 179970
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47666
}) 
 Took0.8648937145868937 minutes
