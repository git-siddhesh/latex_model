device:3,  layer:12,  seq_len:4096,  batch_size:1,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 30253 MB.
GPU memory usage before training: 2346.69 MB
GPU memory occupied from method1: 30253 MB.
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 30253 MB.
GPU memory usage before training: 2346.69 MB
GPU memory occupied from method1: 30253 MB.
device:3,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 30389 MB.
GPU memory usage before training: 7146.69 MB
GPU memory occupied from method1: 30389 MB.
device:3,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 30389 MB.
GPU memory usage before training: 7146.69 MB
GPU memory occupied from method1: 30389 MB.
device:3,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 30389 MB.
GPU memory usage before training: 7146.69 MB
GPU memory occupied from method1: 30389 MB.
device:3,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 30389 MB.
GPU memory usage before training: 7146.69 MB
GPU memory occupied from method1: 30389 MB.
device:3,  layer:12,  seq_len:4096,  batch_size:1,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:1,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 6840.69 MB
GPU memory occupied from method1: 10113 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 6840.69 MB
GPU memory occupied from method1: 10113 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 6840.69 MB
GPU memory occupied from method1: 10113 MB.
Encoding tokens started...
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 6840.69 MB
GPU memory occupied from method1: 10113 MB.
device:0,  layer:10,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 5880.69 MB
GPU memory occupied from method1: 10113 MB.
device:0,  layer:10,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 5880.69 MB
GPU memory occupied from method1: 10113 MB.
device:0,  layer:10,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 5880.69 MB
GPU memory occupied from method1: 10113 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 275 MB.
GPU memory usage before training: 12776.69 MB
GPU memory occupied from method1: 275 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 13112.31 MB
GPU memory occupied from method1: 581 MB.
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 20504.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 27482.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:10,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 275 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 275 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 275 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 275 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 271 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 271 MB.
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 271 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 271 MB.
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:2,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:2,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 15238.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 15238.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:2,  seq_len:256,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 256
device:0,  layer:2,  seq_len:256,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 256
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:2,  seq_len:256,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 256
device:0,  layer:2,  seq_len:256,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 256
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:4,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:4,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
Encoding tokens started...
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 583 MB.
GPU memory usage before training: 5638.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 15876.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 2267 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 2267 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 2267 MB.
GPU memory usage before training: 2000.31 MB
GPU memory occupied from method1: 2267 MB.
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 2267 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 2267 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 2267 MB.
GPU memory usage before training: 2000.31 MB
GPU memory occupied from method1: 2267 MB.
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 12309 MB.
GPU memory usage before training: 15950.69 MB
GPU memory occupied from method1: 12309 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 12309 MB.
GPU memory usage before training: 12042.31 MB
GPU memory occupied from method1: 12309 MB.
device:0,  layer:4,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:4,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 13689 MB.
GPU memory usage before training: 15954.69 MB
GPU memory occupied from method1: 13689 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 13689 MB.
GPU memory usage before training: 13422.31 MB
GPU memory occupied from method1: 13689 MB.
device:0,  layer:8,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:8,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:6,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:6,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:5,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:5,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 583 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:4,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:4,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:4,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:4,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:4,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:4,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 583 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:8,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:8,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:12,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:12,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 1191 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 1335 MB.
No LaTeX files found in the Month and Year: 1 0
Encoding tokens started...
No LaTeX files found in the Month and Year: 1 0
Encoding tokens started...
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
No LaTeX files found in the Month and Year: 1 0
Encoding tokens started...
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
No LaTeX files found in the Month and Year: 1 0
Encoding tokens started...
No LaTeX files found in the Month and Year: 1 0
Encoding tokens started...
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.60 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.49 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 1820 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 3.14 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.50 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 1820 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 1820, TOTAL TOKENS: 29131765
Size of latex_corpus <tokenized>   16312
tokenized_dataset created and took 5.54 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1820
})
lm_datasets created and took 1.90 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 15149
})
Total number of chunks: 14220
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
PAPERS: 1820, TOTAL TOKENS: 29131765
Size of latex_corpus <tokenized>   16312
tokenized_dataset created and took 6.04 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1820
})
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 9 for year: 0 and month: 1
Encoding tokens started...
Validation dataset size: 9 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 9, TOTAL TOKENS: 85347
Size of latex_corpus <tokenized>   184
tokenized_dataset created and took 1.02 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 9
})
PAPERS: 9, TOTAL TOKENS: 85347
Size of latex_corpus <tokenized>   184
tokenized_dataset created and took 1.06 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 9
})
lm_datasets created and took 1.02 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 46
})
Total number of chunks: 41
Train dataset size: 1820 for year: 0 and month: 1
Encoding tokens started...
lm_datasets created and took 1.05 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 46
})
Total number of chunks: 41
Train dataset size: 1820 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 1820, TOTAL TOKENS: 29131765
Size of latex_corpus <tokenized>   16312
tokenized_dataset created and took 2.18 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1820
})
PAPERS: 1820, TOTAL TOKENS: 29131765
Size of latex_corpus <tokenized>   16312
tokenized_dataset created and took 2.28 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1820
})
lm_datasets created and took 1.94 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 15149
})
Total number of chunks: 14220
lm_datasets created and took 2.01 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 15149
})
Total number of chunks: 14220
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 693 MB.
GPU memory usage before training: 426.69 MB
GPU memory occupied from method1: 693 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 693 MB.
GPU memory usage before training: 426.31 MB
GPU memory occupied from method1: 693 MB.
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 9 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 9, TOTAL TOKENS: 85347
Size of latex_corpus <tokenized>   184
tokenized_dataset created and took 1.02 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 9
})
lm_datasets created and took 1.04 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 46
})
Total number of chunks: 41
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 9 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 9, TOTAL TOKENS: 85347
Size of latex_corpus <tokenized>   184
tokenized_dataset created and took 1.00 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 9
})
lm_datasets created and took 1.02 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 46
})
Total number of chunks: 41
Train dataset size: 1 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 1, TOTAL TOKENS: 12771
Size of latex_corpus <tokenized>   72
tokenized_dataset created and took 0.08 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1
})
lm_datasets created and took 0.07 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 7
})
Total number of chunks: 6
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 271 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 271 MB.
GPU memory occupied from method1: 271 MB.
GPU memory usage after training: 25878.69 MB
GPU memory usage after emptying cache: 15546.69 MB
***** Train results ***** TrainOutput(global_step=7, training_loss=6.29357419695173, metrics={'train_runtime': 10.245, 'train_samples_per_second': 0.683, 'train_steps_per_second': 0.683, 'train_loss': 6.29357419695173, 'epoch': 1.0})
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 7.32 GiB. GPU 0 has a total capacity of 31.74 GiB of which 6.94 GiB is free. Including non-PyTorch memory, this process has 24.79 GiB memory in use. Of the allocated memory 21.53 GiB is allocated by PyTorch, and 2.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
device:0,  layer:9,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 9, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 9 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 9, TOTAL TOKENS: 85347
Size of latex_corpus <tokenized>   184
tokenized_dataset created and took 1.01 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 9
})
lm_datasets created and took 1.04 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 46
})
Total number of chunks: 41
Train dataset size: 18 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 18, TOTAL TOKENS: 161778
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.77 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
lm_datasets created and took 1.75 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 87
})
Total number of chunks: 78
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2208.8M parameters
Total Trainable Params: 2208.7721M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 271 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 271 MB.
GPU memory occupied from method1: 271 MB.
GPU memory usage after training: 23740.69 MB
GPU memory usage after emptying cache: 14174.69 MB
***** Train results ***** TrainOutput(global_step=87, training_loss=5.631460957143498, metrics={'train_runtime': 53.5427, 'train_samples_per_second': 1.625, 'train_steps_per_second': 1.625, 'train_loss': 5.631460957143498, 'epoch': 1.0})
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 8.01 GiB. GPU 0 has a total capacity of 31.74 GiB of which 7.59 GiB is free. Including non-PyTorch memory, this process has 24.14 GiB memory in use. Of the allocated memory 20.93 GiB is allocated by PyTorch, and 2.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
device:0,  layer:9,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 9, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.50 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 18 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 18, TOTAL TOKENS: 161778
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.77 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
lm_datasets created and took 1.75 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 87
})
Total number of chunks: 78
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 271 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 271 MB.
GPU memory occupied from method1: 271 MB.
GPU memory usage after training: 25986.69 MB
GPU memory usage after emptying cache: 15578.69 MB
***** Train results ***** TrainOutput(global_step=87, training_loss=5.654990360654634, metrics={'train_runtime': 57.805, 'train_samples_per_second': 1.505, 'train_steps_per_second': 1.505, 'train_loss': 5.654990360654634, 'epoch': 1.0})
GPU memory occupied from method1: 271 MB.
GPU memory usage before training: 15578.69 MB
GPU memory occupied from method1: 271 MB.
Error occured while training the model ???????????????????????????????????
cannot convert float NaN to integer
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.50 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.50 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 18 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 18, TOTAL TOKENS: 161778
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.73 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
lm_datasets created and took 1.78 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 87
})
Total number of chunks: 78
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 271 MB.
***** Train results ***** TrainOutput(global_step=87, training_loss=5.654990360654634, metrics={'train_runtime': 57.8198, 'train_samples_per_second': 1.505, 'train_steps_per_second': 1.505, 'train_loss': 5.654990360654634, 'epoch': 1.0})
GPU memory usage before cleaning cache: 25986.69 MB
GPU memory usage after cleaning cache: 15578.69 MB
GPU memory occupied from nvmlInit: 271 MB.
Error occured while training the model ???????????????????????????????????
cannot convert float NaN to integer
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
device:0,  layer:4,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:4,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 271 MB.
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.50 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 18 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 18, TOTAL TOKENS: 161778
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.77 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
lm_datasets created and took 1.83 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 87
})
Total number of chunks: 78
***** Train results ***** TrainOutput(global_step=87, training_loss=5.654990360654634, metrics={'train_runtime': 57.572, 'train_samples_per_second': 1.511, 'train_steps_per_second': 1.511, 'train_loss': 5.654990360654634, 'epoch': 1.0})
GPU memory usage before cleaning cache: 25986.69 MB
GPU memory usage after cleaning cache: 15578.69 MB
GPU memory occupied from nvmlInit: 271 MB.
Error occured while training the model ???????????????????????????????????
cannot convert float NaN to integer
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.49 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.49 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 18 for year: 0 and month: 1
Encoding tokens started...
lm_datasets created and took 0.52 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 18 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 18, TOTAL TOKENS: 161778
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.71 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
PAPERS: 18, TOTAL TOKENS: 161778
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.78 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
lm_datasets created and took 1.77 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 87
})
Total number of chunks: 78
lm_datasets created and took 1.78 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 87
})
Total number of chunks: 78
***** Train results ***** TrainOutput(global_step=44, training_loss=5.708337263627485, metrics={'train_runtime': 47.7796, 'train_samples_per_second': 1.821, 'train_steps_per_second': 0.921, 'total_flos': 2303552558989312.0, 'train_loss': 5.708337263627485, 'epoch': 1.0})
***** Train results ***** TrainOutput(global_step=44, training_loss=5.717791470614347, metrics={'train_runtime': 47.7786, 'train_samples_per_second': 1.821, 'train_steps_per_second': 0.921, 'train_loss': 5.717791470614347, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30202.31 MB
GPU memory usage before cleaning cache: 30562.69 MB
GPU memory usage after cleaning cache: 20386.69 MB
GPU memory occupied from nvmlInit: 20731 MB.
GPU memory usage after cleaning cache: 20464.31 MB
GPU memory occupied from nvmlInit: 20731 MB.
Error occured while training the model ???????????????????????????????????
cannot convert float NaN to integer
GPU memory usage before cleaning cache: 28904.31 MB
GPU memory usage after cleaning cache: 20464.31 MB
GPU memory occupied from nvmlInit: 20731 MB.
Validation dataset size: 4 for year: 0 and month: 2
Encoding tokens started...
Error occured while training the model ???????????????????????????????????
cannot convert float NaN to integer
GPU memory usage before cleaning cache: 28826.69 MB
GPU memory usage after cleaning cache: 20386.69 MB
GPU memory occupied from nvmlInit: 20731 MB.
Validation dataset size: 4 for year: 0 and month: 2
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 82482
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.53 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
PAPERS: 4, TOTAL TOKENS: 82482
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.52 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 42
})
Total number of chunks: 40
Train dataset size: 18 for year: 0 and month: 2
Encoding tokens started...
lm_datasets created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 42
})
Total number of chunks: 40
Train dataset size: 18 for year: 0 and month: 2
Encoding tokens started...
PAPERS: 18, TOTAL TOKENS: 190397
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.81 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
PAPERS: 18, TOTAL TOKENS: 190397
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.76 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
lm_datasets created and took 1.81 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 102
})
Total number of chunks: 93
lm_datasets created and took 1.78 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 102
})
Total number of chunks: 93
***** Train results ***** TrainOutput(global_step=51, training_loss=0.0, metrics={'train_runtime': 74.3069, 'train_samples_per_second': 1.373, 'train_steps_per_second': 0.686, 'total_flos': 2698242336751616.0, 'train_loss': 0.0, 'epoch': 1.0})
***** Train results ***** TrainOutput(global_step=51, training_loss=0.0, metrics={'train_runtime': 49.5173, 'train_samples_per_second': 2.06, 'train_steps_per_second': 1.03, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 32046.31 MB
GPU memory usage before cleaning cache: 32036.69 MB
GPU memory usage after cleaning cache: 20386.69 MB
GPU memory usage after cleaning cache: 20464.31 MB
GPU memory occupied from nvmlInit: 20731 MB.
GPU memory occupied from nvmlInit: 20731 MB.
Error occured while training the model ???????????????????????????????????
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 5.95 GiB. GPU 1 has a total capacity of 31.74 GiB of which 5.80 GiB is free. Including non-PyTorch memory, this process has 25.93 GiB memory in use. Of the allocated memory 24.67 GiB is allocated by PyTorch, and 794.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
CUDA out of memory. Tried to allocate 5.95 GiB. GPU 0 has a total capacity of 31.74 GiB of which 5.88 GiB is free. Including non-PyTorch memory, this process has 25.85 GiB memory in use. Of the allocated memory 24.67 GiB is allocated by PyTorch, and 716.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
GPU memory usage before cleaning cache: 26560.31 MB
GPU memory usage after cleaning cache: 20464.31 MB
GPU memory occupied from nvmlInit: 20731 MB.
Validation dataset size: 4 for year: 0 and month: 3
Encoding tokens started...
GPU memory usage before cleaning cache: 26482.69 MB
GPU memory usage after cleaning cache: 20386.69 MB
GPU memory occupied from nvmlInit: 20731 MB.
Validation dataset size: 4 for year: 0 and month: 3
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 126732
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.52 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
PAPERS: 4, TOTAL TOKENS: 126732
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.52 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 65
})
Total number of chunks: 61
Train dataset size: 19 for year: 0 and month: 3
Encoding tokens started...
lm_datasets created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 65
})
Total number of chunks: 61
Train dataset size: 19 for year: 0 and month: 3
Encoding tokens started...
PAPERS: 19, TOTAL TOKENS: 247535
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.87 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 19
})
PAPERS: 19, TOTAL TOKENS: 247535
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.84 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 19
})
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 271 MB.
Validation dataset size: 227 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 45796, TOTAL TOKENS: 6977822
Size of latex_corpus <tokenized>   392792
tokenized_dataset created and took 1.96 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 45796
})
lm_datasets created and took 3.03 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47459
})
Total number of chunks: 1780
Test dataset size: 227 for year: 0 and month: 1
Encoding tokens started...
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 275 MB.
Validation dataset size: 227 for year: 0 and month: 1
Encoding tokens started...
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 275 MB.
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 275 MB.
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:11,  seq_len:2048,  batch_size:4,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 4, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:13,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 74.81 MiB is free. Including non-PyTorch memory, this process has 31.66 GiB memory in use. Of the allocated memory 29.03 GiB is allocated by PyTorch, and 2.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|     model.layers.12.self_attn.q_proj.weight     |  16777216  |
|     model.layers.12.self_attn.k_proj.weight     |  4194304   |
|     model.layers.12.self_attn.v_proj.weight     |  4194304   |
|     model.layers.12.self_attn.o_proj.weight     |  16777216  |
|       model.layers.12.mlp.gate_proj.weight      |  58720256  |
|        model.layers.12.mlp.up_proj.weight       |  58720256  |
|       model.layers.12.mlp.down_proj.weight      |  58720256  |
|      model.layers.12.input_layernorm.weight     |    4096    |
| model.layers.12.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 3081.2M parameters
Total Trainable Params: 3081.2201M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 32693 MB.
device:0,  layer:11,  seq_len:2048,  batch_size:2,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 2, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:13,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|     model.layers.12.self_attn.q_proj.weight     |  16777216  |
|     model.layers.12.self_attn.k_proj.weight     |  4194304   |
|     model.layers.12.self_attn.v_proj.weight     |  4194304   |
|     model.layers.12.self_attn.o_proj.weight     |  16777216  |
|       model.layers.12.mlp.gate_proj.weight      |  58720256  |
|        model.layers.12.mlp.up_proj.weight       |  58720256  |
|       model.layers.12.mlp.down_proj.weight      |  58720256  |
|      model.layers.12.input_layernorm.weight     |    4096    |
| model.layers.12.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 3081.2M parameters
Total Trainable Params: 3081.2201M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 5.72 GiB. GPU 0 has a total capacity of 31.74 GiB of which 5.32 GiB is free. Including non-PyTorch memory, this process has 26.41 GiB memory in use. Of the allocated memory 23.77 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 6.64 GiB. GPU 0 has a total capacity of 31.74 GiB of which 6.53 GiB is free. Including non-PyTorch memory, this process has 25.20 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 2.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
