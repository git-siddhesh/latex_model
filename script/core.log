device:3,  layer:12,  seq_len:4096,  batch_size:1,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 30253 MB.
GPU memory usage before training: 2346.69 MB
GPU memory occupied from method1: 30253 MB.
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 30253 MB.
GPU memory usage before training: 2346.69 MB
GPU memory occupied from method1: 30253 MB.
device:3,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 30389 MB.
GPU memory usage before training: 7146.69 MB
GPU memory occupied from method1: 30389 MB.
device:3,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 30389 MB.
GPU memory usage before training: 7146.69 MB
GPU memory occupied from method1: 30389 MB.
device:3,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 30389 MB.
GPU memory usage before training: 7146.69 MB
GPU memory occupied from method1: 30389 MB.
device:3,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 30389 MB.
GPU memory usage before training: 7146.69 MB
GPU memory occupied from method1: 30389 MB.
device:3,  layer:12,  seq_len:4096,  batch_size:1,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:1,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 6840.69 MB
GPU memory occupied from method1: 10113 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 6840.69 MB
GPU memory occupied from method1: 10113 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 6840.69 MB
GPU memory occupied from method1: 10113 MB.
Encoding tokens started...
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 6840.69 MB
GPU memory occupied from method1: 10113 MB.
device:0,  layer:10,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 5880.69 MB
GPU memory occupied from method1: 10113 MB.
device:0,  layer:10,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 5880.69 MB
GPU memory occupied from method1: 10113 MB.
device:0,  layer:10,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 10113 MB.
GPU memory usage before training: 5880.69 MB
GPU memory occupied from method1: 10113 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 275 MB.
GPU memory usage before training: 12776.69 MB
GPU memory occupied from method1: 275 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 13112.31 MB
GPU memory occupied from method1: 581 MB.
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 20504.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 27482.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:10,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 275 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 275 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
GPU memory occupied from method1: 275 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 275 MB.
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 271 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 271 MB.
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 271 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 271 MB.
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:2,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:2,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 15238.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
device:0,  layer:2,  seq_len:1024,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 1024
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 15238.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:2,  seq_len:256,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 256
device:0,  layer:2,  seq_len:256,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 256
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:2,  seq_len:256,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 256
device:0,  layer:2,  seq_len:256,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 256
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:4,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:4,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
Encoding tokens started...
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 583 MB.
GPU memory usage before training: 5638.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 15876.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 2267 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 2267 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 2267 MB.
GPU memory usage before training: 2000.31 MB
GPU memory occupied from method1: 2267 MB.
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:8,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 2267 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 2267 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 2267 MB.
GPU memory usage before training: 2000.31 MB
GPU memory occupied from method1: 2267 MB.
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:6,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 12309 MB.
GPU memory usage before training: 15950.69 MB
GPU memory occupied from method1: 12309 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 12309 MB.
GPU memory usage before training: 12042.31 MB
GPU memory occupied from method1: 12309 MB.
device:0,  layer:4,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:4,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 13689 MB.
GPU memory usage before training: 15954.69 MB
GPU memory occupied from method1: 13689 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 13689 MB.
GPU memory usage before training: 13422.31 MB
GPU memory occupied from method1: 13689 MB.
device:0,  layer:8,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:8,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:6,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:6,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:5,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:5,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 583 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:4,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:4,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:4,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:4,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:2,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:4,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:4,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1118.2M parameters
Total Trainable Params: 1118.2121M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 583 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 583 MB.
device:0,  layer:8,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:8,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:12,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:12,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.31 MB
GPU memory occupied from method1: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 1191 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 1335 MB.
No LaTeX files found in the Month and Year: 1 0
Encoding tokens started...
No LaTeX files found in the Month and Year: 1 0
Encoding tokens started...
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
No LaTeX files found in the Month and Year: 1 0
Encoding tokens started...
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
No LaTeX files found in the Month and Year: 1 0
Encoding tokens started...
No LaTeX files found in the Month and Year: 1 0
Encoding tokens started...
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.60 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.49 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 1820 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 3.14 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.50 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 1820 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 1820, TOTAL TOKENS: 29131765
Size of latex_corpus <tokenized>   16312
tokenized_dataset created and took 5.54 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1820
})
lm_datasets created and took 1.90 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 15149
})
Total number of chunks: 14220
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 581 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 581 MB.
PAPERS: 1820, TOTAL TOKENS: 29131765
Size of latex_corpus <tokenized>   16312
tokenized_dataset created and took 6.04 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1820
})
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 9 for year: 0 and month: 1
Encoding tokens started...
Validation dataset size: 9 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 9, TOTAL TOKENS: 85347
Size of latex_corpus <tokenized>   184
tokenized_dataset created and took 1.02 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 9
})
PAPERS: 9, TOTAL TOKENS: 85347
Size of latex_corpus <tokenized>   184
tokenized_dataset created and took 1.06 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 9
})
lm_datasets created and took 1.02 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 46
})
Total number of chunks: 41
Train dataset size: 1820 for year: 0 and month: 1
Encoding tokens started...
lm_datasets created and took 1.05 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 46
})
Total number of chunks: 41
Train dataset size: 1820 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 1820, TOTAL TOKENS: 29131765
Size of latex_corpus <tokenized>   16312
tokenized_dataset created and took 2.18 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1820
})
PAPERS: 1820, TOTAL TOKENS: 29131765
Size of latex_corpus <tokenized>   16312
tokenized_dataset created and took 2.28 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1820
})
lm_datasets created and took 1.94 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 15149
})
Total number of chunks: 14220
lm_datasets created and took 2.01 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 15149
})
Total number of chunks: 14220
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 693 MB.
GPU memory usage before training: 426.69 MB
GPU memory occupied from method1: 693 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 693 MB.
GPU memory usage before training: 426.31 MB
GPU memory occupied from method1: 693 MB.
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 9 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 9, TOTAL TOKENS: 85347
Size of latex_corpus <tokenized>   184
tokenized_dataset created and took 1.02 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 9
})
lm_datasets created and took 1.04 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 46
})
Total number of chunks: 41
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 9 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 9, TOTAL TOKENS: 85347
Size of latex_corpus <tokenized>   184
tokenized_dataset created and took 1.00 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 9
})
lm_datasets created and took 1.02 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 46
})
Total number of chunks: 41
Train dataset size: 1 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 1, TOTAL TOKENS: 12771
Size of latex_corpus <tokenized>   72
tokenized_dataset created and took 0.08 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 1
})
lm_datasets created and took 0.07 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 7
})
Total number of chunks: 6
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 271 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 271 MB.
GPU memory occupied from method1: 271 MB.
GPU memory usage after training: 25878.69 MB
GPU memory usage after emptying cache: 15546.69 MB
***** Train results ***** TrainOutput(global_step=7, training_loss=6.29357419695173, metrics={'train_runtime': 10.245, 'train_samples_per_second': 0.683, 'train_steps_per_second': 0.683, 'train_loss': 6.29357419695173, 'epoch': 1.0})
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 7.32 GiB. GPU 0 has a total capacity of 31.74 GiB of which 6.94 GiB is free. Including non-PyTorch memory, this process has 24.79 GiB memory in use. Of the allocated memory 21.53 GiB is allocated by PyTorch, and 2.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
device:0,  layer:9,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 9, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 9 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 9, TOTAL TOKENS: 85347
Size of latex_corpus <tokenized>   184
tokenized_dataset created and took 1.01 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 9
})
lm_datasets created and took 1.04 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 46
})
Total number of chunks: 41
Train dataset size: 18 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 18, TOTAL TOKENS: 161778
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.77 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
lm_datasets created and took 1.75 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 87
})
Total number of chunks: 78
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2208.8M parameters
Total Trainable Params: 2208.7721M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 271 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 271 MB.
GPU memory occupied from method1: 271 MB.
GPU memory usage after training: 23740.69 MB
GPU memory usage after emptying cache: 14174.69 MB
***** Train results ***** TrainOutput(global_step=87, training_loss=5.631460957143498, metrics={'train_runtime': 53.5427, 'train_samples_per_second': 1.625, 'train_steps_per_second': 1.625, 'train_loss': 5.631460957143498, 'epoch': 1.0})
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 8.01 GiB. GPU 0 has a total capacity of 31.74 GiB of which 7.59 GiB is free. Including non-PyTorch memory, this process has 24.14 GiB memory in use. Of the allocated memory 20.93 GiB is allocated by PyTorch, and 2.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
device:0,  layer:9,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 9, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.50 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 18 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 18, TOTAL TOKENS: 161778
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.77 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
lm_datasets created and took 1.75 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 87
})
Total number of chunks: 78
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory occupied from method1: 271 MB.
GPU memory usage before training: 314.69 MB
GPU memory occupied from method1: 271 MB.
GPU memory occupied from method1: 271 MB.
GPU memory usage after training: 25986.69 MB
GPU memory usage after emptying cache: 15578.69 MB
***** Train results ***** TrainOutput(global_step=87, training_loss=5.654990360654634, metrics={'train_runtime': 57.805, 'train_samples_per_second': 1.505, 'train_steps_per_second': 1.505, 'train_loss': 5.654990360654634, 'epoch': 1.0})
GPU memory occupied from method1: 271 MB.
GPU memory usage before training: 15578.69 MB
GPU memory occupied from method1: 271 MB.
Error occured while training the model ???????????????????????????????????
cannot convert float NaN to integer
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.50 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.50 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 18 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 18, TOTAL TOKENS: 161778
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.73 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
lm_datasets created and took 1.78 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 87
})
Total number of chunks: 78
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 271 MB.
***** Train results ***** TrainOutput(global_step=87, training_loss=5.654990360654634, metrics={'train_runtime': 57.8198, 'train_samples_per_second': 1.505, 'train_steps_per_second': 1.505, 'train_loss': 5.654990360654634, 'epoch': 1.0})
GPU memory usage before cleaning cache: 25986.69 MB
GPU memory usage after cleaning cache: 15578.69 MB
GPU memory occupied from nvmlInit: 271 MB.
Error occured while training the model ???????????????????????????????????
cannot convert float NaN to integer
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: linear, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
device:0,  layer:4,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:4,  seq_len:4096,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:True,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 4, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 271 MB.
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.50 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 18 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 18, TOTAL TOKENS: 161778
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.77 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
lm_datasets created and took 1.83 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 87
})
Total number of chunks: 78
***** Train results ***** TrainOutput(global_step=87, training_loss=5.654990360654634, metrics={'train_runtime': 57.572, 'train_samples_per_second': 1.511, 'train_steps_per_second': 1.511, 'train_loss': 5.654990360654634, 'epoch': 1.0})
GPU memory usage before cleaning cache: 25986.69 MB
GPU memory usage after cleaning cache: 15578.69 MB
GPU memory occupied from nvmlInit: 271 MB.
Error occured while training the model ???????????????????????????????????
cannot convert float NaN to integer
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 581 MB.
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
Validation dataset size: 4 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.49 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
PAPERS: 4, TOTAL TOKENS: 30951
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.49 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 18 for year: 0 and month: 1
Encoding tokens started...
lm_datasets created and took 0.52 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 18
})
Total number of chunks: 14
Train dataset size: 18 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 18, TOTAL TOKENS: 161778
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.71 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
PAPERS: 18, TOTAL TOKENS: 161778
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.78 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
lm_datasets created and took 1.77 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 87
})
Total number of chunks: 78
lm_datasets created and took 1.78 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 87
})
Total number of chunks: 78
***** Train results ***** TrainOutput(global_step=44, training_loss=5.708337263627485, metrics={'train_runtime': 47.7796, 'train_samples_per_second': 1.821, 'train_steps_per_second': 0.921, 'total_flos': 2303552558989312.0, 'train_loss': 5.708337263627485, 'epoch': 1.0})
***** Train results ***** TrainOutput(global_step=44, training_loss=5.717791470614347, metrics={'train_runtime': 47.7786, 'train_samples_per_second': 1.821, 'train_steps_per_second': 0.921, 'train_loss': 5.717791470614347, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30202.31 MB
GPU memory usage before cleaning cache: 30562.69 MB
GPU memory usage after cleaning cache: 20386.69 MB
GPU memory occupied from nvmlInit: 20731 MB.
GPU memory usage after cleaning cache: 20464.31 MB
GPU memory occupied from nvmlInit: 20731 MB.
Error occured while training the model ???????????????????????????????????
cannot convert float NaN to integer
GPU memory usage before cleaning cache: 28904.31 MB
GPU memory usage after cleaning cache: 20464.31 MB
GPU memory occupied from nvmlInit: 20731 MB.
Validation dataset size: 4 for year: 0 and month: 2
Encoding tokens started...
Error occured while training the model ???????????????????????????????????
cannot convert float NaN to integer
GPU memory usage before cleaning cache: 28826.69 MB
GPU memory usage after cleaning cache: 20386.69 MB
GPU memory occupied from nvmlInit: 20731 MB.
Validation dataset size: 4 for year: 0 and month: 2
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 82482
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.53 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
PAPERS: 4, TOTAL TOKENS: 82482
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.52 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 42
})
Total number of chunks: 40
Train dataset size: 18 for year: 0 and month: 2
Encoding tokens started...
lm_datasets created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 42
})
Total number of chunks: 40
Train dataset size: 18 for year: 0 and month: 2
Encoding tokens started...
PAPERS: 18, TOTAL TOKENS: 190397
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.81 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
PAPERS: 18, TOTAL TOKENS: 190397
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.76 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 18
})
lm_datasets created and took 1.81 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 102
})
Total number of chunks: 93
lm_datasets created and took 1.78 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 102
})
Total number of chunks: 93
***** Train results ***** TrainOutput(global_step=51, training_loss=0.0, metrics={'train_runtime': 74.3069, 'train_samples_per_second': 1.373, 'train_steps_per_second': 0.686, 'total_flos': 2698242336751616.0, 'train_loss': 0.0, 'epoch': 1.0})
***** Train results ***** TrainOutput(global_step=51, training_loss=0.0, metrics={'train_runtime': 49.5173, 'train_samples_per_second': 2.06, 'train_steps_per_second': 1.03, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 32046.31 MB
GPU memory usage before cleaning cache: 32036.69 MB
GPU memory usage after cleaning cache: 20386.69 MB
GPU memory usage after cleaning cache: 20464.31 MB
GPU memory occupied from nvmlInit: 20731 MB.
GPU memory occupied from nvmlInit: 20731 MB.
Error occured while training the model ???????????????????????????????????
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 5.95 GiB. GPU 1 has a total capacity of 31.74 GiB of which 5.80 GiB is free. Including non-PyTorch memory, this process has 25.93 GiB memory in use. Of the allocated memory 24.67 GiB is allocated by PyTorch, and 794.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
CUDA out of memory. Tried to allocate 5.95 GiB. GPU 0 has a total capacity of 31.74 GiB of which 5.88 GiB is free. Including non-PyTorch memory, this process has 25.85 GiB memory in use. Of the allocated memory 24.67 GiB is allocated by PyTorch, and 716.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
GPU memory usage before cleaning cache: 26560.31 MB
GPU memory usage after cleaning cache: 20464.31 MB
GPU memory occupied from nvmlInit: 20731 MB.
Validation dataset size: 4 for year: 0 and month: 3
Encoding tokens started...
GPU memory usage before cleaning cache: 26482.69 MB
GPU memory usage after cleaning cache: 20386.69 MB
GPU memory occupied from nvmlInit: 20731 MB.
Validation dataset size: 4 for year: 0 and month: 3
Encoding tokens started...
PAPERS: 4, TOTAL TOKENS: 126732
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.52 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
PAPERS: 4, TOTAL TOKENS: 126732
Size of latex_corpus <tokenized>   120
tokenized_dataset created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 4
})
lm_datasets created and took 0.52 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 65
})
Total number of chunks: 61
Train dataset size: 19 for year: 0 and month: 3
Encoding tokens started...
lm_datasets created and took 0.51 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 65
})
Total number of chunks: 61
Train dataset size: 19 for year: 0 and month: 3
Encoding tokens started...
PAPERS: 19, TOTAL TOKENS: 247535
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.87 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 19
})
PAPERS: 19, TOTAL TOKENS: 247535
Size of latex_corpus <tokenized>   248
tokenized_dataset created and took 1.84 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 19
})
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 271 MB.
Validation dataset size: 227 for year: 0 and month: 1
Encoding tokens started...
PAPERS: 45796, TOTAL TOKENS: 6977822
Size of latex_corpus <tokenized>   392792
tokenized_dataset created and took 1.96 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask'],
    num_rows: 45796
})
lm_datasets created and took 3.03 minutes
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47459
})
Total number of chunks: 1780
Test dataset size: 227 for year: 0 and month: 1
Encoding tokens started...
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:12,  seq_len:4096,  batch_size:1,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 4096
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 275 MB.
Validation dataset size: 227 for year: 0 and month: 1
Encoding tokens started...
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 275 MB.
device:0,  layer:10,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 275 MB.
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:11,  seq_len:2048,  batch_size:4,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 4, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:13,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 74.81 MiB is free. Including non-PyTorch memory, this process has 31.66 GiB memory in use. Of the allocated memory 29.03 GiB is allocated by PyTorch, and 2.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|     model.layers.12.self_attn.q_proj.weight     |  16777216  |
|     model.layers.12.self_attn.k_proj.weight     |  4194304   |
|     model.layers.12.self_attn.v_proj.weight     |  4194304   |
|     model.layers.12.self_attn.o_proj.weight     |  16777216  |
|       model.layers.12.mlp.gate_proj.weight      |  58720256  |
|        model.layers.12.mlp.up_proj.weight       |  58720256  |
|       model.layers.12.mlp.down_proj.weight      |  58720256  |
|      model.layers.12.input_layernorm.weight     |    4096    |
| model.layers.12.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 3081.2M parameters
Total Trainable Params: 3081.2201M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.69 MB
GPU memory usage after cleaning cache: 314.69 MB
GPU memory occupied from nvmlInit: 32693 MB.
device:0,  layer:11,  seq_len:2048,  batch_size:2,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 2, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:13,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|     model.layers.12.self_attn.q_proj.weight     |  16777216  |
|     model.layers.12.self_attn.k_proj.weight     |  4194304   |
|     model.layers.12.self_attn.v_proj.weight     |  4194304   |
|     model.layers.12.self_attn.o_proj.weight     |  16777216  |
|       model.layers.12.mlp.gate_proj.weight      |  58720256  |
|        model.layers.12.mlp.up_proj.weight       |  58720256  |
|       model.layers.12.mlp.down_proj.weight      |  58720256  |
|      model.layers.12.input_layernorm.weight     |    4096    |
| model.layers.12.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 3081.2M parameters
Total Trainable Params: 3081.2201M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 5.72 GiB. GPU 0 has a total capacity of 31.74 GiB of which 5.32 GiB is free. Including non-PyTorch memory, this process has 26.41 GiB memory in use. Of the allocated memory 23.77 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 6.64 GiB. GPU 0 has a total capacity of 31.74 GiB of which 6.53 GiB is free. Including non-PyTorch memory, this process has 25.20 GiB memory in use. Of the allocated memory 22.12 GiB is allocated by PyTorch, and 2.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
device:0,  layer:11,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:10,  seq_len:2048,  batch_size:4,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 4, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:10,  seq_len:2048,  batch_size:4,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 10, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 4, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|     model.layers.9.self_attn.q_proj.weight     |  16777216  |
|     model.layers.9.self_attn.k_proj.weight     |  4194304   |
|     model.layers.9.self_attn.v_proj.weight     |  4194304   |
|     model.layers.9.self_attn.o_proj.weight     |  16777216  |
|      model.layers.9.mlp.gate_proj.weight       |  58720256  |
|       model.layers.9.mlp.up_proj.weight        |  58720256  |
|      model.layers.9.mlp.down_proj.weight       |  58720256  |
|     model.layers.9.input_layernorm.weight      |    4096    |
| model.layers.9.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2426.9M parameters
Total Trainable Params: 2426.8841M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 108.81 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 28.35 GiB is allocated by PyTorch, and 2.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
device:0,  layer:8,  seq_len:2048,  batch_size:4,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 4, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 938.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 724.81 MiB is free. Including non-PyTorch memory, this process has 31.02 GiB memory in use. Of the allocated memory 27.02 GiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
device:0,  layer:6,  seq_len:2048,  batch_size:4,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 4, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:7,  seq_len:2048,  batch_size:4,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 4, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:8,  seq_len:2048,  batch_size:4,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 4, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 938.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 724.81 MiB is free. Including non-PyTorch memory, this process has 31.02 GiB memory in use. Of the allocated memory 27.02 GiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
device:0,  layer:9,  seq_len:2048,  batch_size:2,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 9, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 2, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|     model.layers.8.self_attn.q_proj.weight     |  16777216  |
|     model.layers.8.self_attn.k_proj.weight     |  4194304   |
|     model.layers.8.self_attn.v_proj.weight     |  4194304   |
|     model.layers.8.self_attn.o_proj.weight     |  16777216  |
|      model.layers.8.mlp.gate_proj.weight       |  58720256  |
|       model.layers.8.mlp.up_proj.weight        |  58720256  |
|      model.layers.8.mlp.down_proj.weight       |  58720256  |
|     model.layers.8.input_layernorm.weight      |    4096    |
| model.layers.8.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 2208.8M parameters
Total Trainable Params: 2208.7721M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:8,  seq_len:2048,  batch_size:4,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 4, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|     model.layers.7.self_attn.q_proj.weight     |  16777216  |
|     model.layers.7.self_attn.k_proj.weight     |  4194304   |
|     model.layers.7.self_attn.v_proj.weight     |  4194304   |
|     model.layers.7.self_attn.o_proj.weight     |  16777216  |
|      model.layers.7.mlp.gate_proj.weight       |  58720256  |
|       model.layers.7.mlp.up_proj.weight        |  58720256  |
|      model.layers.7.mlp.down_proj.weight       |  58720256  |
|     model.layers.7.input_layernorm.weight      |    4096    |
| model.layers.7.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1990.7M parameters
Total Trainable Params: 1990.6601M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:8,  seq_len:2048,  batch_size:4,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 8, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 4, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
device:0,  layer:11,  seq_len:2048,  batch_size:8,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 8, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:11,  seq_len:2048,  batch_size:16,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 16, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:11,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 11, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2645.0M parameters
Total Trainable Params: 2644.9961M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:13,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 2000, Logging steps: 2000, Save steps: 10000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|     model.layers.12.self_attn.q_proj.weight     |  16777216  |
|     model.layers.12.self_attn.k_proj.weight     |  4194304   |
|     model.layers.12.self_attn.v_proj.weight     |  4194304   |
|     model.layers.12.self_attn.o_proj.weight     |  16777216  |
|       model.layers.12.mlp.gate_proj.weight      |  58720256  |
|        model.layers.12.mlp.up_proj.weight       |  58720256  |
|       model.layers.12.mlp.down_proj.weight      |  58720256  |
|      model.layers.12.input_layernorm.weight     |    4096    |
| model.layers.12.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 3081.2M parameters
Total Trainable Params: 3081.2201M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:13,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 20, Logging steps: 20, Save steps: 20, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|     model.layers.12.self_attn.q_proj.weight     |  16777216  |
|     model.layers.12.self_attn.k_proj.weight     |  4194304   |
|     model.layers.12.self_attn.v_proj.weight     |  4194304   |
|     model.layers.12.self_attn.o_proj.weight     |  16777216  |
|       model.layers.12.mlp.gate_proj.weight      |  58720256  |
|        model.layers.12.mlp.up_proj.weight       |  58720256  |
|       model.layers.12.mlp.down_proj.weight      |  58720256  |
|      model.layers.12.input_layernorm.weight     |    4096    |
| model.layers.12.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 3081.2M parameters
Total Trainable Params: 3081.2201M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
device:0,  layer:13,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 5000, Logging steps: 2000, Save steps: 2000, Save total limit: 3, Max seq length: 2048
device:0,  layer:13,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 4000, Logging steps: 2000, Save steps: 2000, Save total limit: 3, Max seq length: 2048
device:0,  layer:13,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 3000, Logging steps: 3000, Save steps: 2000, Save total limit: 3, Max seq length: 2048
device:0,  layer:13,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Training data rows: 100
Epoch: 1, Learning rate: 0.0006, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 2000, Logging steps: 2000, Save steps: 2000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|     model.layers.12.self_attn.q_proj.weight     |  16777216  |
|     model.layers.12.self_attn.k_proj.weight     |  4194304   |
|     model.layers.12.self_attn.v_proj.weight     |  4194304   |
|     model.layers.12.self_attn.o_proj.weight     |  16777216  |
|       model.layers.12.mlp.gate_proj.weight      |  58720256  |
|        model.layers.12.mlp.up_proj.weight       |  58720256  |
|       model.layers.12.mlp.down_proj.weight      |  58720256  |
|      model.layers.12.input_layernorm.weight     |    4096    |
| model.layers.12.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 3081.2M parameters
Total Trainable Params: 3081.2201M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 581 MB.
***** Train results ***** TrainOutput(global_step=7002, training_loss=nan, metrics={'train_runtime': 42985.9752, 'train_samples_per_second': 5.213, 'train_steps_per_second': 0.163, 'train_loss': nan, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31990.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2031.7554, 'eval_samples_per_second': 23.359, 'eval_steps_per_second': 23.359, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6925, training_loss=0.0, metrics={'train_runtime': 42673.4806, 'train_samples_per_second': 5.193, 'train_steps_per_second': 0.162, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 32006.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1912.3569, 'eval_samples_per_second': 24.53, 'eval_steps_per_second': 24.53, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6968, training_loss=0.0, metrics={'train_runtime': 42454.258, 'train_samples_per_second': 5.252, 'train_steps_per_second': 0.164, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 32002.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1983.4649, 'eval_samples_per_second': 23.844, 'eval_steps_per_second': 23.844, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6877, training_loss=0.0, metrics={'train_runtime': 122226.8321, 'train_samples_per_second': 1.801, 'train_steps_per_second': 0.056, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31988.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2086.4157, 'eval_samples_per_second': 22.713, 'eval_steps_per_second': 22.713, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6963, training_loss=0.0, metrics={'train_runtime': 43111.1831, 'train_samples_per_second': 5.169, 'train_steps_per_second': 0.162, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31980.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2015.1339, 'eval_samples_per_second': 23.469, 'eval_steps_per_second': 23.469, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6946, training_loss=0.0, metrics={'train_runtime': 42384.5724, 'train_samples_per_second': 5.244, 'train_steps_per_second': 0.164, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31986.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1959.1306, 'eval_samples_per_second': 24.018, 'eval_steps_per_second': 24.018, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=7000, training_loss=0.0, metrics={'train_runtime': 43283.1423, 'train_samples_per_second': 5.176, 'train_steps_per_second': 0.162, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31980.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1937.148, 'eval_samples_per_second': 24.241, 'eval_steps_per_second': 24.241, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=7153, training_loss=0.0, metrics={'train_runtime': 46793.616, 'train_samples_per_second': 4.892, 'train_steps_per_second': 0.153, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31990.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2110.9154, 'eval_samples_per_second': 22.663, 'eval_steps_per_second': 22.663, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6895, training_loss=0.0, metrics={'train_runtime': 41691.8675, 'train_samples_per_second': 5.292, 'train_steps_per_second': 0.165, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31978.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1980.0466, 'eval_samples_per_second': 23.817, 'eval_steps_per_second': 23.817, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=7010, training_loss=0.0, metrics={'train_runtime': 44217.2213, 'train_samples_per_second': 5.074, 'train_steps_per_second': 0.159, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 32090.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2086.2849, 'eval_samples_per_second': 22.807, 'eval_steps_per_second': 22.807, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6949, training_loss=0.0, metrics={'train_runtime': 42811.2108, 'train_samples_per_second': 5.194, 'train_steps_per_second': 0.162, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31990.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2100.7671, 'eval_samples_per_second': 22.775, 'eval_steps_per_second': 22.775, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6982, training_loss=0.0, metrics={'train_runtime': 43373.1552, 'train_samples_per_second': 5.152, 'train_steps_per_second': 0.161, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31978.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1976.2565, 'eval_samples_per_second': 23.896, 'eval_steps_per_second': 23.896, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6930, training_loss=0.0, metrics={'train_runtime': 42077.5469, 'train_samples_per_second': 5.271, 'train_steps_per_second': 0.165, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 32006.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1948.1763, 'eval_samples_per_second': 24.134, 'eval_steps_per_second': 24.134, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6915, training_loss=0.0, metrics={'train_runtime': 42082.2103, 'train_samples_per_second': 5.258, 'train_steps_per_second': 0.164, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 32024.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1989.0689, 'eval_samples_per_second': 23.736, 'eval_steps_per_second': 23.736, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=7011, training_loss=0.0, metrics={'train_runtime': 43682.6633, 'train_samples_per_second': 5.136, 'train_steps_per_second': 0.16, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31988.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1986.8425, 'eval_samples_per_second': 23.77, 'eval_steps_per_second': 23.77, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6920, training_loss=0.0, metrics={'train_runtime': 42172.8742, 'train_samples_per_second': 5.251, 'train_steps_per_second': 0.164, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 32000.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1962.078, 'eval_samples_per_second': 23.986, 'eval_steps_per_second': 23.986, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6972, training_loss=0.0, metrics={'train_runtime': 43125.5816, 'train_samples_per_second': 5.174, 'train_steps_per_second': 0.162, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31978.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2017.9642, 'eval_samples_per_second': 23.495, 'eval_steps_per_second': 23.495, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6968, training_loss=0.0, metrics={'train_runtime': 42924.9105, 'train_samples_per_second': 5.195, 'train_steps_per_second': 0.162, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31980.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2044.0848, 'eval_samples_per_second': 23.099, 'eval_steps_per_second': 23.099, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6952, training_loss=0.0, metrics={'train_runtime': 41824.4877, 'train_samples_per_second': 5.319, 'train_steps_per_second': 0.166, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 32048.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1992.1102, 'eval_samples_per_second': 23.729, 'eval_steps_per_second': 23.729, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6894, training_loss=0.0, metrics={'train_runtime': 41079.9188, 'train_samples_per_second': 5.371, 'train_steps_per_second': 0.168, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31942.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1937.8025, 'eval_samples_per_second': 24.262, 'eval_steps_per_second': 24.262, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=6923, training_loss=0.0, metrics={'train_runtime': 41812.1533, 'train_samples_per_second': 5.299, 'train_steps_per_second': 0.166, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31986.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1935.948, 'eval_samples_per_second': 24.261, 'eval_steps_per_second': 24.261, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=7041, training_loss=0.0, metrics={'train_runtime': 44546.8922, 'train_samples_per_second': 5.058, 'train_steps_per_second': 0.158, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31942.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2099.7403, 'eval_samples_per_second': 22.817, 'eval_steps_per_second': 22.817, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Train results ***** TrainOutput(global_step=7018, training_loss=0.0, metrics={'train_runtime': 43569.2553, 'train_samples_per_second': 5.155, 'train_steps_per_second': 0.161, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31988.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2045.295, 'eval_samples_per_second': 23.267, 'eval_steps_per_second': 23.267, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 19343 MB.


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 1000, Logging steps: 1000, Save steps: 1000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:3,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 3, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 1000, Logging steps: 1000, Save steps: 1000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 900.1M parameters
Total Trainable Params: 900.1001M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 31670.31 MB
GPU memory usage after cleaning cache: 31670.31 MB
GPU memory occupied from nvmlInit: 31477 MB.


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 1000, Logging steps: 1000, Save steps: 1000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 1000, Logging steps: 1000, Save steps: 1000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 224091
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47459
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
***** Train results ***** TrainOutput(global_step=10, training_loss=11.056021881103515, metrics={'train_runtime': 534.2468, 'train_samples_per_second': 0.599, 'train_steps_per_second': 0.019, 'train_loss': 11.056021881103515, 'epoch': 1.0})
GPU memory usage before cleaning cache: 29186.31 MB
GPU memory usage after cleaning cache: 17432.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 10.901742935180664, 'eval_runtime': 121.2801, 'eval_samples_per_second': 2.639, 'eval_steps_per_second': 2.639, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18254.31 MB
GPU memory usage after cleaning cache: 17432.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
***** Train results ***** TrainOutput(global_step=10, training_loss=10.852947235107422, metrics={'train_runtime': 569.648, 'train_samples_per_second': 0.562, 'train_steps_per_second': 0.018, 'train_loss': 10.852947235107422, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30352.31 MB
GPU memory usage after cleaning cache: 17432.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 10.690185546875, 'eval_runtime': 121.4459, 'eval_samples_per_second': 2.635, 'eval_steps_per_second': 2.635, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18254.31 MB
GPU memory usage after cleaning cache: 17432.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
***** Train results ***** TrainOutput(global_step=10, training_loss=10.625662994384765, metrics={'train_runtime': 586.7941, 'train_samples_per_second': 0.545, 'train_steps_per_second': 0.017, 'train_loss': 10.625662994384765, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30352.31 MB
GPU memory usage after cleaning cache: 17432.31 MB
GPU memory occupied from nvmlInit: 32741 MB.
***** Eval results ***** {'eval_loss': 10.423089981079102, 'eval_runtime': 123.0486, 'eval_samples_per_second': 2.601, 'eval_steps_per_second': 2.601, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18250.31 MB
GPU memory usage after cleaning cache: 17432.31 MB
GPU memory occupied from nvmlInit: 32741 MB.
Training for 0-4
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_4_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
***** Train results ***** TrainOutput(global_step=10, training_loss=10.26239013671875, metrics={'train_runtime': 594.8571, 'train_samples_per_second': 0.538, 'train_steps_per_second': 0.017, 'train_loss': 10.26239013671875, 'epoch': 1.0})
GPU memory usage before cleaning cache: 29902.31 MB
GPU memory usage after cleaning cache: 17432.31 MB
GPU memory occupied from nvmlInit: 32741 MB.
***** Eval results ***** {'eval_loss': 10.839936256408691, 'eval_runtime': 127.7756, 'eval_samples_per_second': 2.504, 'eval_steps_per_second': 2.504, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18252.31 MB
GPU memory usage after cleaning cache: 17432.31 MB
GPU memory occupied from nvmlInit: 32741 MB.
Training for 0-5
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_5_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_5_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
***** Train results ***** TrainOutput(global_step=10, training_loss=10.264179992675782, metrics={'train_runtime': 591.1041, 'train_samples_per_second': 0.541, 'train_steps_per_second': 0.017, 'train_loss': 10.264179992675782, 'epoch': 1.0})
GPU memory usage before cleaning cache: 29902.31 MB
GPU memory usage after cleaning cache: 17432.31 MB
GPU memory occupied from nvmlInit: 32741 MB.
***** Eval results ***** {'eval_loss': 9.875460624694824, 'eval_runtime': 120.5842, 'eval_samples_per_second': 2.654, 'eval_steps_per_second': 2.654, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18252.31 MB
GPU memory usage after cleaning cache: 17432.31 MB
GPU memory occupied from nvmlInit: 32741 MB.
Training for 0-6
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_6_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_6_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
***** Train results ***** TrainOutput(global_step=10, training_loss=9.82052993774414, metrics={'train_runtime': 581.0669, 'train_samples_per_second': 0.551, 'train_steps_per_second': 0.017, 'train_loss': 9.82052993774414, 'epoch': 1.0})
GPU memory usage before cleaning cache: 29884.31 MB
GPU memory usage after cleaning cache: 17432.31 MB
GPU memory occupied from nvmlInit: 32741 MB.
***** Eval results ***** {'eval_loss': 9.6425199508667, 'eval_runtime': 3069.0796, 'eval_samples_per_second': 0.104, 'eval_steps_per_second': 0.104, 'epoch': 1.0}


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:1,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 1, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 463.9M parameters
Total Trainable Params: 463.8761M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 12116.31 MB
GPU memory usage after cleaning cache: 12116.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=11.08588104248047, metrics={'train_runtime': 124.3621, 'train_samples_per_second': 2.573, 'train_steps_per_second': 0.08, 'train_loss': 11.08588104248047, 'epoch': 1.0})
GPU memory usage before cleaning cache: 21960.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 10.981001853942871, 'eval_runtime': 1.8764, 'eval_samples_per_second': 5.329, 'eval_steps_per_second': 5.329, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=10.92519760131836, metrics={'train_runtime': 131.0013, 'train_samples_per_second': 2.443, 'train_steps_per_second': 0.076, 'train_loss': 10.92519760131836, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22332.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 10.739563941955566, 'eval_runtime': 1.144, 'eval_samples_per_second': 8.741, 'eval_steps_per_second': 8.741, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=10.756111145019531, metrics={'train_runtime': 153.4983, 'train_samples_per_second': 2.085, 'train_steps_per_second': 0.065, 'train_loss': 10.756111145019531, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22332.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 10.625699996948242, 'eval_runtime': 1.1417, 'eval_samples_per_second': 8.759, 'eval_steps_per_second': 8.759, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-4
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_4_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=10.517933654785157, metrics={'train_runtime': 130.795, 'train_samples_per_second': 2.447, 'train_steps_per_second': 0.076, 'train_loss': 10.517933654785157, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22322.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 10.943838119506836, 'eval_runtime': 1.1533, 'eval_samples_per_second': 8.671, 'eval_steps_per_second': 8.671, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-5
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_5_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_5_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=10.534432220458985, metrics={'train_runtime': 132.4995, 'train_samples_per_second': 2.415, 'train_steps_per_second': 0.075, 'train_loss': 10.534432220458985, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22322.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 10.432711601257324, 'eval_runtime': 0.9492, 'eval_samples_per_second': 10.535, 'eval_steps_per_second': 10.535, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18968.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-6
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_6_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_6_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=10.215601348876953, metrics={'train_runtime': 133.3094, 'train_samples_per_second': 2.4, 'train_steps_per_second': 0.075, 'train_loss': 10.215601348876953, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22320.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 10.264602661132812, 'eval_runtime': 1.0607, 'eval_samples_per_second': 9.428, 'eval_steps_per_second': 9.428, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-7
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_7_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=10.032538604736327, metrics={'train_runtime': 133.6978, 'train_samples_per_second': 2.393, 'train_steps_per_second': 0.075, 'train_loss': 10.032538604736327, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22322.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 9.835820198059082, 'eval_runtime': 1.1712, 'eval_samples_per_second': 8.538, 'eval_steps_per_second': 8.538, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18968.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-8
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_8_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_8_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=10.07645034790039, metrics={'train_runtime': 131.421, 'train_samples_per_second': 2.435, 'train_steps_per_second': 0.076, 'train_loss': 10.07645034790039, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22320.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 9.543509483337402, 'eval_runtime': 1.0878, 'eval_samples_per_second': 9.193, 'eval_steps_per_second': 9.193, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-9
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_9_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_9_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=9.955860137939453, metrics={'train_runtime': 131.8353, 'train_samples_per_second': 2.427, 'train_steps_per_second': 0.076, 'train_loss': 9.955860137939453, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22326.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 9.801454544067383, 'eval_runtime': 1.0917, 'eval_samples_per_second': 9.16, 'eval_steps_per_second': 9.16, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-10
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_10_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_10_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=9.45269546508789, metrics={'train_runtime': 135.8869, 'train_samples_per_second': 2.355, 'train_steps_per_second': 0.074, 'train_loss': 9.45269546508789, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22320.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 9.505555152893066, 'eval_runtime': 1.0682, 'eval_samples_per_second': 9.362, 'eval_steps_per_second': 9.362, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-11
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_11_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_11_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=10.512096405029297, metrics={'train_runtime': 134.2275, 'train_samples_per_second': 2.384, 'train_steps_per_second': 0.075, 'train_loss': 10.512096405029297, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22324.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 9.594738960266113, 'eval_runtime': 1.1477, 'eval_samples_per_second': 8.713, 'eval_steps_per_second': 8.713, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-12
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_12_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_12_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=9.385856628417969, metrics={'train_runtime': 130.146, 'train_samples_per_second': 2.459, 'train_steps_per_second': 0.077, 'train_loss': 9.385856628417969, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22320.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 9.38286018371582, 'eval_runtime': 1.0354, 'eval_samples_per_second': 9.658, 'eval_steps_per_second': 9.658, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=9.209186553955078, metrics={'train_runtime': 131.4424, 'train_samples_per_second': 2.435, 'train_steps_per_second': 0.076, 'train_loss': 9.209186553955078, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22320.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 8.865325927734375, 'eval_runtime': 1.0888, 'eval_samples_per_second': 9.184, 'eval_steps_per_second': 9.184, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18968.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=9.138448333740234, metrics={'train_runtime': 133.2064, 'train_samples_per_second': 2.402, 'train_steps_per_second': 0.075, 'train_loss': 9.138448333740234, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22326.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 8.649900436401367, 'eval_runtime': 1.1919, 'eval_samples_per_second': 8.39, 'eval_steps_per_second': 8.39, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=8.96243896484375, metrics={'train_runtime': 169.4225, 'train_samples_per_second': 1.889, 'train_steps_per_second': 0.059, 'train_loss': 8.96243896484375, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22326.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 8.636340141296387, 'eval_runtime': 1.0673, 'eval_samples_per_second': 9.369, 'eval_steps_per_second': 9.369, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-4
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_4_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=8.779737091064453, metrics={'train_runtime': 129.7963, 'train_samples_per_second': 2.465, 'train_steps_per_second': 0.077, 'train_loss': 8.779737091064453, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22424.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 9.071035385131836, 'eval_runtime': 1.0412, 'eval_samples_per_second': 9.604, 'eval_steps_per_second': 9.604, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-5
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_5_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_5_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=8.434947204589843, metrics={'train_runtime': 133.6209, 'train_samples_per_second': 2.395, 'train_steps_per_second': 0.075, 'train_loss': 8.434947204589843, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22332.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 9.133508682250977, 'eval_runtime': 0.9223, 'eval_samples_per_second': 10.843, 'eval_steps_per_second': 10.843, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-6
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_6_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_6_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=8.219346618652343, metrics={'train_runtime': 134.6828, 'train_samples_per_second': 2.376, 'train_steps_per_second': 0.074, 'train_loss': 8.219346618652343, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22318.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 8.265251159667969, 'eval_runtime': 1.0725, 'eval_samples_per_second': 9.324, 'eval_steps_per_second': 9.324, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-7
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_7_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=8.5009033203125, metrics={'train_runtime': 134.1668, 'train_samples_per_second': 2.385, 'train_steps_per_second': 0.075, 'train_loss': 8.5009033203125, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22332.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 8.579306602478027, 'eval_runtime': 1.043, 'eval_samples_per_second': 9.587, 'eval_steps_per_second': 9.587, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-8
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_8_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_8_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=8.300059509277343, metrics={'train_runtime': 127.9128, 'train_samples_per_second': 2.502, 'train_steps_per_second': 0.078, 'train_loss': 8.300059509277343, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22306.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 8.185470581054688, 'eval_runtime': 1.2912, 'eval_samples_per_second': 7.745, 'eval_steps_per_second': 7.745, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-9
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_9_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_9_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=8.282926940917969, metrics={'train_runtime': 130.419, 'train_samples_per_second': 2.454, 'train_steps_per_second': 0.077, 'train_loss': 8.282926940917969, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22332.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.79656982421875, 'eval_runtime': 1.2624, 'eval_samples_per_second': 7.922, 'eval_steps_per_second': 7.922, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-10
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_10_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_10_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=8.146751403808594, metrics={'train_runtime': 129.045, 'train_samples_per_second': 2.48, 'train_steps_per_second': 0.077, 'train_loss': 8.146751403808594, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22590.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.832283973693848, 'eval_runtime': 1.1102, 'eval_samples_per_second': 9.008, 'eval_steps_per_second': 9.008, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-11
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_11_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_11_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.965321350097656, metrics={'train_runtime': 134.9761, 'train_samples_per_second': 2.371, 'train_steps_per_second': 0.074, 'train_loss': 7.965321350097656, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22326.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.558880805969238, 'eval_runtime': 1.1467, 'eval_samples_per_second': 8.721, 'eval_steps_per_second': 8.721, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-12
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_12_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_12_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.754170227050781, metrics={'train_runtime': 135.8172, 'train_samples_per_second': 2.356, 'train_steps_per_second': 0.074, 'train_loss': 7.754170227050781, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22318.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.7054948806762695, 'eval_runtime': 1.1869, 'eval_samples_per_second': 8.426, 'eval_steps_per_second': 8.426, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.842838287353516, metrics={'train_runtime': 132.4515, 'train_samples_per_second': 2.416, 'train_steps_per_second': 0.075, 'train_loss': 7.842838287353516, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22226.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.766085147857666, 'eval_runtime': 1.3426, 'eval_samples_per_second': 7.448, 'eval_steps_per_second': 7.448, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.715802764892578, metrics={'train_runtime': 131.0069, 'train_samples_per_second': 2.443, 'train_steps_per_second': 0.076, 'train_loss': 7.715802764892578, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22324.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.8519697189331055, 'eval_runtime': 1.1457, 'eval_samples_per_second': 8.728, 'eval_steps_per_second': 8.728, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.955517578125, metrics={'train_runtime': 133.7491, 'train_samples_per_second': 2.393, 'train_steps_per_second': 0.075, 'train_loss': 7.955517578125, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22324.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.389235496520996, 'eval_runtime': 1.152, 'eval_samples_per_second': 8.681, 'eval_steps_per_second': 8.681, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-4
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_4_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.504740905761719, metrics={'train_runtime': 133.7975, 'train_samples_per_second': 2.392, 'train_steps_per_second': 0.075, 'train_loss': 7.504740905761719, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22332.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.55602502822876, 'eval_runtime': 1.1372, 'eval_samples_per_second': 8.794, 'eval_steps_per_second': 8.794, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-5
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_5_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_5_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.4332130432128904, metrics={'train_runtime': 133.9927, 'train_samples_per_second': 2.388, 'train_steps_per_second': 0.075, 'train_loss': 7.4332130432128904, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22326.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.444685935974121, 'eval_runtime': 1.1718, 'eval_samples_per_second': 8.534, 'eval_steps_per_second': 8.534, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-6
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_6_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_6_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.414232635498047, metrics={'train_runtime': 132.9825, 'train_samples_per_second': 2.406, 'train_steps_per_second': 0.075, 'train_loss': 7.414232635498047, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22320.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.540820121765137, 'eval_runtime': 1.1511, 'eval_samples_per_second': 8.687, 'eval_steps_per_second': 8.687, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-7
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_7_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.907048034667969, metrics={'train_runtime': 132.7862, 'train_samples_per_second': 2.41, 'train_steps_per_second': 0.075, 'train_loss': 7.907048034667969, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22320.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.813271999359131, 'eval_runtime': 1.1617, 'eval_samples_per_second': 8.608, 'eval_steps_per_second': 8.608, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-8
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_8_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_8_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.475544738769531, metrics={'train_runtime': 132.6833, 'train_samples_per_second': 2.412, 'train_steps_per_second': 0.075, 'train_loss': 7.475544738769531, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22326.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.420926570892334, 'eval_runtime': 1.0627, 'eval_samples_per_second': 9.41, 'eval_steps_per_second': 9.41, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-9
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_9_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_9_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.837909698486328, metrics={'train_runtime': 131.6176, 'train_samples_per_second': 2.431, 'train_steps_per_second': 0.076, 'train_loss': 7.837909698486328, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22326.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.10336971282959, 'eval_runtime': 1.1538, 'eval_samples_per_second': 8.667, 'eval_steps_per_second': 8.667, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-10
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_10_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_10_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.090788269042969, metrics={'train_runtime': 135.3091, 'train_samples_per_second': 2.365, 'train_steps_per_second': 0.074, 'train_loss': 7.090788269042969, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22332.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.92090368270874, 'eval_runtime': 1.0711, 'eval_samples_per_second': 9.337, 'eval_steps_per_second': 9.337, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-11
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_11_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_11_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.4900154113769535, metrics={'train_runtime': 134.0682, 'train_samples_per_second': 2.387, 'train_steps_per_second': 0.075, 'train_loss': 7.4900154113769535, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22326.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.209722995758057, 'eval_runtime': 1.0944, 'eval_samples_per_second': 9.138, 'eval_steps_per_second': 9.138, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-12
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_12_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_12_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.171936798095703, metrics={'train_runtime': 136.1652, 'train_samples_per_second': 2.35, 'train_steps_per_second': 0.073, 'train_loss': 7.171936798095703, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22324.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.530148983001709, 'eval_runtime': 1.1074, 'eval_samples_per_second': 9.03, 'eval_steps_per_second': 9.03, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.241844177246094, metrics={'train_runtime': 130.5926, 'train_samples_per_second': 2.45, 'train_steps_per_second': 0.077, 'train_loss': 7.241844177246094, 'epoch': 1.0})
GPU memory usage before cleaning cache: 22332.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.199798583984375, 'eval_runtime': 1.1501, 'eval_samples_per_second': 8.695, 'eval_steps_per_second': 8.695, 'epoch': 1.0}
GPU memory usage before cleaning cache: 18966.31 MB
GPU memory usage after cleaning cache: 18028.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.392284393310547, metrics={'train_runtime': 134.3036, 'train_samples_per_second': 2.383, 'train_steps_per_second': 0.074, 'train_loss': 7.392284393310547, 'epoch': 1.0})
GPU memory usage before cleaning cache: 29846.31 MB
GPU memory usage after cleaning cache: 25552.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.204108238220215, 'eval_runtime': 1.0751, 'eval_samples_per_second': 9.302, 'eval_steps_per_second': 9.302, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26490.31 MB
GPU memory usage after cleaning cache: 25552.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.135623168945313, metrics={'train_runtime': 135.8647, 'train_samples_per_second': 2.355, 'train_steps_per_second': 0.074, 'train_loss': 7.135623168945313, 'epoch': 1.0})
GPU memory usage before cleaning cache: 29850.31 MB
GPU memory usage after cleaning cache: 25552.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.86728572845459, 'eval_runtime': 1.164, 'eval_samples_per_second': 8.591, 'eval_steps_per_second': 8.591, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26490.31 MB
GPU memory usage after cleaning cache: 25552.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-4
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_4_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.639476013183594, metrics={'train_runtime': 135.7804, 'train_samples_per_second': 2.357, 'train_steps_per_second': 0.074, 'train_loss': 7.639476013183594, 'epoch': 1.0})
GPU memory usage before cleaning cache: 29844.31 MB
GPU memory usage after cleaning cache: 25552.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.337533473968506, 'eval_runtime': 1.0701, 'eval_samples_per_second': 9.345, 'eval_steps_per_second': 9.345, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26490.31 MB
GPU memory usage after cleaning cache: 25552.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-5
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_5_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_5_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.970265197753906, metrics={'train_runtime': 135.2456, 'train_samples_per_second': 2.366, 'train_steps_per_second': 0.074, 'train_loss': 6.970265197753906, 'epoch': 1.0})
GPU memory usage before cleaning cache: 29844.31 MB
GPU memory usage after cleaning cache: 25552.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.3256988525390625, 'eval_runtime': 1.1849, 'eval_samples_per_second': 8.44, 'eval_steps_per_second': 8.44, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26490.31 MB
GPU memory usage after cleaning cache: 25552.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-6
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_6_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_6_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.095700073242187, metrics={'train_runtime': 143.634, 'train_samples_per_second': 2.228, 'train_steps_per_second': 0.07, 'train_loss': 7.095700073242187, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30052.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.14031982421875, 'eval_runtime': 1.8977, 'eval_samples_per_second': 5.27, 'eval_steps_per_second': 5.27, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26690.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-7
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_7_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.002973937988282, metrics={'train_runtime': 130.8071, 'train_samples_per_second': 2.446, 'train_steps_per_second': 0.076, 'train_loss': 7.002973937988282, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30054.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.829310417175293, 'eval_runtime': 1.1521, 'eval_samples_per_second': 8.68, 'eval_steps_per_second': 8.68, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26688.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-8
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_8_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_8_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.9484703063964846, metrics={'train_runtime': 133.3579, 'train_samples_per_second': 2.4, 'train_steps_per_second': 0.075, 'train_loss': 6.9484703063964846, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30048.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.838756561279297, 'eval_runtime': 1.1445, 'eval_samples_per_second': 8.738, 'eval_steps_per_second': 8.738, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26688.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-9
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_9_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_9_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.247166442871094, metrics={'train_runtime': 142.0463, 'train_samples_per_second': 2.253, 'train_steps_per_second': 0.07, 'train_loss': 7.247166442871094, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30042.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.669739723205566, 'eval_runtime': 1.1041, 'eval_samples_per_second': 9.057, 'eval_steps_per_second': 9.057, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26690.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-10
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_10_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_10_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.910746765136719, metrics={'train_runtime': 133.9915, 'train_samples_per_second': 2.388, 'train_steps_per_second': 0.075, 'train_loss': 6.910746765136719, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30048.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.522822380065918, 'eval_runtime': 1.1541, 'eval_samples_per_second': 8.665, 'eval_steps_per_second': 8.665, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26688.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-11
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_11_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_11_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.876390838623047, metrics={'train_runtime': 132.0408, 'train_samples_per_second': 2.423, 'train_steps_per_second': 0.076, 'train_loss': 6.876390838623047, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30054.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.394343376159668, 'eval_runtime': 1.3196, 'eval_samples_per_second': 7.578, 'eval_steps_per_second': 7.578, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26688.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-12
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_12_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_12_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.814959716796875, metrics={'train_runtime': 143.3327, 'train_samples_per_second': 2.233, 'train_steps_per_second': 0.07, 'train_loss': 6.814959716796875, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30054.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.6578545570373535, 'eval_runtime': 1.2564, 'eval_samples_per_second': 7.959, 'eval_steps_per_second': 7.959, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26688.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 4-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_4_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_4_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=7.753936767578125, metrics={'train_runtime': 134.2595, 'train_samples_per_second': 2.383, 'train_steps_per_second': 0.074, 'train_loss': 7.753936767578125, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30048.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.701019287109375, 'eval_runtime': 1.1824, 'eval_samples_per_second': 8.457, 'eval_steps_per_second': 8.457, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26688.31 MB
GPU memory usage after cleaning cache: 25750.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 4-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_4_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_4_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.872737884521484, metrics={'train_runtime': 132.7136, 'train_samples_per_second': 2.411, 'train_steps_per_second': 0.075, 'train_loss': 6.872737884521484, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30054.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.777541160583496, 'eval_runtime': 1.2621, 'eval_samples_per_second': 7.924, 'eval_steps_per_second': 7.924, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26692.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 4-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_4_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_4_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.540650939941406, metrics={'train_runtime': 136.6491, 'train_samples_per_second': 2.342, 'train_steps_per_second': 0.073, 'train_loss': 6.540650939941406, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30050.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.242254734039307, 'eval_runtime': 1.166, 'eval_samples_per_second': 8.576, 'eval_steps_per_second': 8.576, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26690.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 4-4
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_4_4_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_4_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.8734580993652346, metrics={'train_runtime': 134.8638, 'train_samples_per_second': 2.373, 'train_steps_per_second': 0.074, 'train_loss': 6.8734580993652346, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30044.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.7101593017578125, 'eval_runtime': 1.0531, 'eval_samples_per_second': 9.496, 'eval_steps_per_second': 9.496, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26692.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 4-5
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_4_5_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_4_5_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.575127410888672, metrics={'train_runtime': 133.4963, 'train_samples_per_second': 2.397, 'train_steps_per_second': 0.075, 'train_loss': 6.575127410888672, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30048.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.445883274078369, 'eval_runtime': 1.3187, 'eval_samples_per_second': 7.583, 'eval_steps_per_second': 7.583, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26690.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 4-6
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_4_6_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_4_6_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.676268768310547, metrics={'train_runtime': 132.9035, 'train_samples_per_second': 2.408, 'train_steps_per_second': 0.075, 'train_loss': 6.676268768310547, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30056.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 7.158106327056885, 'eval_runtime': 1.1268, 'eval_samples_per_second': 8.875, 'eval_steps_per_second': 8.875, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26690.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 4-7
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_4_7_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_4_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.761418151855469, metrics={'train_runtime': 133.9993, 'train_samples_per_second': 2.388, 'train_steps_per_second': 0.075, 'train_loss': 6.761418151855469, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30046.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.521286964416504, 'eval_runtime': 1.1887, 'eval_samples_per_second': 8.413, 'eval_steps_per_second': 8.413, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26690.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 4-8
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_4_8_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_4_8_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.604871368408203, metrics={'train_runtime': 135.1838, 'train_samples_per_second': 2.367, 'train_steps_per_second': 0.074, 'train_loss': 6.604871368408203, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30044.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.671394348144531, 'eval_runtime': 1.1538, 'eval_samples_per_second': 8.667, 'eval_steps_per_second': 8.667, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26690.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 4-9
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_4_9_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_4_9_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.62364501953125, metrics={'train_runtime': 133.1519, 'train_samples_per_second': 2.403, 'train_steps_per_second': 0.075, 'train_loss': 6.62364501953125, 'epoch': 1.0})
GPU memory usage before cleaning cache: 29944.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': 6.573561668395996, 'eval_runtime': 1.0012, 'eval_samples_per_second': 9.988, 'eval_steps_per_second': 9.988, 'epoch': 1.0}
GPU memory usage before cleaning cache: 26690.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 4-10
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_4_10_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_4_10_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.631275939941406, metrics={'train_runtime': 128.7785, 'train_samples_per_second': 2.485, 'train_steps_per_second': 0.078, 'train_loss': 6.631275939941406, 'epoch': 1.0})
GPU memory usage before cleaning cache: 30056.31 MB
GPU memory usage after cleaning cache: 25752.31 MB
GPU memory occupied from nvmlInit: 31477 MB.


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=5.4709780154764494e+23, metrics={'train_runtime': 121.1681, 'train_samples_per_second': 2.641, 'train_steps_per_second': 0.083, 'train_loss': 5.4709780154764494e+23, 'epoch': 1.0})
GPU memory usage before cleaning cache: 14928.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7646, 'eval_samples_per_second': 13.078, 'eval_steps_per_second': 13.078, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9414.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
Error occured while training the model ???????????????????????????????????
Attempting to unscale FP16 gradients.


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=11.056029510498046, metrics={'train_runtime': 173.4125, 'train_samples_per_second': 1.845, 'train_steps_per_second': 0.058, 'train_loss': 11.056029510498046, 'epoch': 1.0})
GPU memory usage before cleaning cache: 29898.31 MB
GPU memory usage after cleaning cache: 17372.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7962, 'eval_samples_per_second': 12.559, 'eval_steps_per_second': 12.559, 'epoch': 1.0}
GPU memory usage before cleaning cache: 17622.31 MB
GPU memory usage after cleaning cache: 13970.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
Error occured while training the model ???????????????????????????????????
Attempting to unscale FP16 gradients.


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
Error occured while training the model ???????????????????????????????????
Attempting to unscale FP16 gradients.


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.1,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
Error occured while training the model ???????????????????????????????????
Attempting to unscale FP16 gradients.


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.1,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=5.4709780154764494e+23, metrics={'train_runtime': 122.3433, 'train_samples_per_second': 2.616, 'train_steps_per_second': 0.082, 'train_loss': 5.4709780154764494e+23, 'epoch': 1.0})
GPU memory usage before cleaning cache: 14928.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7008, 'eval_samples_per_second': 14.269, 'eval_steps_per_second': 14.269, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9414.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 114.9375, 'train_samples_per_second': 2.784, 'train_steps_per_second': 0.087, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8052, 'eval_samples_per_second': 12.42, 'eval_steps_per_second': 12.42, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_3_datasets.pkl


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.1,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.1,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.1,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=5.4709780154764494e+23, metrics={'train_runtime': 122.9845, 'train_samples_per_second': 2.602, 'train_steps_per_second': 0.081, 'train_loss': 5.4709780154764494e+23, 'epoch': 1.0})
GPU memory usage before cleaning cache: 14928.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7426, 'eval_samples_per_second': 13.467, 'eval_steps_per_second': 13.467, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9414.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 119.7206, 'train_samples_per_second': 2.673, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8315, 'eval_samples_per_second': 12.026, 'eval_steps_per_second': 12.026, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 119.1387, 'train_samples_per_second': 2.686, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15476.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8385, 'eval_samples_per_second': 11.926, 'eval_steps_per_second': 11.926, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-4
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_4_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 116.6706, 'train_samples_per_second': 2.743, 'train_steps_per_second': 0.086, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15284.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.853, 'eval_samples_per_second': 11.724, 'eval_steps_per_second': 11.724, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-5
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_5_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_5_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 123.2168, 'train_samples_per_second': 2.597, 'train_steps_per_second': 0.081, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15290.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7117, 'eval_samples_per_second': 14.05, 'eval_steps_per_second': 14.05, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9406.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-6
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_6_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_6_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 116.1125, 'train_samples_per_second': 2.756, 'train_steps_per_second': 0.086, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15292.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7814, 'eval_samples_per_second': 12.798, 'eval_steps_per_second': 12.798, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-7
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_7_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 118.7813, 'train_samples_per_second': 2.694, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15488.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7576, 'eval_samples_per_second': 13.199, 'eval_steps_per_second': 13.199, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9408.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-8
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_8_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_8_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 121.3776, 'train_samples_per_second': 2.636, 'train_steps_per_second': 0.082, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15488.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7979, 'eval_samples_per_second': 12.533, 'eval_steps_per_second': 12.533, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9414.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-9
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_9_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_9_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 119.4987, 'train_samples_per_second': 2.678, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7619, 'eval_samples_per_second': 13.124, 'eval_steps_per_second': 13.124, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-10
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_10_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_10_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 119.4662, 'train_samples_per_second': 2.679, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15470.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8928, 'eval_samples_per_second': 11.201, 'eval_steps_per_second': 11.201, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-11
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_11_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_11_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 121.0692, 'train_samples_per_second': 2.643, 'train_steps_per_second': 0.083, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15442.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8438, 'eval_samples_per_second': 11.851, 'eval_steps_per_second': 11.851, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-12
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_12_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_12_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 115.9004, 'train_samples_per_second': 2.761, 'train_steps_per_second': 0.086, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15470.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8019, 'eval_samples_per_second': 12.47, 'eval_steps_per_second': 12.47, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9410.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 120.8235, 'train_samples_per_second': 2.648, 'train_steps_per_second': 0.083, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15472.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7878, 'eval_samples_per_second': 12.694, 'eval_steps_per_second': 12.694, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9406.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 119.5545, 'train_samples_per_second': 2.677, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15482.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8117, 'eval_samples_per_second': 12.32, 'eval_steps_per_second': 12.32, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 118.5349, 'train_samples_per_second': 2.7, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15480.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8684, 'eval_samples_per_second': 11.515, 'eval_steps_per_second': 11.515, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9416.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-4
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_4_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.1,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.1,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.1,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=5.4709780154764494e+23, metrics={'train_runtime': 119.3263, 'train_samples_per_second': 2.682, 'train_steps_per_second': 0.084, 'train_loss': 5.4709780154764494e+23, 'epoch': 1.0})
GPU memory usage before cleaning cache: 14928.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7295, 'eval_samples_per_second': 13.707, 'eval_steps_per_second': 13.707, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9414.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 114.3655, 'train_samples_per_second': 2.798, 'train_steps_per_second': 0.087, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8778, 'eval_samples_per_second': 11.393, 'eval_steps_per_second': 11.393, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 119.0862, 'train_samples_per_second': 2.687, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15476.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8246, 'eval_samples_per_second': 12.127, 'eval_steps_per_second': 12.127, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-4
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_4_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 119.5951, 'train_samples_per_second': 2.676, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15284.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8388, 'eval_samples_per_second': 11.922, 'eval_steps_per_second': 11.922, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-5
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_5_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_5_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 125.285, 'train_samples_per_second': 2.554, 'train_steps_per_second': 0.08, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15290.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.6821, 'eval_samples_per_second': 14.661, 'eval_steps_per_second': 14.661, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9406.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-6
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_6_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_6_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 117.2126, 'train_samples_per_second': 2.73, 'train_steps_per_second': 0.085, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15292.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7692, 'eval_samples_per_second': 13.0, 'eval_steps_per_second': 13.0, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-7
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_7_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 118.4358, 'train_samples_per_second': 2.702, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15488.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8934, 'eval_samples_per_second': 11.194, 'eval_steps_per_second': 11.194, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9408.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-8
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_8_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_8_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 119.0124, 'train_samples_per_second': 2.689, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15488.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7769, 'eval_samples_per_second': 12.872, 'eval_steps_per_second': 12.872, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9414.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-9
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_9_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_9_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 122.6809, 'train_samples_per_second': 2.608, 'train_steps_per_second': 0.082, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.757, 'eval_samples_per_second': 13.21, 'eval_steps_per_second': 13.21, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-10
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_10_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_10_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 125.4115, 'train_samples_per_second': 2.552, 'train_steps_per_second': 0.08, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15470.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.774, 'eval_samples_per_second': 12.92, 'eval_steps_per_second': 12.92, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-11
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_11_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_11_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 116.3634, 'train_samples_per_second': 2.75, 'train_steps_per_second': 0.086, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15442.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8175, 'eval_samples_per_second': 12.233, 'eval_steps_per_second': 12.233, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-12
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_12_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_12_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 117.1866, 'train_samples_per_second': 2.731, 'train_steps_per_second': 0.085, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15470.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7248, 'eval_samples_per_second': 13.797, 'eval_steps_per_second': 13.797, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9410.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 123.2796, 'train_samples_per_second': 2.596, 'train_steps_per_second': 0.081, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15472.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8176, 'eval_samples_per_second': 12.231, 'eval_steps_per_second': 12.231, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9406.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 119.2378, 'train_samples_per_second': 2.684, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15482.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.831, 'eval_samples_per_second': 12.034, 'eval_steps_per_second': 12.034, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 119.1128, 'train_samples_per_second': 2.687, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15480.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.891, 'eval_samples_per_second': 11.223, 'eval_steps_per_second': 11.223, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9416.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-4
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_4_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 120.4938, 'train_samples_per_second': 2.656, 'train_steps_per_second': 0.083, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15504.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7367, 'eval_samples_per_second': 13.575, 'eval_steps_per_second': 13.575, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9408.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-5
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_5_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_5_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 125.6121, 'train_samples_per_second': 2.548, 'train_steps_per_second': 0.08, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15472.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.6889, 'eval_samples_per_second': 14.516, 'eval_steps_per_second': 14.516, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9408.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-6
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_6_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_6_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 126.995, 'train_samples_per_second': 2.52, 'train_steps_per_second': 0.079, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15284.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8183, 'eval_samples_per_second': 12.221, 'eval_steps_per_second': 12.221, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9414.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-7
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_7_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 140.4511, 'train_samples_per_second': 2.278, 'train_steps_per_second': 0.071, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15488.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8436, 'eval_samples_per_second': 11.854, 'eval_steps_per_second': 11.854, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9404.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-8
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_8_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_8_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 122.9597, 'train_samples_per_second': 2.602, 'train_steps_per_second': 0.081, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15650.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8213, 'eval_samples_per_second': 12.176, 'eval_steps_per_second': 12.176, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-9
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_9_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_9_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 133.2945, 'train_samples_per_second': 2.401, 'train_steps_per_second': 0.075, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8113, 'eval_samples_per_second': 12.325, 'eval_steps_per_second': 12.325, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-10
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_10_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_10_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 155.3896, 'train_samples_per_second': 2.059, 'train_steps_per_second': 0.064, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15708.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7916, 'eval_samples_per_second': 12.632, 'eval_steps_per_second': 12.632, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-11
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_11_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_11_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 123.9643, 'train_samples_per_second': 2.581, 'train_steps_per_second': 0.081, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15460.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7696, 'eval_samples_per_second': 12.994, 'eval_steps_per_second': 12.994, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 1-12
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_1_12_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_1_12_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 121.6968, 'train_samples_per_second': 2.629, 'train_steps_per_second': 0.082, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15454.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.836, 'eval_samples_per_second': 11.961, 'eval_steps_per_second': 11.961, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 121.0856, 'train_samples_per_second': 2.643, 'train_steps_per_second': 0.083, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15560.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.827, 'eval_samples_per_second': 12.091, 'eval_steps_per_second': 12.091, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 114.6859, 'train_samples_per_second': 2.79, 'train_steps_per_second': 0.087, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15472.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8924, 'eval_samples_per_second': 11.206, 'eval_steps_per_second': 11.206, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 118.6847, 'train_samples_per_second': 2.696, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8888, 'eval_samples_per_second': 11.251, 'eval_steps_per_second': 11.251, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9414.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-4
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_4_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 122.7184, 'train_samples_per_second': 2.608, 'train_steps_per_second': 0.081, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15482.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7432, 'eval_samples_per_second': 13.455, 'eval_steps_per_second': 13.455, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9410.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-5
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_5_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_5_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 116.3266, 'train_samples_per_second': 2.751, 'train_steps_per_second': 0.086, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.848, 'eval_samples_per_second': 11.792, 'eval_steps_per_second': 11.792, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-6
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_6_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_6_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 116.4391, 'train_samples_per_second': 2.748, 'train_steps_per_second': 0.086, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15470.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8139, 'eval_samples_per_second': 12.286, 'eval_steps_per_second': 12.286, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-7
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_7_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 125.4733, 'train_samples_per_second': 2.55, 'train_steps_per_second': 0.08, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15470.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8164, 'eval_samples_per_second': 12.248, 'eval_steps_per_second': 12.248, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-8
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_8_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_8_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 125.1168, 'train_samples_per_second': 2.558, 'train_steps_per_second': 0.08, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.755, 'eval_samples_per_second': 13.246, 'eval_steps_per_second': 13.246, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9414.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-9
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_9_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_9_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 124.4872, 'train_samples_per_second': 2.571, 'train_steps_per_second': 0.08, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8391, 'eval_samples_per_second': 11.918, 'eval_steps_per_second': 11.918, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-10
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_10_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_10_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 115.775, 'train_samples_per_second': 2.764, 'train_steps_per_second': 0.086, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8052, 'eval_samples_per_second': 12.419, 'eval_steps_per_second': 12.419, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-11
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_11_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_11_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 121.5061, 'train_samples_per_second': 2.634, 'train_steps_per_second': 0.082, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7687, 'eval_samples_per_second': 13.008, 'eval_steps_per_second': 13.008, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 2-12
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_2_12_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_2_12_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 119.2228, 'train_samples_per_second': 2.684, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15456.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8477, 'eval_samples_per_second': 11.796, 'eval_steps_per_second': 11.796, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 117.3541, 'train_samples_per_second': 2.727, 'train_steps_per_second': 0.085, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15478.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8059, 'eval_samples_per_second': 12.408, 'eval_steps_per_second': 12.408, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 121.1273, 'train_samples_per_second': 2.642, 'train_steps_per_second': 0.083, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7577, 'eval_samples_per_second': 13.197, 'eval_steps_per_second': 13.197, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9402.31 MB
GPU memory usage after cleaning cache: 8548.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 3-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_3_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_3_3_datasets.pkl


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.1,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:2,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.1,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 2, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:8,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 8, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:1,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 1, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=6.1164909362792965, metrics={'train_runtime': 45.17, 'train_samples_per_second': 0.221, 'train_steps_per_second': 0.221, 'train_loss': 6.1164909362792965, 'epoch': 1.0})
GPU memory usage before cleaning cache: 14502.31 MB
GPU memory usage after cleaning cache: 8718.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.9422, 'eval_samples_per_second': 10.614, 'eval_steps_per_second': 10.614, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9552.31 MB
GPU memory usage after cleaning cache: 8718.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 38.8334, 'train_samples_per_second': 0.258, 'train_steps_per_second': 0.258, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15208.31 MB
GPU memory usage after cleaning cache: 8718.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.9272, 'eval_samples_per_second': 10.785, 'eval_steps_per_second': 10.785, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9540.31 MB
GPU memory usage after cleaning cache: 8718.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 42.6717, 'train_samples_per_second': 0.234, 'train_steps_per_second': 0.234, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15154.31 MB
GPU memory usage after cleaning cache: 8718.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.7869, 'eval_samples_per_second': 12.708, 'eval_steps_per_second': 12.708, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9540.31 MB
GPU memory usage after cleaning cache: 8718.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-4
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_4_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 44.4303, 'train_samples_per_second': 0.225, 'train_steps_per_second': 0.225, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 14282.31 MB
GPU memory usage after cleaning cache: 8718.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8852, 'eval_samples_per_second': 11.297, 'eval_steps_per_second': 11.297, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9540.31 MB
GPU memory usage after cleaning cache: 8718.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-5
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_5_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_5_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=10, training_loss=0.0, metrics={'train_runtime': 49.4393, 'train_samples_per_second': 0.202, 'train_steps_per_second': 0.202, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15154.31 MB
GPU memory usage after cleaning cache: 8718.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.673, 'eval_samples_per_second': 14.859, 'eval_steps_per_second': 14.859, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9544.31 MB
GPU memory usage after cleaning cache: 8718.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-6
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_6_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_6_datasets.pkl


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:13,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 1000, Logging steps: 1000, Save steps: 1000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|     model.layers.12.self_attn.q_proj.weight     |  16777216  |
|     model.layers.12.self_attn.k_proj.weight     |  4194304   |
|     model.layers.12.self_attn.v_proj.weight     |  4194304   |
|     model.layers.12.self_attn.o_proj.weight     |  16777216  |
|       model.layers.12.mlp.gate_proj.weight      |  58720256  |
|        model.layers.12.mlp.up_proj.weight       |  58720256  |
|       model.layers.12.mlp.down_proj.weight      |  58720256  |
|      model.layers.12.input_layernorm.weight     |    4096    |
| model.layers.12.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 3081.2M parameters
Total Trainable Params: 3081.2201M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 17334.31 MB
GPU memory usage after cleaning cache: 17334.31 MB
GPU memory occupied from nvmlInit: 31477 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 224091
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47459
})
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 24.81 MiB is free. Process 1296146 has 16.62 GiB memory in use. Including non-PyTorch memory, this process has 15.09 GiB memory in use. Of the allocated memory 14.55 GiB is allocated by PyTorch, and 169.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 24229 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 24229 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=1.8220137351979076e+23, metrics={'train_runtime': 296.4349, 'train_samples_per_second': 3.238, 'train_steps_per_second': 0.101, 'train_loss': 1.8220137351979076e+23, 'epoch': 1.0})
GPU memory usage before cleaning cache: 14906.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 24229 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8285, 'eval_samples_per_second': 12.07, 'eval_steps_per_second': 12.07, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9474.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 24229 MB.
Training for 0-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=0.0, metrics={'train_runtime': 342.4024, 'train_samples_per_second': 2.804, 'train_steps_per_second': 0.088, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15496.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 24229 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8829, 'eval_samples_per_second': 11.326, 'eval_steps_per_second': 11.326, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9462.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 24229 MB.
Training for 0-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=0.0, metrics={'train_runtime': 343.6843, 'train_samples_per_second': 2.793, 'train_steps_per_second': 0.087, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15484.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 24229 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8954, 'eval_samples_per_second': 11.169, 'eval_steps_per_second': 11.169, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9462.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 24229 MB.
Training for 0-4
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_4_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=0.0, metrics={'train_runtime': 345.8394, 'train_samples_per_second': 2.776, 'train_steps_per_second': 0.087, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15500.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 272 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.9333, 'eval_samples_per_second': 10.714, 'eval_steps_per_second': 10.714, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9462.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 272 MB.
Training for 0-5
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_5_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_5_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=0.0, metrics={'train_runtime': 348.6108, 'train_samples_per_second': 2.754, 'train_steps_per_second': 0.086, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15500.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 29137 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8008, 'eval_samples_per_second': 12.488, 'eval_steps_per_second': 12.488, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9466.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 29137 MB.
Training for 0-6
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_6_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_6_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=0.0, metrics={'train_runtime': 358.1124, 'train_samples_per_second': 2.681, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15614.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 30557 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8828, 'eval_samples_per_second': 11.328, 'eval_steps_per_second': 11.328, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9462.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 30557 MB.
Training for 0-7
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_7_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=0.0, metrics={'train_runtime': 353.5007, 'train_samples_per_second': 2.716, 'train_steps_per_second': 0.085, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15500.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 30557 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8201, 'eval_samples_per_second': 12.193, 'eval_steps_per_second': 12.193, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9468.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 30557 MB.
Training for 0-8
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_8_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_8_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=0.0, metrics={'train_runtime': 361.6651, 'train_samples_per_second': 2.654, 'train_steps_per_second': 0.083, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15500.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 30557 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1.0199, 'eval_samples_per_second': 9.805, 'eval_steps_per_second': 9.805, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9474.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 30557 MB.
Training for 0-9
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_9_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_9_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=0.0, metrics={'train_runtime': 348.4925, 'train_samples_per_second': 2.755, 'train_steps_per_second': 0.086, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15948.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 30557 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8911, 'eval_samples_per_second': 11.222, 'eval_steps_per_second': 11.222, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9462.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 30557 MB.
Training for 0-10
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_10_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_10_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=0.0, metrics={'train_runtime': 365.6644, 'train_samples_per_second': 2.625, 'train_steps_per_second': 0.082, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15488.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 30557 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.8942, 'eval_samples_per_second': 11.183, 'eval_steps_per_second': 11.183, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9462.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 30557 MB.
Training for 0-11
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_11_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_11_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=0.0, metrics={'train_runtime': 356.6754, 'train_samples_per_second': 2.692, 'train_steps_per_second': 0.084, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15516.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 30557 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 0.966, 'eval_samples_per_second': 10.352, 'eval_steps_per_second': 10.352, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9462.31 MB
GPU memory usage after cleaning cache: 8640.31 MB
GPU memory occupied from nvmlInit: 30557 MB.
Training for 0-12
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_12_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_12_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31677 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:13,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 1000, Logging steps: 1000, Save steps: 1000, Save total limit: 3, Max seq length: 2048


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:13,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 1000, Logging steps: 1000, Save steps: 1000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|     model.layers.12.self_attn.q_proj.weight     |  16777216  |
|     model.layers.12.self_attn.k_proj.weight     |  4194304   |
|     model.layers.12.self_attn.v_proj.weight     |  4194304   |
|     model.layers.12.self_attn.o_proj.weight     |  16777216  |
|       model.layers.12.mlp.gate_proj.weight      |  58720256  |
|        model.layers.12.mlp.up_proj.weight       |  58720256  |
|       model.layers.12.mlp.down_proj.weight      |  58720256  |
|      model.layers.12.input_layernorm.weight     |    4096    |
| model.layers.12.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 3081.2M parameters
Total Trainable Params: 3081.2201M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31677 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 224091
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47459
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:5,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 5, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1336.3M parameters
Total Trainable Params: 1336.3241M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 29492.31 MB
GPU memory usage after cleaning cache: 29492.31 MB
GPU memory occupied from nvmlInit: 31677 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 8.81 MiB is free. Process 2452822 has 28.48 GiB memory in use. Including non-PyTorch memory, this process has 3.23 GiB memory in use. Of the allocated memory 2.85 GiB is allocated by PyTorch, and 25.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:12,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 12, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 4000, Logging steps: 4000, Save steps: 4000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 2863.1M parameters
Total Trainable Params: 2863.1081M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31677 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 224091
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47459
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:13,  seq_len:2048,  batch_size:32,  float16:True,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_sample_file_index:1
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 13, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 4000, Logging steps: 4000, Save steps: 4000, Save total limit: 3, Max seq length: 2048
+-------------------------------------------------+------------+
|                     Modules                     | Parameters |
+-------------------------------------------------+------------+
|            model.embed_tokens.weight            | 122880000  |
|      model.layers.0.self_attn.q_proj.weight     |  16777216  |
|      model.layers.0.self_attn.k_proj.weight     |  4194304   |
|      model.layers.0.self_attn.v_proj.weight     |  4194304   |
|      model.layers.0.self_attn.o_proj.weight     |  16777216  |
|       model.layers.0.mlp.gate_proj.weight       |  58720256  |
|        model.layers.0.mlp.up_proj.weight        |  58720256  |
|       model.layers.0.mlp.down_proj.weight       |  58720256  |
|      model.layers.0.input_layernorm.weight      |    4096    |
|  model.layers.0.post_attention_layernorm.weight |    4096    |
|      model.layers.1.self_attn.q_proj.weight     |  16777216  |
|      model.layers.1.self_attn.k_proj.weight     |  4194304   |
|      model.layers.1.self_attn.v_proj.weight     |  4194304   |
|      model.layers.1.self_attn.o_proj.weight     |  16777216  |
|       model.layers.1.mlp.gate_proj.weight       |  58720256  |
|        model.layers.1.mlp.up_proj.weight        |  58720256  |
|       model.layers.1.mlp.down_proj.weight       |  58720256  |
|      model.layers.1.input_layernorm.weight      |    4096    |
|  model.layers.1.post_attention_layernorm.weight |    4096    |
|      model.layers.2.self_attn.q_proj.weight     |  16777216  |
|      model.layers.2.self_attn.k_proj.weight     |  4194304   |
|      model.layers.2.self_attn.v_proj.weight     |  4194304   |
|      model.layers.2.self_attn.o_proj.weight     |  16777216  |
|       model.layers.2.mlp.gate_proj.weight       |  58720256  |
|        model.layers.2.mlp.up_proj.weight        |  58720256  |
|       model.layers.2.mlp.down_proj.weight       |  58720256  |
|      model.layers.2.input_layernorm.weight      |    4096    |
|  model.layers.2.post_attention_layernorm.weight |    4096    |
|      model.layers.3.self_attn.q_proj.weight     |  16777216  |
|      model.layers.3.self_attn.k_proj.weight     |  4194304   |
|      model.layers.3.self_attn.v_proj.weight     |  4194304   |
|      model.layers.3.self_attn.o_proj.weight     |  16777216  |
|       model.layers.3.mlp.gate_proj.weight       |  58720256  |
|        model.layers.3.mlp.up_proj.weight        |  58720256  |
|       model.layers.3.mlp.down_proj.weight       |  58720256  |
|      model.layers.3.input_layernorm.weight      |    4096    |
|  model.layers.3.post_attention_layernorm.weight |    4096    |
|      model.layers.4.self_attn.q_proj.weight     |  16777216  |
|      model.layers.4.self_attn.k_proj.weight     |  4194304   |
|      model.layers.4.self_attn.v_proj.weight     |  4194304   |
|      model.layers.4.self_attn.o_proj.weight     |  16777216  |
|       model.layers.4.mlp.gate_proj.weight       |  58720256  |
|        model.layers.4.mlp.up_proj.weight        |  58720256  |
|       model.layers.4.mlp.down_proj.weight       |  58720256  |
|      model.layers.4.input_layernorm.weight      |    4096    |
|  model.layers.4.post_attention_layernorm.weight |    4096    |
|      model.layers.5.self_attn.q_proj.weight     |  16777216  |
|      model.layers.5.self_attn.k_proj.weight     |  4194304   |
|      model.layers.5.self_attn.v_proj.weight     |  4194304   |
|      model.layers.5.self_attn.o_proj.weight     |  16777216  |
|       model.layers.5.mlp.gate_proj.weight       |  58720256  |
|        model.layers.5.mlp.up_proj.weight        |  58720256  |
|       model.layers.5.mlp.down_proj.weight       |  58720256  |
|      model.layers.5.input_layernorm.weight      |    4096    |
|  model.layers.5.post_attention_layernorm.weight |    4096    |
|      model.layers.6.self_attn.q_proj.weight     |  16777216  |
|      model.layers.6.self_attn.k_proj.weight     |  4194304   |
|      model.layers.6.self_attn.v_proj.weight     |  4194304   |
|      model.layers.6.self_attn.o_proj.weight     |  16777216  |
|       model.layers.6.mlp.gate_proj.weight       |  58720256  |
|        model.layers.6.mlp.up_proj.weight        |  58720256  |
|       model.layers.6.mlp.down_proj.weight       |  58720256  |
|      model.layers.6.input_layernorm.weight      |    4096    |
|  model.layers.6.post_attention_layernorm.weight |    4096    |
|      model.layers.7.self_attn.q_proj.weight     |  16777216  |
|      model.layers.7.self_attn.k_proj.weight     |  4194304   |
|      model.layers.7.self_attn.v_proj.weight     |  4194304   |
|      model.layers.7.self_attn.o_proj.weight     |  16777216  |
|       model.layers.7.mlp.gate_proj.weight       |  58720256  |
|        model.layers.7.mlp.up_proj.weight        |  58720256  |
|       model.layers.7.mlp.down_proj.weight       |  58720256  |
|      model.layers.7.input_layernorm.weight      |    4096    |
|  model.layers.7.post_attention_layernorm.weight |    4096    |
|      model.layers.8.self_attn.q_proj.weight     |  16777216  |
|      model.layers.8.self_attn.k_proj.weight     |  4194304   |
|      model.layers.8.self_attn.v_proj.weight     |  4194304   |
|      model.layers.8.self_attn.o_proj.weight     |  16777216  |
|       model.layers.8.mlp.gate_proj.weight       |  58720256  |
|        model.layers.8.mlp.up_proj.weight        |  58720256  |
|       model.layers.8.mlp.down_proj.weight       |  58720256  |
|      model.layers.8.input_layernorm.weight      |    4096    |
|  model.layers.8.post_attention_layernorm.weight |    4096    |
|      model.layers.9.self_attn.q_proj.weight     |  16777216  |
|      model.layers.9.self_attn.k_proj.weight     |  4194304   |
|      model.layers.9.self_attn.v_proj.weight     |  4194304   |
|      model.layers.9.self_attn.o_proj.weight     |  16777216  |
|       model.layers.9.mlp.gate_proj.weight       |  58720256  |
|        model.layers.9.mlp.up_proj.weight        |  58720256  |
|       model.layers.9.mlp.down_proj.weight       |  58720256  |
|      model.layers.9.input_layernorm.weight      |    4096    |
|  model.layers.9.post_attention_layernorm.weight |    4096    |
|     model.layers.10.self_attn.q_proj.weight     |  16777216  |
|     model.layers.10.self_attn.k_proj.weight     |  4194304   |
|     model.layers.10.self_attn.v_proj.weight     |  4194304   |
|     model.layers.10.self_attn.o_proj.weight     |  16777216  |
|       model.layers.10.mlp.gate_proj.weight      |  58720256  |
|        model.layers.10.mlp.up_proj.weight       |  58720256  |
|       model.layers.10.mlp.down_proj.weight      |  58720256  |
|      model.layers.10.input_layernorm.weight     |    4096    |
| model.layers.10.post_attention_layernorm.weight |    4096    |
|     model.layers.11.self_attn.q_proj.weight     |  16777216  |
|     model.layers.11.self_attn.k_proj.weight     |  4194304   |
|     model.layers.11.self_attn.v_proj.weight     |  4194304   |
|     model.layers.11.self_attn.o_proj.weight     |  16777216  |
|       model.layers.11.mlp.gate_proj.weight      |  58720256  |
|        model.layers.11.mlp.up_proj.weight       |  58720256  |
|       model.layers.11.mlp.down_proj.weight      |  58720256  |
|      model.layers.11.input_layernorm.weight     |    4096    |
| model.layers.11.post_attention_layernorm.weight |    4096    |
|     model.layers.12.self_attn.q_proj.weight     |  16777216  |
|     model.layers.12.self_attn.k_proj.weight     |  4194304   |
|     model.layers.12.self_attn.v_proj.weight     |  4194304   |
|     model.layers.12.self_attn.o_proj.weight     |  16777216  |
|       model.layers.12.mlp.gate_proj.weight      |  58720256  |
|        model.layers.12.mlp.up_proj.weight       |  58720256  |
|       model.layers.12.mlp.down_proj.weight      |  58720256  |
|      model.layers.12.input_layernorm.weight     |    4096    |
| model.layers.12.post_attention_layernorm.weight |    4096    |
|                model.norm.weight                |    4096    |
|                  lm_head.weight                 | 122880000  |
+-------------------------------------------------+------------+
MISTRAL model size: 3081.2M parameters
Total Trainable Params: 3081.2201M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float16
GPU memory usage before cleaning cache: 314.31 MB
GPU memory usage after cleaning cache: 314.31 MB
GPU memory occupied from nvmlInit: 31677 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 224091
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47459
})
***** Train results ***** TrainOutput(global_step=7002, training_loss=nan, metrics={'train_runtime': 38812.158, 'train_samples_per_second': 5.774, 'train_steps_per_second': 0.18, 'train_loss': nan, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31990.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 31677 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1980.3018, 'eval_samples_per_second': 23.966, 'eval_steps_per_second': 23.966, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 31677 MB.
Training for 0-2
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_2_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 221617
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 46910
})
***** Train results ***** TrainOutput(global_step=6925, training_loss=0.0, metrics={'train_runtime': 37242.9941, 'train_samples_per_second': 5.951, 'train_steps_per_second': 0.186, 'train_loss': 0.0, 'epoch': 1.0})
GPU memory usage before cleaning cache: 31980.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 272 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1877.2179, 'eval_samples_per_second': 24.989, 'eval_steps_per_second': 24.989, 'epoch': 1.0}
GPU memory usage before cleaning cache: 19974.31 MB
GPU memory usage after cleaning cache: 19076.31 MB
GPU memory occupied from nvmlInit: 272 MB.
Training for 0-3
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_3_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 222983
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47293
})
Error occured while training the model ???????????????????????????????????
[Errno 32] Broken pipe


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:2,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1,  local_model_path:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.81 MB
GPU memory usage after cleaning cache: 440.81 MB
GPU memory occupied from nvmlInit: 1061 MB.
Training for 0-1
val_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl
train_local_pickel_path: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/train_0_1_datasets.pkl
File not found: /home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL/val_0_1_datasets.pkl


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:2,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1,  local_model_path:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 2, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 682.0M parameters
Total Trainable Params: 681.9881M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.81 MB
GPU memory usage after cleaning cache: 440.81 MB
GPU memory occupied from nvmlInit: 1061 MB.
Training for 0-1
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_1_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=10.899973297119141, metrics={'train_runtime': 451.2536, 'train_samples_per_second': 2.127, 'train_steps_per_second': 0.066, 'train_loss': 10.899973297119141, 'epoch': 1.0})
GPU memory usage before cleaning cache: 15314.81 MB
GPU memory usage after cleaning cache: 9046.81 MB
GPU memory occupied from nvmlInit: 9667 MB.
***** Eval results ***** {'eval_loss': 10.622215270996094, 'eval_runtime': 1.6488, 'eval_samples_per_second': 6.065, 'eval_steps_per_second': 6.065, 'epoch': 1.0}
GPU memory usage before cleaning cache: 9854.81 MB
GPU memory usage after cleaning cache: 9046.81 MB
GPU memory occupied from nvmlInit: 9667 MB.
Training for 0-2
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_2_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:6,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1,  local_model_path:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 6, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1554.4M parameters
Total Trainable Params: 1554.4361M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.81 MB
GPU memory usage after cleaning cache: 440.81 MB
GPU memory occupied from nvmlInit: 1061 MB.
Training for 0-1
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_1_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_sample_file_index:1,  local_model_path:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.81 MB
GPU memory usage after cleaning cache: 440.81 MB
GPU memory occupied from nvmlInit: 1061 MB.
Training for 0-1
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_1_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=10.878241221110025, metrics={'train_runtime': 1318.298, 'train_samples_per_second': 0.728, 'train_steps_per_second': 0.023, 'train_loss': 10.878241221110025, 'epoch': 1.0})
GPU memory usage before cleaning cache: 39924.81 MB
GPU memory usage after cleaning cache: 23124.81 MB
GPU memory occupied from nvmlInit: 23745 MB.
***** Eval results ***** {'eval_loss': 10.579565048217773, 'eval_runtime': 4.2025, 'eval_samples_per_second': 2.38, 'eval_steps_per_second': 2.38, 'epoch': 1.0}
GPU memory usage before cleaning cache: 23934.81 MB
GPU memory usage after cleaning cache: 23124.81 MB
GPU memory occupied from nvmlInit: 23745 MB.
Training for 0-2
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_2_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=9.932413736979166, metrics={'train_runtime': 1740.9566, 'train_samples_per_second': 0.551, 'train_steps_per_second': 0.017, 'train_loss': 9.932413736979166, 'epoch': 1.0})
GPU memory usage before cleaning cache: 38392.81 MB
GPU memory usage after cleaning cache: 23124.81 MB
GPU memory occupied from nvmlInit: 23745 MB.
***** Eval results ***** {'eval_loss': 9.990306854248047, 'eval_runtime': 4.113, 'eval_samples_per_second': 2.431, 'eval_steps_per_second': 2.431, 'epoch': 1.0}
GPU memory usage before cleaning cache: 23934.81 MB
GPU memory usage after cleaning cache: 23124.81 MB
GPU memory occupied from nvmlInit: 23745 MB.
Training for 0-3
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_3_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
***** Train results ***** TrainOutput(global_step=30, training_loss=9.349660491943359, metrics={'train_runtime': 1740.8557, 'train_samples_per_second': 0.551, 'train_steps_per_second': 0.017, 'train_loss': 9.349660491943359, 'epoch': 1.0})
GPU memory usage before cleaning cache: 38390.81 MB
GPU memory usage after cleaning cache: 23124.81 MB
GPU memory occupied from nvmlInit: 23745 MB.
***** Eval results ***** {'eval_loss': 8.366008758544922, 'eval_runtime': 4.4907, 'eval_samples_per_second': 2.227, 'eval_steps_per_second': 2.227, 'epoch': 1.0}
GPU memory usage before cleaning cache: 23934.81 MB
GPU memory usage after cleaning cache: 23124.81 MB
GPU memory occupied from nvmlInit: 23745 MB.
Training for 0-4
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_4_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_sample_file_index:1,  local_model_path:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 4000, Logging steps: 4000, Save steps: 4000, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.81 MB
GPU memory usage after cleaning cache: 440.81 MB
GPU memory occupied from nvmlInit: 1061 MB.
Training for 0-1
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_1_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_1_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180799
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47526
})
***** Train results ***** TrainOutput(global_step=5649, training_loss=4.710553807228049, metrics={'train_runtime': 39485.49, 'train_samples_per_second': 4.579, 'train_steps_per_second': 0.143, 'train_loss': 4.710553807228049, 'epoch': 1.0})
GPU memory usage before cleaning cache: 39132.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
***** Eval results ***** {'eval_loss': 3.9447503089904785, 'eval_runtime': 2091.4325, 'eval_samples_per_second': 22.724, 'eval_steps_per_second': 22.724, 'epoch': 1.0}
GPU memory usage before cleaning cache: 23998.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
Training for 0-2
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_2_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_2_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 177569
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47262
})
***** Train results ***** TrainOutput(global_step=5549, training_loss=3.723205071437421, metrics={'train_runtime': 254335.8058, 'train_samples_per_second': 0.698, 'train_steps_per_second': 0.022, 'train_loss': 3.723205071437421, 'epoch': 1.0})
GPU memory usage before cleaning cache: 39820.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
***** Eval results ***** {'eval_loss': 3.388298273086548, 'eval_runtime': 2011.7241, 'eval_samples_per_second': 23.493, 'eval_steps_per_second': 23.493, 'epoch': 1.0}
GPU memory usage before cleaning cache: 23998.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
Training for 0-3
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_3_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_3_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 179003
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47718
})
***** Train results ***** TrainOutput(global_step=5593, training_loss=3.34086343979081, metrics={'train_runtime': 37231.2941, 'train_samples_per_second': 4.808, 'train_steps_per_second': 0.15, 'train_loss': 3.34086343979081, 'epoch': 1.0})
GPU memory usage before cleaning cache: 36808.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
***** Eval results ***** {'eval_loss': 3.0611155033111572, 'eval_runtime': 2189.3412, 'eval_samples_per_second': 21.796, 'eval_steps_per_second': 21.796, 'epoch': 1.0}
GPU memory usage before cleaning cache: 23998.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
Training for 0-4
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_4_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_4_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 176940
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47391
})
***** Train results ***** TrainOutput(global_step=5529, training_loss=3.0505217871111414, metrics={'train_runtime': 34483.6486, 'train_samples_per_second': 5.131, 'train_steps_per_second': 0.16, 'train_loss': 3.0505217871111414, 'epoch': 1.0})
GPU memory usage before cleaning cache: 40246.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
***** Eval results ***** {'eval_loss': 2.8472506999969482, 'eval_runtime': 2040.0785, 'eval_samples_per_second': 23.23, 'eval_steps_per_second': 23.23, 'epoch': 1.0}
GPU memory usage before cleaning cache: 23998.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
Training for 0-5
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_5_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_5_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178907
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47563
})
***** Train results ***** TrainOutput(global_step=5590, training_loss=2.9187594336985687, metrics={'train_runtime': 36955.0789, 'train_samples_per_second': 4.841, 'train_steps_per_second': 0.151, 'train_loss': 2.9187594336985687, 'epoch': 1.0})
GPU memory usage before cleaning cache: 39852.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2112.2655, 'eval_samples_per_second': 22.518, 'eval_steps_per_second': 22.518, 'epoch': 1.0}
GPU memory usage before cleaning cache: 23998.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
Training for 0-6
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_6_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_6_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 177640
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47314
})
***** Train results ***** TrainOutput(global_step=5551, training_loss=2.7812120000900737, metrics={'train_runtime': 35354.7582, 'train_samples_per_second': 5.025, 'train_steps_per_second': 0.157, 'train_loss': 2.7812120000900737, 'epoch': 1.0})
GPU memory usage before cleaning cache: 39820.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
***** Eval results ***** {'eval_loss': 2.5956943035125732, 'eval_runtime': 2003.5991, 'eval_samples_per_second': 23.615, 'eval_steps_per_second': 23.615, 'epoch': 1.0}
GPU memory usage before cleaning cache: 23998.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
Training for 0-7
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180179
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47392
})
GPU memory usage before cleaning cache: 39820.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
***** Train results ***** TrainOutput(global_step=5791, training_loss=2.613982263695821, metrics={'train_runtime': 45854.6142, 'train_samples_per_second': 4.042, 'train_steps_per_second': 0.126, 'train_loss': 2.613982263695821, 'epoch': 1.0})
GPU memory usage before cleaning cache: 39820.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
***** Eval results ***** {'eval_loss': 2.4362058639526367, 'eval_runtime': 2082.9123, 'eval_samples_per_second': 22.795, 'eval_steps_per_second': 22.795, 'epoch': 1.0}
GPU memory usage before cleaning cache: 24000.81 MB
GPU memory usage after cleaning cache: 23174.81 MB
GPU memory occupied from nvmlInit: 23795 MB.
Training for 0-9
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_9_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_9_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 176889
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47312
})
Error occured while training the model ???????????????????????????????????
[Errno 32] Broken pipe


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:/home/iitgn_cse/latex_model/model_main_fp322024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:/home/iitgn_cse/latex_model/model_main_fp322024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
------------------------------------------------------------------------------------------
Training for 0-7
------------------------------------------------------------------------------------------
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
Error occured while training the model ???????????????????????????????????
Can't find a valid checkpoint at /home/iitgn_cse/latex_model/model_main_fp322024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
------------------------------------------------------------------------------------------
Training for 0-7
------------------------------------------------------------------------------------------
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
Error occured while training the model ???????????????????????????????????
[Errno 2] No such file or directory: '/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096/trainer_state.json'


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED__________________________________________________________
------------------------------------------------------------------------------------------
Training for 0-7
------------------------------------------------------------------------------------------
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
Error occured while training the model ???????????????????????????????????
[Errno 2] No such file or directory: '/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096/trainer_state.json'


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:None
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED__________________________________________________________
------------------------------------------------------------------------------------------
Training for 0-7
------------------------------------------------------------------------------------------
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED__________________________________________________________
------------------------------------------------------------------------------------------
Training for 0-7
------------------------------------------------------------------------------------------
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp_322024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp_32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp_32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp_32_2024-04-10/latex/main_fp_32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 10, Save steps: 10, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED__________________________________________________________
_______________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-20/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
_______________________________________________________________________
------------------------------------------------------------------------------------------
Training for 0-7
------------------------------------------------------------------------------------------
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp_32_2024-04-10/latex/main_fp_32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 4000, Logging steps: 4000, Save steps: 4000, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED__________________________________________________________
_______________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
_______________________________________________________________________
------------------------------------------------------------------------------------------
Training for 0-7
------------------------------------------------------------------------------------------
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl


______________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 4000, Logging steps: 4000, Save steps: 4000, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED__________________________________________________________
_______________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
_______________________________________________________________________
------------------------------------------------------------------------------------------
Training for 0-7
------------------------------------------------------------------------------------------
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180179
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47392
})
Error occured while training the model ???????????????????????????????????
CUDA out of memory. Tried to allocate 3.50 GiB. GPU 0 has a total capacity of 39.39 GiB of which 2.90 GiB is free. Including non-PyTorch memory, this process has 36.47 GiB memory in use. Of the allocated memory 33.20 GiB is allocated by PyTorch, and 2.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 3e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 5, Save steps: 5, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________
Loading model...
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED__________________________________________________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-23/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 3e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 5, Save steps: 5, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________
Loading model...
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED__________________________________________________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-23/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 3e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 5, Save steps: 5, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________
Loading model...
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED__________________________________________________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-23/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
______________________________________________________________________________________________________________________________________________________
Training for 0-7
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-
Error occured while training the model#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-
[enforce fail at inline_container.cc:595] . unexpected pos 1531792640 vs 1531792528


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 3e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 5, Save steps: 5, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________
Loading model...
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED__________________________________________________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-23/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
______________________________________________________________________________________________________________________________________________________
Training for 0-7
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})
------------------------------------------------------------------------------------------------------------------------------------------------------
***** Train results ***** TrainOutput(global_step=30, training_loss=3.5505252838134767, metrics={'train_runtime': 1430.4111, 'train_samples_per_second': 0.671, 'train_steps_per_second': 0.021, 'train_loss': 3.5505252838134767, 'epoch': 1.0})
------------------------------------------------------------------------------------------------------------------------------------------------------
GPU memory usage before cleaning cache: 35660.88 MB
GPU memory usage after cleaning cache: 21918.88 MB
GPU memory occupied from nvmlInit: 22539 MB.
------------------------------------------------------------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 4.9418792724609375, 'eval_runtime': 4.492, 'eval_samples_per_second': 2.226, 'eval_steps_per_second': 2.226, 'epoch': 1.0}
------------------------------------------------------------------------------------------------------------------------------------------------------
Model saved
GPU memory usage before cleaning cache: 22618.88 MB
GPU memory usage after cleaning cache: 21918.88 MB
GPU memory occupied from nvmlInit: 22539 MB.
______________________________________________________________________________________________________________________________________________________
Training for 0-8
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_8_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_8_datasets.pkl


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 3e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 5, Save steps: 5, Save total limit: 3, Max seq length: 2048
Tokenizer loaded__________________________________________________________
Loading model...
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED took __________ 0.04233916203180949 minutes_________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-23/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
trainer: <transformers.trainer.Trainer object at 0x7ff5759ff510> 
trainer.args: TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=10,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=32,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./logs_grad_clip,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.9,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=/home/iitgn_cse/latex_model/model_exp_fp32_2024-04-23/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_16-27-17,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=5,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=3,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=100,
weight_decay=0.1,
) 
 Took0.06728677352269491 minutes
______________________________________________________________________________________________________________________________________________________
Training for 0-7
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
}) 
 Took0.5895007491111756 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=10, training_loss=4.231257820129395, metrics={'train_runtime': 477.8786, 'train_samples_per_second': 0.67, 'train_steps_per_second': 0.021, 'train_loss': 4.231257820129395, 'epoch': 1.0})
 Took7.966794236501058 minutes
GPU memory usage before cleaning cache: 35624.88 MB
GPU memory usage after cleaning cache: 21918.88 MB
GPU memory occupied from nvmlInit: 22539 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 4.936944007873535, 'eval_runtime': 4.5107, 'eval_samples_per_second': 2.217, 'eval_steps_per_second': 2.217, 'epoch': 1.0}
 Took0.0752086599667867 minutes
Model STATE saved successfully 
 Took0.9096853057543437 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-23/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8732083082199097 minutes
GPU memory usage before cleaning cache: 22618.88 MB
GPU memory usage after cleaning cache: 21918.88 MB
GPU memory occupied from nvmlInit: 22539 MB.
______________________________________________________________________________________________________________________________________________________
Training for 0-7 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 0-8
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_8_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_8_datasets.pkl


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 3e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 5, Save steps: 5, Save total limit: 3, Max seq length: 2048
latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
Tokenizer loaded__________________________________________________________
Loading model...
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED took __________ 0.04252569278081258 minutes_________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-23/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
trainer: <transformers.trainer.Trainer object at 0x7f7951a67810> 
trainer.args: TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=32,
eval_delay=0,
eval_steps=10,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=32,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./logs_grad_clip,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.9,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=/home/iitgn_cse/latex_model/model_exp_fp32_2024-04-23/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_16-41-30,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=5,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=3,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=100,
weight_decay=0.1,
) 
 Took0.06768449544906616 minutes
______________________________________________________________________________________________________________________________________________________
Training for 0-7
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
}) 
 Took0.5853901386260987 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=10, training_loss=4.231257820129395, metrics={'train_runtime': 478.0984, 'train_samples_per_second': 0.669, 'train_steps_per_second': 0.021, 'train_loss': 4.231257820129395, 'epoch': 1.0})
 Took7.970303177833557 minutes
GPU memory usage before cleaning cache: 35624.88 MB
GPU memory usage after cleaning cache: 21918.88 MB
GPU memory occupied from nvmlInit: 22539 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 4.936944007873535, 'eval_runtime': 4.4769, 'eval_samples_per_second': 2.234, 'eval_steps_per_second': 2.234, 'epoch': 1.0}
 Took0.07464479207992554 minutes
Model STATE saved successfully 
 Took6.635983784993489e-06 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-23/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8513089656829834 minutes
GPU memory usage before cleaning cache: 22618.88 MB
GPU memory usage after cleaning cache: 21918.88 MB
GPU memory occupied from nvmlInit: 22539 MB.
______________________________________________________________________________________________________________________________________________________
Training for 0-7 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 0-8
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_8_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_8_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
}) 
 Took0.6001360336939494 minutes


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:True,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 3e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 10, Logging steps: 5, Save steps: 5, Save total limit: 3, Max seq length: 2048
latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
Tokenizer loaded__________________________________________________________
Loading model...
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED took __________ 0.04447570244471232 minutes_________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_exp_fp32_2024-04-23/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
trainer.args: TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=32,
eval_delay=0,
eval_steps=10,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=32,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./logs_grad_clip,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.9,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=/home/iitgn_cse/latex_model/model_exp_fp32_2024-04-23/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_16-55-00_test=True,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=5,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=3,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=100,
weight_decay=0.1,
) 
 Took0.07725190321604411 minutes
______________________________________________________________________________________________________________________________________________________
Training for 0-7
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 320
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
}) 
 Took0.6145638744036357 minutes
trainer.args: TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=False,
eval_accumulation_steps=32,
eval_delay=0,
eval_steps=10,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=32,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./logs_grad_clip,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=5,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.9,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=/home/iitgn_cse/latex_model/model_exp_fp32_2024-04-23/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_16-55-00_test=True,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=5,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=3,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=100,
weight_decay=0.1,
)


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 3e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 4000, Logging steps: 500, Save steps: 2000, Save total limit: 3, Max seq length: 2048
latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
Tokenizer loaded__________________________________________________________
Loading model...
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED took __________ 0.04047585328420003 minutes_________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
trainer.args: TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=32,
eval_delay=0,
eval_steps=4000,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=32,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=3e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./logs_grad_clip,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.9,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_3e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-05-40_test=False,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=2000,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=3,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=100,
weight_decay=0.1,
) 
 Took0.07030778725941976 minutes
______________________________________________________________________________________________________________________________________________________
Training for 0-7
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180179
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47392
}) 
 Took0.5605612397193909 minutes


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

device:0,  layer:7,  seq_len:2048,  batch_size:32,  float16:False,  adafactor:False,  enb_grad_checkpoint:False,  data_percent:0.001,  vocab:30000,  checkpoint:None,  max_grad_norm:0.9,  test:False,  start_month_index:7,  start_year_index:0,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
D_emb: 4096, Vocal: 30000, d_head: 128, d_FF: 14336, N_Layer: 7, N_Head: 32, KV_Head: 8, Window: 4096
Epoch: 1, Learning rate: 2e-05, Weight decay: 0.1, Warmup steps: 100, LR scheduler type: cosine, Batch size: 32, Eval steps: 4000, Logging steps: 500, Save steps: 2000, Save total limit: 3, Max seq length: 2048
latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
Tokenizer loaded__________________________________________________________
Loading model...
+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED took __________ 0.04216513236363729 minutes_________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
trainer.args: TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=32,
eval_delay=0,
eval_steps=4000,
evaluation_strategy=IntervalStrategy.STEPS,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=32,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=./logs_grad_clip,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.COSINE,
max_grad_norm=0.9,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=1,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
output_dir=/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-07-17_test=False,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=2000,
save_strategy=IntervalStrategy.STEPS,
save_total_limit=3,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=100,
weight_decay=0.1,
) 
 Took0.06896034479141236 minutes
______________________________________________________________________________________________________________________________________________________
Training for 0-7
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_7_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180179
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47392
}) 
 Took0.5603395779927571 minutes
