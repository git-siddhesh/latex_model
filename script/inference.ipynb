{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = '/home/dosisiddhesh/latex_model/model2/latex/main3_ep_1_lr_0.0006_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_13_N_Head_32_KV_Head_8_W_4096/checkpoint-2000'\n",
    "\n",
    "from optimum.intel import OVModelForCausalLM, OVConfig\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.45s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(30000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-12): 13 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)#, export=True, config=OVConfig)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n"
     ]
    }
   ],
   "source": [
    "# print the first layer of the model\n",
    "def print_first_layer(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name)\n",
    "        print(param)\n",
    "        break\n",
    "\n",
    "print_first_layer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(30000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-12): 13 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# dataset path \n",
    "DATA_PATH_PICKEL = \"/home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL\"\n",
    "# load the data test_{year}_{month}_datasets.pkl\n",
    "local_path = os.path.join(DATA_PATH_PICKEL, 'test_0_1_datasets.pkl')\n",
    "test_dataset = None\n",
    "with open(local_path, 'rb') as f:\n",
    "    test_dataset = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "# dataset path \n",
    "DATA_PATH_PICKEL = \"/home/dosisiddhesh/SID_DATA_PROCESSED/DATA_PICKEL\"\n",
    "# load the data test_{year}_{month}_datasets.pkl\n",
    "local_path = os.path.join(DATA_PATH_PICKEL, 'train_0_1_datasets.pkl')\n",
    "test_dataset = None\n",
    "with open(local_path, 'rb') as f:\n",
    "    test_dataset = pickle.load(f)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],\n",
       "    num_rows: 224091\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'int'>\n",
      "{'input_ids': [93, 4362, 1838, 93, 4354, 124, 4313, 2630, 2897, 126, 93, 1539, 93, 2348, 9220, 1771, 40, 1044, 1108, 93, 1539, 93, 115, 1022, 1840, 126, 93, 1539, 93, 2056, 1022, 1080, 124, 1458, 1108, 93, 1539, 93, 6021, 1022, 1116, 124, 1745, 1108, 93, 1539, 93, 6221, 1022, 1080, 124, 1745, 1108, 93, 4800, 1022, 4050, 1261, 1116, 124, 1179, 1108, 93, 4800, 1022, 4356, 1261, 1080, 124, 1179, 1108, 93, 4800, 1022, 3788, 1261, 2055, 126, 93, 1539, 93, 6822, 2600, 1139, 113, 1108, 93, 1539, 93, 6773, 2600, 1139, 108, 1108, 93, 1539, 93, 8436, 2600, 1139, 114, 1108, 93, 1116, 124, 2023, 126, 1022, 6144, 93, 1139, 9122, 12503, 9230, 1008, 3266, 1314, 59, 2804, 3085, 1027, 3711, 1039, 13749, 3720, 3792, 126, 1148, 93, 3267, 124, 15437, 126, 3171, 47, 66, 47, 4734, 1144, 16293, 93, 2757, 124, 102, 46, 4474, 59, 93, 1757, 6678, 1144, 16293, 65, 1978, 47, 111, 1299, 47, 110, 1051, 47, 1051, 126, 1148, 4650, 1054, 8923, 2727, 45, 15751, 4521, 2555, 1148, 50, 1721, 1866, 15751, 45, 14865, 1148, 1027, 1148, 81, 47, 72, 47, 26959, 7844, 93, 2757, 124, 102, 46, 4474, 59, 93, 1757, 15282, 65, 1361, 51, 47, 1008, 115, 47, 1277, 47, 1789, 126, 1148, 4650, 1054, 8923, 3623, 1025, 1015, 13182, 9512, 1025, 5641, 1148, 2784, 1007, 12627, 1619, 1465, 2523, 1332, 1075, 1805, 45, 23818, 1148, 1303, 3239, 1278, 15751, 45, 14865, 93, 1080, 124, 2023, 126, 93, 1116, 124, 3041, 126, 1111, 9230, 1008, 1015, 13749, 3669, 1008, 1015, 10734, 93, 1335, 93, 1297, 1073, 53, 2233, 3266, 1314, 1008, 2998, 4339, 1023, 3980, 47, 2144, 8219, 1027, 1232, 46, 4167, 2158, 1025, 1423, 12209, 1054, 1898, 7991, 1027, 1423, 1009, 46, 5410, 4028, 12104, 1122, 3929, 47, 2786, 1712, 1122, 3482, 1039, 1015, 2163, 1025, 1015, 2467, 1744, 1027, 2467, 46, 3493, 1744, 5197, 1027, 1015, 4318, 1025, 1015, 3484, 46, 1039, 46, 4167, 41, 9230, 42, 5280, 1039, 1015, 6867, 1025, 1015, 2867, 1310, 1025, 1559, 2856, 3672, 3792, 47, 93, 1080, 124, 3041, 126, 93, 1310, 124, 4122, 126, 1156, 4820, 1120, 1352, 46, 3954, 2092, 1025, 15390, 98, 1597, 1025, 4361, 2941, 3131, 45, 3720, 1144, 13749, 2891, 45, 2899, 47, 1156, 1015, 2613, 2987, 1079, 1623, 2415, 11463, 3792, 93, 1137, 124, 11463, 1150, 2786, 1122, 1352, 46, 6515, 3792, 1008, 1271, 1966, 1015, 2467, 1027, 1015, 3493, 1744, 1122, 1008, 1015, 7635, 5192, 47, 19850, 1233, 2745, 6515, 4527, 45, 1015, 3792, 1271, 3852, 1027, 1080, 1008, 1015, 7635, 5192, 17786, 3626, 4527, 2368, 1039, 1015, 1628, 8733, 12954, 4854, 1025, 1015, 1519, 1039, 4170, 5192, 1027, 1423, 7773, 1039, 1015, 2467, 1324, 47, 1440, 3134, 1023, 11559, 3720, 1144, 1012, 13749, 2227, 1027, 7197, 2357, 1015, 5975, 1744, 5202, 98, 7662, 1025, 1015, 3283, 5192, 47, 1249, 2379, 2237, 1039, 4127, 1095, 1611, 4089, 1025, 13749, 3720, 3792, 1122, 5475, 1120, 9225, 1597, 9374, 1633, 1015, 15390, 1008, 1015, 11349, 1978, 93, 1137, 124, 7405, 85, 46, 3569, 126, 1027, 1015, 3344, 1025, 98, 19496, 41, 7635, 42, 5192, 2368, 1039, 1628, 8733, 12954, 4854, 1233, 98, 7635, 5192, 1039, 1015, 3283, 1324, 93, 1137, 124, 4479, 10233, 10596, 1083, 1150, 24199, 1700, 1442, 1954, 4638, 1039, 2415, 1015, 13749, 3720, 5475, 45, 1027, 5507, 3084, 4536, 1054, 1015, 4318, 1025, 1015, 6802, 1025, 1464, 26836, 9137, 1315, 1954, 5109, 93, 1137, 124, 2990, 50, 45, 76, 8377, 126, 41, 1650, 7393, 47, 93, 1137, 124, 2599, 45, 5060, 50, 126, 1054, 98, 3818, 1289, 1249, 1623, 2415, 1015, 13749, 3711, 1039, 1015, 2690, 2867, 1310, 1043, 2475, 124, 51, 1176, 70, 1121, 1025, 98, 2478, 2442, 51, 1957, 93, 1840, 37, 1642, 42, 1120, 1406, 2467, 3312, 1025, 1015, 2690, 1750, 37, 70, 37, 1008, 1015, 21542, 1335, 93, 1297, 1073, 53, 18129, 1978, 47, 2077, 1023, 98, 1597, 1025, 6320, 6588, 1095, 1043, 2475, 124, 51, 1176, 70, 1121, 1226, 1032, 3437, 1008, 1015, 1786, 4196, 1488, 59, 93, 4050, 93, 2475, 124, 51, 1176, 70, 42, 93, 1801, 102, 1224, 1135, 124, 50, 1261, 1335, 126, 71, 1068, 1662, 42, 44, 1022, 1088, 80, 1176, 50, 42, 1037, 93, 1167, 124, 1359, 51, 126, 93, 4356, 1316, 1043, 1335, 37, 1023, 1015, 3014, 2277, 1008, 1015, 1314, 45, 1043, 1662, 62, 70, 48, 2075, 124, 2819, 1082, 1027, 37, 2075, 124, 2819, 1082, 1023, 1015, 1750, 1025, 1015, 2819, 1014, 11242, 4493, 1271, 17601, 1015, 4737, 1025, 1015, 8733, 13919, 1015, 15390, 47, 1111, 4167, 1561, 3118, 1025, 1015, 1362, 37, 71, 1068, 1662, 1121, 1054, 1716, 1043, 1662, 37, 1777, 3980, 1008, 7393, 47, 93, 1137, 124, 5060, 50, 45, 17535, 1150, 1111, 3484, 46, 1039, 46, 4167, 2101, 1023, 98, 9230, 5280, 1054, 1046, 6121, 4527, 1233, 1015, 9230, 1008, 1015, 13749, 3669, 47, 3054, 45, 4318, 1025, 1015, 3484, 46, 1039, 46, 4167, 5280, 4553, 4456, 1025, 1015, 13749, 9230, 47, 1802, 5781, 1401, 1095, 1008, 1015, 21542, 1335, 93, 1297, 1073, 53, 18129, 1978, 1012, 2804, 3085, 1054, 1015, 13749, 9230, 1226, 1032, 2220, 47, 23562, 1027, 4571, 1025, 1015, 9230, 5280, 1039, 1015, 1362, 37, 71, 1068, 1662, 1121, 1023, 1324, 1025, 1015, 9948, 1025, 1231, 2987, 47, 1619, 3056, 6310, 1023, 1095, 1025, 1015, 9813, 1025, 2611, 1068, 1084, 124, 1359, 51, 1762, 1156, 1015, 11349, 1978, 98, 1584, 1858, 1009, 1015, 2508, 1025, 1015, 9230, 1008, 1015, 13749, 3669, 1777, 1638, 1008, 3289, 1466, 1137, 124, 4873, 3350, 1150, 1249, 4016, 1015, 6320, 1025, 3289, 47, 93, 1137, 124, 4873, 3350, 126, 1008, 1015, 21542, 1335, 93, 1297, 1073, 53, 18129, 1978, 5700, 1654, 1025, 1015, 3467, 3085, 1054, 1015, 9230, 47, 5991, 1039, 6320, 1025, 7393, 47, 93, 1137, 124, 8377, 45, 5060, 51, 126, 1054, 1015, 1559, 2856, 2467, 1744, 1015, 2690, 2867, 1310, 1023, 15761, 1027, 1442, 1015, 1488, 93, 4050, 93, 2475, 124, 79, 1176, 70, 42, 93, 1801, 102, 1224, 1135, 124, 50, 1261, 1335, 126, 71, 1068, 1662, 45, 93, 1389, 42, 44, 1022, 1088, 80, 1176, 50, 42, 1037, 93, 1167, 124, 1359, 46, 79, 126, 93, 4356, 1316, 37, 79, 37, 1023, 1015, 1597, 1025, 2467, 3312, 45, 1043, 1389, 62, 79, 48, 2180, 124, 2819, 1449, 1027, 37, 2180, 124, 2819, 126, 93, 1801, 50, 48, 93, 1335, 37, 1023, 98, 4807, 1597, 1025, 3312, 5267, 1008, 1015, 2819, 1014, 11242, 47, 2497, 1095, 1008, 1015, 4390, 1043, 1335, 93, 1039, 49, 37, 1027, 1043, 1389, 37, 2718, 37, 79, 93, 1801, 93, 1389, 48, 93, 1335, 37, 1023, 98, 1898, 1597, 47, 1111, 1362, 37, 71, 1068, 1662, 1174, 1389, 1121, 1054, 1015, 21542, 1335, 93, 1297, 1073, 53, 18129, 1978, 1777, 3929, 7122, 1054, 98, 3138, 2446, 1025, 1043, 1662, 37, 1027, 1043, 1389, 37, 1008, 3289, 47, 93, 1137, 124, 17535, 1150, 1156, 7393, 47, 93, 1137, 124, 8377, 45, 5060, 51, 45, 83, 2459, 126, 1046, 1777, 12096, 1095, 1015, 4167, 4196, 2101, 1025, 1015, 1406, 46, 2856, 2867, 1310, 1226, 1032, 3929, 1233, 1015, 1786, 2611, 59, 93, 4050, 93, 3965, 1022, 1335, 93, 1840, 49, 126, 93, 1335, 93, 3146, 93, 2475, 124, 51, 126, 62, 93, 3965, 1022, 1389, 93, 1840, 49, 126, 71, 93, 1194, 41, 93, 1135, 124, 70, 1065, 2075, 124, 2819, 1204, 93, 1389, 93, 1162, 1289, 93, 1167, 124, 12417, 126, 93, 4356, 1156, 1231, 5502, 1046, 1023, 4540, 1095, 1015, 2318, 1043, 1389, 93, 1840, 49, 37, 2528, 47, 1111, 2034, 1023, 1095, 1015, 1362, 37, 71, 41, 70, 48, 2075, 124, 2819, 1037, 93, 1389, 1121, 1023, 2559, 1039, 2287, 4527, 4005, 1008, 1043, 1389, 1186, 1156, 2298, 45, 1008, 1015, 21542, 1335, 93, 1297, 1073, 53, 18129, 1978, 1464, 4527, 4216, 2899, 1008, 1015, 9230, 5280, 47, 1111, 5502, 18814, 16409, 1095, 2158, 4005, 1008, 1043, 1389, 37, 7377, 1591, 1611, 1008, 1015, 3493, 5856, 47, 7910, 9813, 45, 1025, 5692, 45, 3298, 1095, 1015, 15761, 1488, 1025, 1015, 1406, 46, 2856, 2867, 1310, 1023, 5453, 1638, 1144, 1969, 47, 1068, 1084, 124, 1359, 51, 1142, 1120, 37, 71, 41, 70, 48, 2075, 124, 2819, 1142, 62, 71, 41, 70, 48, 2075, 124, 2819, 1037, 49, 1610, 20934, 1025, 5502, 1068, 1084, 124, 12417, 1142, 1008, 1015, 3484, 46, 1039, 46, 4167, 1561, 1023, 4170, 6608, 1025, 1231, 2205, 47, 2497, 1095, 1816, 6320, 1008, 11439, 1025, 1231, 5502, 2302, 1638, 1008, 7393, 1466, 1137, 124, 15858, 45, 9328, 1434, 99, 1150, 1111, 2285, 1025, 1015, 2987, 1023, 1015, 1786, 47, 1156, 8007, 47, 51, 1079, 3524, 1015, 1314, 1027, 4402, 1015, 9230, 1008, 1015, 13749, 3669, 47, 11002, 45, 1079, 2613, 1015, 1680, 1750, 12209, 1025, 1015, 9230, 1027, 4402, 1015, 5676, 1025, 15858, 40, 116, 5598, 1008, 1015, 3266, 1314, 47, 1249, 1525, 4402, 1015, 2804, 3085, 1025, 1015, 4028, 12104, 1025, 1015, 13749, 9230, 47, 1156, 8007, 47, 52, 1079, 4016, 1046, 1054, 1015, 4668, 1025, 1015, 3484, 46, 1039, 46, 4167, 1561, 41, 9230, 5280, 42, 1025, 1015, 1362, 37, 71, 1068, 1662, 1174, 1389, 1610, 2077, 1079, 5312, 6106, 1015, 8888, 1025, 2158, 4005, 1008, 1043, 1389, 37, 1054, 1043, 1389, 93, 1840, 49, 37, 1027, 1952, 13750, 1008, 1015, 3493, 2049, 47, 8007, 47, 53, 3455, 1733, 4571, 1025, 1015, 1712, 47, 1156, 2298, 45, 1015, 2446, 1025, 9813, 1025, 1015, 3484, 46, 1039, 46, 4167, 1561, 3118, 1023, 4556, 47, 93, 1310, 124, 9122, 12503, 9230, 1008, 1015, 3266, 1314, 126, 1249, 1717, 1015, 1314, 1025, 1324, 3059, 2064, 3266, 1307, 45, 1884, 1144, 1015, 9054, 1012, 1652, 93, 4050, 84, 62, 93, 1202, 101, 1073, 53, 126, 121, 93, 1194, 92, 93, 1135, 124, 50, 1065, 51, 126, 93, 1194, 41, 93, 2698, 1022, 1185, 126, 93, 1297, 93, 1162, 2235, 51, 126, 46, 93, 1135, 124, 50, 1065, 51, 126, 110, 1073, 51, 126, 93, 1297, 1073, 51, 126, 44, 93, 1135, 1022, 1335, 1065, 53, 9344, 93, 1297, 1073, 53, 126, 93, 1162, 3605, 93, 1167, 124, 1652, 126, 93, 4356, 1316, 1043, 1335, 63, 49, 1186, 1111, 2309, 1025, 1015, 1314, 1023, 11753, 1233, 2939, 45, 3299, 1015, 4029, 1043, 1297, 62, 49, 37, 1023, 19496, 47, 5530, 8733, 12954, 4854, 1233, 1231, 5192, 1039, 1015, 7333, 2249, 1027, 1423, 7773, 1039, 1015, 3954, 5192, 1023, 1015, 3134, 1271, 3088, 6000, 1039, 1015, 11463, 2478, 1079, 1122, 6410, 1039, 2415, 2677, 47, 1413, 1149, 1717, 1714, 1015, 1428, 37, 110, 62, 49, 1186, 2077, 1023, 98, 1901, 2559, 13749, 2227, 1008, 1015, 8710, 1978, 1638, 1144, 1015, 2611, 93, 1137, 124, 4560, 45, 11734, 126, 93, 4050, 93, 2894, 124, 2444, 1176, 121, 60, 1793, 124, 49, 1037, 93, 1405, 42, 62, 93, 1135, 124, 53, 93, 1665, 124, 52, 2835, 1665, 1022, 1335, 1108, 93, 1135, 1022, 1405, 4073, 121, 46, 1793, 124, 49, 5922, 51, 126, 44, 93, 1405, 1073, 51, 2239, 93, 1167, 124, 2444, 126, 93, 4356, 2882, 37, 1793, 124, 49, 93, 1185, 1082, 1023, 1015, 2023, 1025, 1015, 13749, 1027, 1043, 1405, 37, 1023, 1423, 1982, 47, 6698, 1039, 1015, 4994, 6517, 1025, 1015, 8710, 1978, 1015, 1652, 1025, 1015, 13749, 2448, 1313, 1641, 1009, 1043, 1405, 1183, 93, 4050, 2228, 124, 2444, 2660, 49, 1283, 93, 1886, 84, 1068, 2894, 124, 2444, 1142, 62, 93, 1135, 124, 1380, 93, 1299, 1073, 51, 2835, 1335, 1150, 93, 1167, 124, 84, 46, 2444, 126, 93, 4356, 1156, 1015, 1428, 37, 110, 93, 2664, 49, 37, 1015, 1868, 2101, 11626, 1015, 4994, 6517, 47, 3201, 2745, 4007, 6320, 1046, 1226, 1032, 2123, 1095, 1530, 1122, 1147, 3382, 2891, 1025, 1015, 6933, 2428, 1025, 3335, 1120, 1795, 1652, 47, 1111, 3344, 1025, 1015, 5192, 1043, 1297, 62, 49, 37, 1023, 6112, 1144, 1015, 5957, 13749, 45, 98, 4493, 1271, 1226, 1032, 10317, 1061, 1012, 3721, 2227, 1025, 1015, 2428, 1025, 3335, 47, 1802, 16770, 1015, 1652, 1628, 1015, 4285, 1095, 1015, 1982, 1025, 1015, 4493, 1023, 1043, 1405, 1186, 66, 7016, 1054, 3143, 1025, 1464, 5860, 1027, 4668, 1025, 1015, 4409, 3024, 1777, 5109, 1008, 93, 1137, 124, 13161, 1150, 3489, 1043, 1405, 110, 93, 1181, 50, 37, 1015, 5957, 13749, 4493, 14460, 2237, 1015, 13749, 1068, 1084, 124, 2444, 1142, 1025, 1015, 8710, 1978, 1038, 37, 121, 93, 1181, 93, 1405, 37, 1027, 1061], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'num_chunks': 6}\n"
     ]
    }
   ],
   "source": [
    "# select the first 100 samples\n",
    "for i in range(100):\n",
    "    # print(test_dataset[i])\n",
    "    print(type(test_dataset[i]))\n",
    "    print(type(test_dataset[i]['input_ids'][0]))\n",
    "    print(test_dataset[i])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6665, 1233, 1015, 19822, 22516, 11746, 37, 6564, 1350, 2224]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['input_ids'][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_1.0%_30000_new\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latex prompt\n",
    "prompt = r\"$$\\int_{0}^{1} x^2 dx = ?$$\"\n",
    "# tokenize the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(inputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=tensor(nan, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan],\n",
       "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>)), (tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<AddBackward0>), tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]], device='cuda:0',\n",
       "       grad_fn=<TransposeBackward0>))), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs, labels=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Assuming model_checkpoint is defined somewhere and model is loaded as shown in the question\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_checkpoint).to('cuda')\n",
    "\n",
    "# Assuming test_dataset is defined as shown in the question\n",
    "# Convert the test dataset to a DataLoader for batch processing\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/5879 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m         \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# Prepare input and move tensors to the same device as model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Move model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "total_loss = 0\n",
    "total_examples = 0\n",
    "\n",
    "# Disable gradient calculations for evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        batch.to('cuda')\n",
    "        # Prepare input and move tensors to the same device as model\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch.get('attention_mask', None) if 'attention_mask' in batch else None\n",
    "        labels = input_ids.clone()  # For causal LM, labels are usually the input_ids shifted\n",
    "        \n",
    "        # Forward pass, returns a dictionary with 'loss' key among others\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        # Accumulate loss and number of examples\n",
    "        total_loss += outputs.loss.item() * input_ids.size(0)\n",
    "        total_examples += input_ids.size(0)\n",
    "\n",
    "        break\n",
    "# Calculate mean loss\n",
    "mean_loss = total_loss / total_examples\n",
    "\n",
    "# Perplexity calculation from the mean loss\n",
    "perplexity = torch.exp(torch.tensor(mean_loss))\n",
    "\n",
    "print(f\"Perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking the eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dosisiddhesh/miniconda3/envs/mist3_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import datasets\n",
    "path = '/home/dosisiddhesh/latex_model/data/sample_val_10.pkl'\n",
    "val = pickle.load(open(path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop 'token_type_ids', 'num_chunks' from the dataset\n",
    "val2 = val.remove_columns(['token_type_ids', 'num_chunks'])\n",
    "val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [93, 4362, 1838, 93, 4354, 124, 4313, 2630, 2897, 126, 93, 23999, 93, 7186, 1013, 93, 1539, 93, 23488, 1085, 6720, 1022, 12087, 46, 9402, 93, 17849, 1968, 93, 18435, 93, 12087, 9402, 126, 93, 1649, 124, 13523, 4475, 18105, 19634, 1009, 20382, 24532, 3447, 1027, 1015, 1043, 1546, 37, 24006, 1008, 1043, 1299, 79, 37, 15535, 93, 2757, 124, 4898, 4395, 1144, 1015, 7308, 1033, 1031, 3428, 1014, 2483, 22161, 106, 98, 1027, 7902, 4751, 16370, 2203, 17044, 126, 126, 93, 1166, 124, 75, 47, 4243, 14539, 93, 2757, 124, 6159, 59, 17449, 1465, 14539, 65, 118, 3287, 47, 1040, 126, 1027, 70, 47, 12603, 27508, 9335, 93, 2757, 124, 6159, 59, 1287, 1157, 9335, 65, 118, 3287, 47, 1040, 126, 126, 93, 4191, 124, 16901, 126, 1148, 22649, 1031, 71, 11256, 106, 126, 17997, 11128, 1785, 1148, 17419, 1031, 11332, 4893, 1148, 70, 46, 15353, 50, 11332, 4893, 45, 13557, 126, 93, 3083, 1022, 8901, 126, 93, 4730, 93, 20267, 124, 3311, 126, 93, 1116, 124, 3041, 126, 6067, 1009, 1015, 3406, 12003, 2163, 4638, 1144, 78, 47, 1921, 107, 123, 1023, 45, 7614, 47, 1371, 47, 24868, 47, 1022, 1139, 11154, 126, 41, 2809, 42, 9525, 45, 1025, 1043, 1299, 79, 37, 3672, 2812, 1015, 3509, 1025, 18105, 19634, 1009, 20382, 24532, 3447, 1079, 6380, 45, 2364, 98, 5821, 2874, 3567, 1025, 1015, 4161, 45, 1015, 1043, 1546, 37, 5451, 1038, 1043, 1665, 124, 116, 126, 62, 1948, 93, 1546, 62, 1278, 1829, 93, 1010, 124, 1476, 1065, 1469, 1261, 1230, 5732, 126, 37, 1027, 1120, 98, 1968, 1025, 37, 93, 3103, 93, 1546, 62, 5798, 93, 1010, 124, 2646, 1065, 2511, 126, 1022, 1230, 5732, 126, 37, 1008, 11235, 4818, 1120, 1015, 3741, 3855, 47, 1111, 2772, 10902, 7043, 1582, 1015, 6075, 1008, 1015, 1208, 1750, 2146, 1061, 3500, 1233, 1012, 15238, 1039, 4673, 1025, 3741, 1432, 47, 1249, 1525, 3524, 2114, 1690, 1452, 1072, 45, 2812, 4413, 45, 1015, 2032, 7947, 1218, 1039, 37, 93, 1665, 124, 116, 126, 93, 1801, 26271, 1022, 1230, 5732, 126, 1186, 93, 1080, 124, 3041, 126, 93, 3267, 3511, 6405, 126, 93, 10134, 1022, 1046, 21313, 59, 1303, 47, 1123, 47, 2375, 60, 1303, 47, 2153, 47, 18488, 60, 1303, 47, 2943, 47, 9968, 60, 1364, 47, 1865, 47, 77, 99, 60, 1404, 47, 2336, 47, 8082, 60, 1404, 47, 2336, 47, 66, 114, 1148, 126, 93, 3267, 3511, 6405, 126, 93, 10134, 1022, 1046, 18419, 59, 20382, 24532, 3447, 45, 13523, 7904, 45, 1043, 1299, 79, 1350, 15535, 45, 126, 93, 10134, 1022, 1046, 1043, 1546, 37, 24006, 45, 18105, 9225, 45, 11615, 5455, 1222, 93, 8128, 93, 9284, 124, 3233, 1065, 50, 126, 93, 1310, 124, 4122, 126, 1111, 1043, 1546, 37, 5451, 6743, 98, 14307, 4363, 1008, 5975, 1750, 5537, 3434, 93, 1137, 124, 7405, 2133, 1150, 11727, 1580, 45, 1046, 1226, 1032, 3379, 1061, 98, 2653, 5451, 1025, 1015, 1486, 4161, 37, 1939, 124, 51, 74, 45, 23477, 1321, 109, 41, 116, 1692, 1008, 1015, 37, 3368, 124, 51, 74, 1317, 23477, 126, 62, 2164, 124, 1728, 126, 37, 2984, 1008, 1043, 1299, 79, 37, 3672, 3566, 45, 1038, 98, 7024, 1025, 7273, 41, 4055, 42, 1750, 37, 93, 1665, 124, 116, 1743, 1948, 93, 1546, 62, 1278, 2218, 1022, 1230, 5732, 126, 37, 1027, 1120, 98, 1968, 37, 93, 3103, 93, 1546, 62, 3751, 1022, 1230, 5732, 1082, 93, 1137, 124, 2129, 2268, 1150, 1156, 1231, 2205, 1079, 1122, 12876, 1120, 1015, 6313, 1025, 5794, 1231, 5451, 1608, 2559, 2405, 1038, 4673, 45, 5669, 2799, 3709, 1027, 2804, 2874, 3567, 1025, 1015, 4161, 47, 1111, 6785, 4048, 1025, 9611, 4553, 45, 1008, 1818, 45, 1015, 1654, 1025, 1733, 2874, 3567, 2096, 2221, 4445, 1038, 1015, 5451, 1750, 1015, 4161, 3632, 8173, 7889, 1027, 4646, 1015, 3422, 1895, 6056, 1144, 14103, 93, 1137, 124, 1881, 3510, 1150, 2095, 1015, 1611, 2452, 45, 1976, 2874, 3567, 2549, 1122, 1858, 1009, 5173, 1978, 1027, 3299, 3718, 98, 24579, 1025, 1733, 6515, 10627, 3723, 1025, 1015, 3047, 4161, 47, 1440, 11559, 4553, 1015, 3886, 1025, 1012, 1750, 2249, 1316, 1015, 2874, 4475, 4161, 5779, 8361, 1120, 1015, 24201, 11814, 1324, 47, 1440, 2249, 1023, 5819, 3255, 1039, 4673, 45, 2221, 4445, 1038, 1095, 1787, 1015, 4161, 1023, 8173, 2064, 45, 1027, 14103, 2353, 1147, 3709, 1009, 1046, 47, 1156, 1611, 3520, 45, 1015, 4161, 1023, 15244, 1072, 2874, 1240, 14397, 1308, 5173, 1978, 3864, 15276, 1901, 1733, 1316, 45, 1054, 3896, 1038, 4673, 47, 1942, 1231, 1023, 1313, 1015, 1428, 1530, 1023, 8100, 3305, 1324, 1226, 1207, 2730, 1046, 60, 1324, 1226, 9119, 2874, 1240, 1617, 1015, 4161, 1842, 1015, 2276, 5170, 1038, 4673, 1623, 1032, 1008, 1818, 1725, 1816, 1233, 2955, 2220, 1008, 5173, 1978, 47, 1111, 10407, 2402, 1039, 7727, 12832, 5669, 2799, 1027, 16064, 1867, 1233, 1046, 45, 1023, 1144, 3298, 1025, 20382, 24532, 3447, 41, 1438, 4506, 1211, 1137, 124, 5032, 3288, 1150, 25887, 98, 6515, 5561, 2089, 1046, 9942, 2804, 7366, 45, 6515, 8408, 1027, 14103, 1027, 3939, 1054, 98, 3230, 10400, 7371, 1025, 20382, 2799, 5970, 47, 3502, 5071, 2405, 1009, 2744, 4601, 1018, 5645, 10633, 5071, 2769, 1023, 7727, 10910, 1008, 1733, 1208, 1750, 3476, 1271, 45, 1054, 1015, 1214, 3188, 45, 1226, 1582, 1032, 3500, 8668, 47, 1430, 3792, 6682, 1582, 19025, 10274, 1015, 3723, 1669, 1023, 4002, 1039, 1032, 37, 113, 95, 51, 5070, 53, 93, 1299, 103, 1781, 51, 37, 1120, 37, 113, 37, 1015, 2998, 3183, 1025, 1015, 19025, 5913, 1027, 37, 103, 37, 1015, 2684, 9378, 3344, 2277, 93, 1137, 124, 5032, 3288, 1150, 2417, 45, 1231, 3723, 3864, 3775, 1008, 1015, 2249, 3424, 4673, 47, 66, 2402, 1233, 4673, 45, 3699, 45, 1015, 3723, 11626, 2952, 1027, 1015, 9374, 1025, 14103, 3632, 14846, 2464, 47, 9150, 45, 1015, 1654, 1025, 2874, 3567, 2549, 1054, 19025, 10274, 1442, 1954, 2123, 1039, 1700, 45, 106, 47, 102, 47, 1015, 1022, 1046, 5434, 126, 2874, 4475, 6026, 9336, 1015, 4673, 2249, 45, 1027, 1855, 7376, 4373, 3551, 1904, 1027, 2772, 4123, 1025, 1015, 2032, 46, 7947, 6319, 1233, 4673, 1027, 1218, 1039, 2730, 50, 4019, 93, 1137, 124, 70, 75, 1866, 1150, 1111, 4198, 1025, 1438, 4506, 1039, 1893, 1966, 10274, 1027, 14459, 1061, 3467, 5506, 1025, 5949, 3632, 2562, 1308, 6745, 1122, 8356, 1061, 5374, 3312, 1842, 1008, 98, 8164, 3509, 93, 1137, 124, 25848, 3063, 1037, 11201, 1039, 1015, 1200, 2983, 18105, 46, 19634, 1009, 20382, 24532, 3447, 41, 15329, 1438, 4506, 42, 93, 1137, 124, 15278, 3411, 45, 12289, 3350, 45, 67, 8861, 2268, 1150, 2882, 45, 1015, 3723, 1039, 1561, 37, 79, 62, 50, 45, 51, 45, 52, 45, 93, 1418, 37, 1023, 3844, 1008, 2158, 1025, 98, 4147, 1025, 2158, 1025, 1015, 1488, 37, 102, 95, 79, 5070, 103, 1073, 9906, 126, 78, 1073, 79, 44, 50, 46, 9906, 126, 42, 1183, 1120, 37, 109, 62, 50, 45, 93, 1418, 45, 4087, 79, 44, 50, 3600, 51, 94, 1186, 2882, 45, 37, 103, 37, 1027, 37, 78, 37, 1122, 1015, 2684, 9378, 3344, 2277, 1027, 9225, 1868, 2381, 47, 1111, 6037, 37, 102, 37, 1023, 98, 5168, 1669, 1120, 4339, 1025, 1750, 8611, 1218, 1008, 2158, 1025, 1015, 19025, 7991, 1027, 1015, 3045, 37, 119, 1390, 1185, 37, 2442, 119, 95, 51, 62, 50, 37, 42, 1027, 2400, 46, 5410, 2457, 37, 108, 37, 1025, 1015, 14459, 45, 3188, 1015, 4283, 1884, 1061, 5030, 2357, 1015, 1179, 37, 1837, 67, 1263, 108, 4030, 124, 78, 126, 119, 44, 108, 1183, 1120, 37, 1837, 67, 37, 1015, 9225, 2998, 3183, 1027, 1043, 108, 4030, 124, 78, 1082, 1015, 9225, 1868, 1218, 1039, 5197, 3190, 1144, 2744, 6338, 1008, 1015, 15329, 1438, 4506, 3723, 47, 19340, 4180, 93, 1137, 124, 67, 8861, 2383, 45, 1921, 2203, 45, 71, 1881, 2203, 126, 1356, 45, 3699, 45, 1095, 1015, 3646, 2148, 1025, 1464, 1012, 3723, 1981, 1313, 1032, 1061, 3633, 1061, 1046, 1777, 1008, 1015, 8173, 5913, 1050, 1428, 47, 7645, 1038, 4673, 45, 1316, 1015, 1795, 9225, 1868, 3131, 2738, 1032, 4054, 45, 1530, 2899, 1982, 1414, 5197, 1039, 1015, 3672, 9221, 1054, 1015, 6543, 1486, 5455, 47, 1111, 5464, 11559, 4879, 13020, 1061, 1324, 1031, 4489, 1233, 1015, 4673, 2249, 47, 1156, 1015, 2298, 1428, 1025, 1043, 1299, 79, 37, 3672, 45, 98, 6146, 4318, 1442, 1954, 4638, 45, 1200, 3811, 45, 1218, 1039, 4704, 1561, 93, 1137, 124, 1921, 2203, 45, 71, 1881, 2203, 1037, 106, 47, 102, 47, 1218, 1039, 1027, 3808, 2158, 1025, 1561, 37, 50, 48, 103, 95, 51, 37, 41, 1714, 1561, 1296, 37, 50, 5070, 103, 95, 7718, 1121, 41, 2112, 1561, 42, 1027, 37, 50, 48, 103, 95, 53, 37, 1027, 37, 50, 5070, 103, 95, 51, 78, 95, 51, 1121, 41, 4704, 1561, 1289, 66, 2641, 1039, 4673, 2508, 3939, 1039, 7189, 1015, 1208, 1750, 4905, 45, 1120, 1015, 2049, 1095, 1015, 2112, 1561, 3711, 5781, 1401, 1039, 1032, 3129, 1667, 1015, 1714, 1561, 1324, 5796, 2757, 124, 6547, 3289, 1823, 1137, 124, 1921, 2203, 126, 1027, 3289, 1823, 1137, 124, 71, 1881, 2203, 126, 2672, 98, 2641, 1025, 1015, 1208, 1750, 3711, 3362, 1144, 15238, 1025, 3255, 1039, 4673, 1432, 1039, 4673, 1018, 1144, 15238, 1025, 1015, 1978, 1039, 1015, 3255, 1039, 4673, 2249, 2381, 45, 1842, 1582, 1015, 1714, 1324, 11097, 1015, 4527, 1025, 1015, 3480, 6338, 5312, 45, 1200, 1079, 1623, 6849, 2491, 1039, 3289, 1823, 1137, 124, 1921, 2203, 126, 1008, 1231, 7836, 47, 1150, 1430, 3896, 45, 1008, 1015, 37, 2164, 124, 1728, 1082, 2984, 1921, 107, 123, 1023, 93, 1137, 124, 1921, 2203, 126, 9029, 1095, 1015, 3672, 2639, 37, 1949, 124, 52, 1317, 52, 1321, 50, 37, 4271, 1233, 37, 2117, 47, 54, 6661, 1230, 4019, 3209, 52, 126, 37, 1038, 1714, 1561, 45, 1039, 37, 2943, 47, 54, 6661, 1230, 4019, 3209, 52, 126, 37, 1038, 2112, 1561, 1027, 1039, 37, 3452, 47, 53, 6661, 1230, 4019, 3209, 52, 126, 37, 1038, 4704, 1561, 47, 12945, 45, 1642, 10748, 1039, 2874, 1240, 1617, 1231, 4161, 1858, 1009, 1015, 26148, 1025, 1015, 2112, 1561, 2101, 1120, 2852, 1039, 1015, 1714, 1561, 1324, 45, 1623, 1022, 1046, 9119, 126, 9336, 15329, 1438, 4506, 1842, 1046, 1623, 4864, 7194, 1022, 1046, 7122, 126, 1039, 9336, 1015, 2276, 2032, 7947, 2510, 1008, 1015, 12196, 1025, 1015, 4673, 2249, 47, 2417, 45, 1039, 1032, 3516, 1120, 1015, 4318, 1025, 3289, 1823, 1137, 124, 1921, 2203, 126, 1015, 1817, 2874, 3567, 2096, 2738, 3279, 1878, 4042, 1015, 5273, 3646, 2148, 1025, 15329, 1438, 4506, 45, 1027, 5401, 45, 1039, 3852, 1120, 45, 1095, 1046, 7122, 16793, 1015, 3672, 1432, 3424, 4673, 47, 1111, 1043, 1546, 37, 5451, 1226, 1032, 5573, 1061, 1012, 3467, 3150, 1025, 5949, 2812, 15329, 1438, 4506, 1192, 1137, 124, 4809, 2383, 45, 6030, 2383, 45, 5609, 1866, 1150, 1440, 4553, 1015, 6072, 1025, 1146, 2146, 1878, 1015, 20382, 5561, 47, 2095, 1015, 1611, 2452, 45, 1015, 2874, 3567, 1025, 1015, 4161, 2749, 1015, 15108, 19987, 2767, 8490, 41, 74, 3225, 42, 2448, 1313, 4101, 1146, 2146, 1027, 1442, 5627, 1954, 2895, 1192, 1137, 124, 4935, 1866, 1037, 1842, 1015, 2112, 1561, 2158, 1315, 1954, 3113, 1039, 1032, 1716, 47, 1676, 98, 5015, 45, 1015, 1208, 1750, 4905, 3354, 1401, 1039, 1032, 1725, 1816, 1233, 2955, 2596, 2812, 15329, 1438, 4506, 1192, 1137, 124, 1921, 2203, 126, 41, 1650, 1525, 1015, 4571, 2939, 1289, 1156, 2298, 45, 1952, 1895, 37, 2692, 124, 1158, 8977, 1603, 47, 1364, 1183, 2777, 98, 9378, 8263, 3014, 2277, 37, 2198, 1022, 1299, 2999, 1743, 1158, 47, 1603, 1183, 98, 4154, 4423, 1324, 45, 3530, 1120, 1015, 3741, 1324, 37, 2198, 1022, 1299, 2999, 1743, 1364, 47, 53, 93, 1557, 49, 47, 50, 1186, 1156, 1015, 2613, 1700, 1079, 1356, 1320, 1015, 74, 3225, 2738, 1032, 5421, 1008, 1561, 1039, 7293, 106, 42, 7766, 2804, 14103, 45, 2348, 42, 1199, 1072, 1120, 15329, 1438, 4506, 1038, 4673, 1027, 5351, 42, 3524, 1015, 1043, 1546, 37, 5451, 3025, 4856, 1039, 4101, 3626, 3467, 2146, 1667, 2955, 5157, 1038, 4673, 47, 93, 1310, 124, 11615, 10261, 29221, 126, 1249, 8727, 15811, 1009, 1015, 3964, 1025, 3289, 1823, 1137, 124, 1921, 2203, 126, 1027, 2491, 1039, 1095, 1700, 1054, 1648, 4062, 47, 1111, 1043, 1299, 79, 37, 3672, 4161, 1023, 1638, 1144, 93, 1032, 2012, 1022, 1299, 79], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [2605, 1557, 62, 93, 1400, 118, 41, 93, 108, 4030, 124, 78, 126, 119, 44, 113, 40, 45, 93, 1359, 40, 42, 93, 1828, 92, 66, 1390, 1557, 44, 67, 1390, 1557, 124, 93, 5643, 1771, 124, 114, 126, 44, 93, 5643, 1771, 124, 114, 3961, 93, 1302, 51, 126, 93, 1828, 94, 118, 41, 93, 108, 4030, 124, 78, 126, 119, 44, 113, 45, 93, 1359, 42, 93, 1739, 1316, 2374, 108, 4030, 124, 78, 126, 119, 44, 113, 45, 114, 42, 37, 1027, 2374, 108, 4030, 124, 78, 126, 119, 44, 113, 3444, 114, 7015, 1122, 1015, 12533, 1027, 14802, 8263, 1027, 9378, 7991, 2381, 47, 1111, 10487, 8263, 1868, 1043, 108, 4030, 124, 78, 1082, 1027, 1015, 3045, 37, 119, 1390, 1185, 1183, 41, 37, 119, 95, 51, 62, 50, 37, 42, 4295, 1015, 9225, 2400, 46, 5410, 2457, 37, 113, 37, 1027, 37, 113, 2803, 1027, 1043, 1359, 37, 1027, 1043, 1359, 2803, 1122, 1015, 2156, 5761, 47, 1111, 3585, 1115, 1339, 2518, 17202, 93, 1557, 16993, 3770, 1039, 1015, 14316, 4316, 37, 85, 1073, 1454, 1743, 85, 10619, 93, 1415, 1073, 1454, 126, 8463, 1230, 106, 126, 93, 1662, 1073, 99, 1277, 126, 85, 3079, 93, 27125, 1183, 1120, 37, 98, 37, 1027, 37, 99, 37, 1015, 12533, 1027, 14802, 9378, 14316, 2175, 1008, 1015, 18975, 3128, 2381, 47, 37, 66, 1390, 1557, 37, 1027, 37, 67, 1390, 1557, 37, 1122, 11482, 5186, 1009, 1015, 13664, 1258, 110, 3186, 37, 116, 45, 117, 37, 1027, 37, 118, 37, 1271, 1054, 1009, 46, 5410, 17782, 1027, 18665, 6541, 37, 116, 44, 117, 44, 118, 62, 51, 41, 110, 95, 51, 44, 78, 95, 51, 42, 1183, 1120, 37, 110, 37, 1015, 2941, 9378, 1868, 1027, 37, 78, 37, 1015, 2941, 8263, 1868, 47, 1156, 1015, 4055, 2665, 45, 1015, 1486, 2595, 6026, 1122, 3844, 1061, 41, 1015, 10234, 1043, 1557, 37, 10121, 1054, 37, 107, 62, 109, 93, 1557, 50, 48, 51, 4307, 93, 1116, 124, 1745, 126, 1939, 124, 109, 1317, 93, 1557, 2605, 1557, 41, 116, 1709, 102, 12383, 1230, 106, 126, 93, 2838, 124, 109, 93, 1557, 2605, 1557, 41, 116, 1283, 124, 1022, 1230, 2512, 1053, 2838, 124, 109, 93, 1557, 2605, 1557, 41, 116, 42, 93, 1302, 114, 126, 62, 124, 50, 93, 1302, 1380, 93, 1299, 93, 1665, 124, 116, 1108, 39, 93, 1828, 41, 9633, 70, 44, 78, 8586, 2057, 109, 1390, 1557, 41, 116, 2979, 1068, 1665, 124, 116, 2048, 78, 42, 2363, 109, 1390, 1557, 41, 116, 42, 93, 2055, 1148, 39, 46, 39, 41, 70, 46, 78, 8586, 66, 1390, 23613, 124, 109, 93, 1557, 50, 126, 41, 116, 42, 46, 1068, 1665, 124, 116, 2171, 78, 42, 2363, 124, 109, 93, 1557, 50, 2605, 1557, 41, 116, 2543, 93, 1828, 42, 93, 1080, 124, 1745, 126, 1316, 45, 1054, 6149, 3672, 45, 37, 114, 62, 1431, 1862, 114, 5601, 1023, 1015, 4055, 3183, 1027, 37, 70, 1263, 1665, 124, 114, 95, 51, 44, 78, 95, 51, 126, 37, 1027, 37, 93, 1498, 1263, 1665, 124, 114, 95, 51, 44, 110, 95, 51, 126, 37, 1015, 4055, 8263, 1027, 9378, 4601, 2381, 47, 3370, 45, 37, 93, 1665, 124, 116, 1743, 70, 1337, 1498, 37, 1023, 1015, 2690, 4055, 1750, 1027, 37, 93, 2838, 124, 109, 93, 1557, 2605, 1557, 41, 116, 42, 37, 1122, 1015, 2276, 2032, 46, 7947, 47, 1111, 8516, 6026, 37, 2057, 109, 41, 116, 1121, 1027, 37, 2363, 109, 41, 116, 1121, 1122, 1884, 1144, 1015, 6354, 93, 1032, 93, 1828, 41, 2057, 109, 1390, 1557, 41, 116, 42, 45, 2363, 109, 1390, 1557, 41, 116, 42, 93, 1828, 42, 62, 93, 1885, 1872, 50, 1321, 50, 1022, 1230, 101, 126, 123, 1317, 93, 1828, 41, 66, 1390, 1557, 41, 116, 45, 123, 1296, 67, 1390, 1557, 41, 116, 45, 123, 42, 93, 1828, 42, 2164, 109, 41, 123, 42, 93, 1739, 1120, 37, 123, 62, 1022, 1230, 2343, 126, 93, 1394, 37, 1027, 1043, 1394, 37, 1015, 4055, 3672, 1336, 47, 1156, 2566, 45, 37, 66, 1390, 1557, 37, 1027, 37, 67, 1390, 1557, 37, 1226, 1032, 3844, 1061, 93, 1116, 124, 1745, 126, 66, 1390, 1557, 2675, 93, 1828, 41, 93, 1239, 1390, 1557, 44, 124, 116, 46, 118, 93, 1302, 53, 126, 93, 1378, 1390, 1557, 93, 1828, 42, 1148, 67, 1390, 1557, 2675, 93, 1828, 2132, 78, 11826, 117, 93, 1302, 25544, 1053, 1828, 42, 93, 1378, 1390, 1557, 93, 1080, 124, 1745, 126, 1120, 37, 117, 3988, 9334, 95, 51, 41, 50, 8319, 1230, 2343, 1053, 1394, 1121, 1027, 37, 118, 62, 78, 95, 51, 44, 110, 95, 51, 46, 19597, 93, 1498, 46, 9334, 95, 51, 1022, 1230, 2343, 126, 93, 1394, 1186, 1619, 3723, 1025, 1966, 1043, 1239, 1390, 1557, 37, 1027, 1043, 1378, 1390, 1557, 37, 3068, 1015, 2839, 1025, 15329, 1438, 4506, 1442, 1954, 1638, 1008, 3289, 1823, 1137, 124, 1921, 2203, 126, 1218, 1039, 4704, 1561, 45, 106, 47, 102, 47, 3808, 2158, 1025, 1561, 37, 102, 48, 103, 95, 51, 37, 41, 1714, 1561, 1296, 37, 102, 95, 51, 5070, 103, 95, 51, 78, 42, 37, 41, 2112, 1561, 42, 1027, 37, 102, 95, 52, 48, 103, 95, 53, 37, 1027, 37, 102, 95, 52, 5070, 103, 95, 51, 78, 95, 51, 1121, 41, 4704, 1561, 1289, 2077, 45, 3699, 45, 1008, 1733, 2413, 1015, 3047, 8263, 1868, 3389, 1442, 1954, 24977, 47, 1440, 5031, 1015, 4571, 1009, 6338, 98, 2702, 15365, 1184, 93, 2757, 124, 1430, 3896, 45, 1015, 37, 2164, 124, 1728, 1082, 3672, 2639, 3455, 1733, 5197, 37, 110, 48, 78, 37, 1039, 1206, 6338, 41, 1650, 13444, 2104, 2706, 46, 3350, 42, 1008, 1095, 3977, 1296, 4069, 1025, 17905, 1206, 13323, 1025, 1561, 37, 50, 5070, 103, 95, 51, 78, 95, 52, 1692, 37, 50, 5070, 103, 95, 53, 78, 1121, 1018, 2744, 1061, 29325, 1144, 1015, 3723, 4540, 1008, 1015, 20382, 5561, 10781, 1249, 1315, 12870, 1039, 2970, 3263, 1642, 7872, 1008, 2158, 1025, 37, 50, 48, 78, 37, 1018, 37, 50, 48, 103, 95, 51, 1186, 1676, 98, 5015, 1027, 1008, 4654, 1039, 3289, 1823, 1137, 124, 1921, 2203, 1037, 1530, 1023, 1147, 3389, 1025, 69, 45, 71, 45, 1027, 2744, 1486, 5455, 1009, 1015, 1208, 1750, 4905, 47, 2417, 45, 1015, 1208, 1750, 2146, 1122, 13806, 3500, 1144, 84, 1027, 81, 1486, 5455, 47, 3321, 2700, 4879, 1015, 1786, 3723, 45, 93, 1116, 124, 1179, 126, 1939, 124, 109, 1317, 93, 1557, 2605, 1557, 62, 1939, 124, 109, 1317, 93, 1557, 2660, 50, 42, 1317, 93, 1557, 126, 44, 1939, 124, 109, 1317, 93, 1557, 2660, 51, 42, 1317, 93, 1557, 126, 44, 1939, 124, 109, 1317, 93, 1557, 2660, 52, 42, 1317, 93, 1557, 2171, 93, 2303, 93, 1167, 124, 1169, 59, 103, 3723, 126, 93, 1080, 124, 1179, 126, 1316, 93, 1116, 124, 1745, 126, 1939, 124, 109, 1317, 93, 1557, 2660, 50, 42, 1317, 93, 1557, 126, 2675, 124, 110, 93, 1302, 103, 95, 51, 126, 1974, 124, 109, 1317, 93, 1557, 2660, 50, 45, 50, 42, 1317, 93, 1557, 126, 93, 3208, 1498, 93, 1302, 110, 9908, 1148, 1939, 124, 109, 1317, 93, 1557, 2660, 51, 42, 1317, 93, 1557, 126, 2675, 124, 110, 93, 1302, 103, 95, 51, 78, 126, 1974, 124, 109, 1317, 93, 1557, 2660, 50, 45, 51, 42, 1317, 93, 1557, 126, 93, 3208, 1498, 93, 1302, 110, 126, 4093, 1148, 1939, 124, 109, 1317, 93, 1557, 2660, 52, 42, 1317, 93, 1557, 126, 2675, 124, 110, 95, 52, 93, 1302, 103, 95, 53, 126, 1974, 124, 109, 1317, 93, 1557, 2660, 52, 45, 52, 42, 1317, 93, 1557, 126, 3329, 1022, 1498, 93, 1302, 110, 126, 4093, 44, 124, 110, 95, 52, 93, 1302, 103, 95, 51, 78, 95, 51, 126, 1974, 124, 109, 1317, 93, 1557, 2660, 50, 45, 52, 42, 1317, 93, 1557, 126, 93, 3208, 1498, 93, 1302, 110, 126, 4093, 93, 1167, 124, 1169, 59, 103, 3723, 51, 126, 93, 1080, 124, 1745, 126, 37, 117, 1704, 111, 45, 110, 42, 1317, 93, 1557, 126, 96, 124, 109, 1317, 93, 1557, 1082, 1122, 9482, 2072, 1025, 1015, 9482, 2389, 1043, 1498, 48, 110, 1183, 2703, 1025, 37, 103, 45, 78, 37, 1027, 37, 110, 1183, 3564, 6883, 5684, 1122, 5297, 2465, 1039, 1032, 10984, 2677, 47, 1111, 10450, 1025, 1015, 4028, 10250, 3964, 1623, 1032, 6346, 2939, 60, 37, 111, 44, 50, 37, 5477, 1015, 2476, 1008, 37, 50, 48, 103, 37, 1027, 37, 110, 37, 1015, 2690, 1561, 1008, 1015, 15329, 1438, 4506, 6624, 47, 1111, 14103, 2190, 37, 1022, 1230, 5382, 126, 1939, 124, 109, 1317, 93, 1557, 3209, 50, 126, 62, 46, 114, 37, 3632, 1008, 5173, 1978, 93, 1116, 124, 1745, 126, 1022, 1230, 5382, 2480, 1974, 124, 109, 1317, 93, 1557, 2660, 50, 45, 50, 42, 1317, 93, 1557, 126, 62, 1022, 1230, 5382, 2480, 1974, 124, 109, 1317, 93, 1557, 2660, 50, 45, 51, 42, 1317, 93, 1557, 126, 2675, 1022, 1230, 5382, 2480, 1974, 124, 109, 1317, 93, 1557, 2660, 50, 45, 52, 42, 1317, 93, 1557, 126, 62, 49, 1148, 1022, 1230, 5382, 2480, 1974, 124, 109, 1317, 93, 1557, 2660, 52, 45, 52, 42, 1317, 93, 1557, 126, 2675, 124, 114, 93, 1302, 110, 126, 125, 1974, 124, 109, 1317, 93, 1557, 2660, 50, 45, 50, 42, 1317, 93, 1557, 126, 2430, 51, 93, 1080, 124, 1745, 126, 1111, 3672, 9221, 37, 1949, 124, 109, 1174, 1557, 2605, 1557, 37, 1027, 3084, 8652, 37, 2692, 124, 109, 1174, 1557, 2605, 1557, 37, 1122, 1884, 1144, 93, 1116, 124, 1179, 126, 1022, 1230, 1434, 126, 1939, 124, 109, 1174, 1557, 2605, 1557, 62, 114, 1073, 9906, 126, 93, 1828, 41, 1949, 124, 109, 1174, 1557, 2605, 1557, 44, 114, 95, 51, 2692, 124, 109, 1174, 1557, 2605, 1557, 44, 93, 2303, 93, 1828, 42, 93, 1080, 124, 1179, 126, 12945, 45, 1015, 3723, 1025, 1969, 6870, 1084, 124, 1169, 59, 103, 3723, 1142, 1054, 1015, 6026, 14521, 1302, 1039, 1015, 4673, 2146, 45, 37, 1949, 124, 109, 1174, 1557, 2605, 1557, 37, 1027, 37, 2692, 124, 109, 1174, 1557, 2605, 1557, 37, 41, 1650, 2939, 1289, 93, 1310, 124, 1434, 46, 5472, 1015, 7248, 5722, 23284, 3018, 126, 1676, 1079, 1315, 7456, 45, 1008, 3289, 1823, 1137, 124, 1921, 2203, 1037, 98, 6146, 3723, 1054, 1015, 2718, 4236, 117, 37, 6026, 1043, 1239, 1224, 1557, 1082, 1027, 1043, 1378, 1224, 1557, 126, 37, 1777, 1628, 4002, 45, 1842, 1733, 2744, 1561, 2158, 2302, 24977, 1008, 1015, 1486, 2595, 6026, 47, 1440, 5031, 1015, 4571, 2730, 6338, 98, 2702, 17270, 45, 2221, 1816, 6338, 1122, 5496, 47, 1156, 1206, 1193, 1786, 10705, 1079, 2970, 3263, 1015, 6026, 45, 1027, 11330, 1015, 4673, 1027, 3255, 1039, 4673, 2146, 1008, 1015, 17896, 1025, 15329, 1438, 4506, 47, 1430, 1015, 3672, 9221, 1079, 1315, 93, 1116, 124, 1179, 126, 1949, 124, 51, 74, 1317, 23477, 1321, 109, 110, 1073, 9906, 44, 50, 126, 62, 124, 110, 93, 1302, 103, 95, 51, 126, 93, 1239, 1704, 50, 45, 50, 1283, 44, 124, 110, 95, 51, 93, 1302, 103, 95, 51, 78, 126, 93, 1239, 1704, 50, 45, 51, 1283, 44, 124, 110, 95, 52, 93, 1302, 103, 95, 51, 78, 95, 51, 126, 93, 1239, 1704, 50, 45, 52, 1283, 44, 124, 110, 95, 52, 93, 1302, 103, 95, 53, 126, 93, 1239, 1704, 52, 45, 52, 1283, 44, 93, 1080, 124, 1179, 126, 1027, 98, 2204, 3085, 1054, 37, 2692, 124, 51, 74, 1317, 23477, 1321, 109, 110, 1073, 9906, 44, 52, 126, 1186, 3396, 1231, 3723, 1079, 1981, 1016, 6672, 1015, 2641, 1025, 3289, 1823, 1137, 124, 1921, 2203, 1150, 1430, 98, 3775, 3698, 1079, 1207, 1200, 1054, 1015, 1022, 1046, 1824, 126, 3741, 1432, 45, 1027, 1120, 1015, 1022, 1046, 1824, 126, 3105, 1904, 1025, 1015, 2146, 45, 37, 78, 62, 21278, 7912, 1230, 5732, 126, 1183, 37, 110, 62, 7341, 1317, 1022, 1230, 5732, 126, 1183, 37, 103, 62, 2554, 1022, 1230, 5732, 126, 37, 1027, 37, 27421, 62, 50, 47, 1753, 1186, 1111, 2049, 1025, 1015, 2641, 1023, 3437, 1008, 2976, 50, 47, 1676, 1079, 1650, 1233, 1015, 1803, 1015, 4271, 1008, 1015, 2146, 1122, 1716, 45, 1027, 1122, 6525, 2812, 1406, 2745, 8712, 47, 1440, 1777, 1039, 1032, 3333], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [2221, 1015, 3286, 1633, 1921, 107, 123, 1023, 40, 116, 2146, 1027, 7031, 1023, 1025, 2744, 1561, 47, 1802, 1023, 1525, 2010, 16721, 1095, 1015, 3931, 1043, 1564, 95, 51, 62, 1123, 47, 2293, 37, 1023, 6849, 4186, 1401, 1025, 37, 2692, 124, 49, 11613, 9147, 37, 1027, 1043, 1359, 37, 1271, 7619, 1120, 52, 47, 52, 1027, 56, 47, 52, 2381, 1039, 1015, 2690, 1043, 1564, 95, 51, 1186, 1111, 9055, 2049, 10322, 1015, 1043, 1359, 1350, 2101, 1023, 20800, 2042, 1532, 1144, 1015, 9232, 1025, 7393, 1823, 1137, 124, 67, 8861, 2383, 45, 1921, 2203, 45, 71, 1881, 2203, 1150, 5114, 7857, 1231, 1016, 46, 5472, 4068, 1079, 1315, 2596, 23886, 1039, 7387, 1015, 4527, 1039, 1015, 3672, 9221, 1027, 3084, 8652, 1025, 1015, 6543, 1486, 5455, 45, 1061, 4638, 1008, 2976, 51, 47, 66, 2653, 26913, 3470, 14235, 1233, 2976, 51, 1023, 1095, 1015, 3711, 1025, 1561, 37, 50, 5070, 103, 95, 51, 78, 95, 51, 1121, 1023, 4009, 4154, 1716, 47, 11019, 1008, 1733, 2413, 45, 3699, 45, 1023, 1015, 3711, 1025, 1561, 37, 50, 48, 103, 95, 53, 37, 1525, 1716, 47, 1440, 1023, 1200, 1008, 1015, 37, 2164, 124, 1728, 1082, 2984, 1008, 2298, 47, 1156, 1077, 5697, 1580, 45, 1697, 1149, 2010, 1095, 1054, 1231, 2984, 1015, 26148, 1025, 1015, 4704, 1561, 3711, 1023, 2368, 1039, 1015, 26148, 1025, 1966, 1015, 37, 50, 48, 103, 95, 53, 37, 1027, 1015, 37, 50, 5070, 103, 95, 51, 78, 1121, 2158, 1027, 2448, 1313, 13031, 1233, 98, 21812, 1625, 1633, 1898, 4527, 47, 2050, 45, 1046, 6470, 1095, 3255, 1039, 4673, 1015, 37, 50, 48, 103, 95, 51, 37, 3723, 6184, 7464, 1667, 1015, 37, 50, 48, 78, 37, 3723, 47, 15082, 45, 1015, 2158, 1008, 37, 50, 48, 103, 95, 51, 37, 1027, 37, 50, 5070, 103, 95, 51, 78, 42, 37, 1122, 1025, 6938, 6620, 47, 3109, 2677, 45, 1046, 1023, 2653, 1095, 1642, 2874, 3567, 2096, 1623, 1582, 1032, 3516, 1120, 1015, 15329, 1438, 4506, 2089, 45, 1308, 1046, 29860, 1966, 1015, 1714, 1027, 1015, 2112, 1561, 1061, 10996, 3056, 47, 93, 1116, 124, 1803, 1565, 117, 94, 93, 1116, 124, 2023, 126, 93, 1116, 124, 2312, 1065, 100, 125, 100, 125, 100, 126, 39, 1921, 107, 123, 1023, 41, 3289, 1823, 2020, 1022, 1137, 124, 1921, 2203, 4463, 39, 1440, 4898, 1148, 93, 1879, 39, 3899, 1022, 1139, 74, 126, 39, 3899, 1022, 1139, 2152, 126, 1148, 93, 1879, 37, 3550, 37, 39, 3638, 51, 47, 2784, 1043, 1557, 37, 49, 47, 1536, 39, 3638, 51, 47, 3510, 1043, 1557, 37, 49, 47, 1536, 1148, 37, 4842, 37, 39, 50, 47, 2336, 1043, 1557, 37, 49, 47, 1563, 39, 50, 47, 1986, 1043, 1557, 37, 49, 47, 1563, 1148, 37, 9380, 37, 39, 3638, 50, 47, 1188, 1043, 1557, 37, 49, 47, 1653, 39, 3638, 50, 47, 1303, 1043, 1557, 37, 49, 47, 1653, 1148, 37, 24857, 37, 39, 52, 47, 2153, 1043, 1557, 37, 49, 47, 1563, 39, 52, 47, 52, 1043, 1557, 37, 49, 47, 1615, 1148, 1043, 1402, 5028, 44, 93, 1402, 6027, 37, 39, 51, 47, 2336, 1043, 1557, 37, 49, 47, 52, 39, 50, 47, 57, 1043, 1557, 37, 49, 47, 53, 1148, 1043, 1402, 15823, 37, 39, 3638, 51, 47, 57, 1043, 1557, 37, 49, 47, 55, 39, 3638, 52, 47, 53, 1043, 1557, 37, 49, 47, 56, 1148, 1043, 1402, 2692, 55, 37, 39, 50, 47, 53, 1043, 1557, 37, 49, 47, 52, 39, 50, 47, 58, 1043, 1557, 37, 49, 47, 52, 1148, 37, 2692, 124, 1380, 2048, 93, 1402, 2692, 124, 1379, 126, 37, 39, 55, 47, 50, 1043, 1557, 37, 49, 47, 55, 39, 55, 47, 50, 1043, 1557, 37, 49, 47, 55, 1148, 37, 2692, 124, 1158, 1082, 39, 3638, 51, 47, 53, 1043, 1557, 37, 49, 47, 53, 39, 3638, 51, 47, 53, 1043, 1557, 37, 49, 47, 53, 1148, 93, 1080, 124, 2312, 126, 93, 1080, 124, 2023, 126, 93, 2762, 92, 26358, 102, 3220, 6010, 7248, 1750, 4905, 2220, 1233, 5472, 1015, 4673, 2146, 37, 1949, 124, 49, 11613, 2605, 1557, 1183, 37, 2692, 124, 49, 11613, 2605, 1557, 1183, 37, 1949, 124, 50, 1174, 1557, 2605, 1557, 37, 2596, 1008, 1015, 2613, 1700, 1027, 1008, 3289, 1823, 2020, 93, 1137, 124, 1921, 2203, 1037, 1015, 6929, 5883, 46, 7084, 18363, 10754, 1027, 1015, 8263, 1043, 1359, 4236, 2101, 1039, 1015, 15329, 1438, 4506, 5170, 47, 1249, 3279, 37, 78, 62, 21278, 7912, 1230, 5732, 126, 1183, 37, 110, 62, 7341, 7912, 1230, 5732, 126, 1183, 37, 103, 62, 2554, 1317, 1022, 1230, 5732, 126, 37, 1027, 37, 27421, 62, 50, 47, 1753, 1186, 8270, 1432, 1122, 4002, 1233, 3289, 1823, 1137, 124, 10015, 2943, 1150, 1111, 1714, 2216, 10227, 1039, 1015, 1700, 1025, 1921, 107, 123, 1023, 1192, 1137, 124, 1921, 2203, 126, 45, 1316, 1015, 2718, 37, 117, 37, 6026, 1043, 1239, 1390, 1557, 37, 1027, 1043, 1378, 1390, 1557, 37, 1315, 1954, 11814, 1608, 15329, 1438, 4506, 1842, 1313, 1015, 4673, 2146, 47, 1156, 1193, 1428, 45, 1079, 1315, 2970, 11814, 1015, 4673, 2146, 41, 1650, 1197, 1054, 4062, 42, 1027, 8548, 1039, 1015, 1824, 1432, 47, 1111, 3389, 1025, 1015, 6929, 5883, 46, 7084, 18363, 10754, 1027, 1015, 8263, 1043, 1359, 1350, 2101, 1009, 1015, 8548, 2146, 1226, 1032, 2596, 1008, 3289, 1823, 1137, 124, 1921, 2203, 1150, 2077, 1023, 1582, 1324, 3150, 1025, 5949, 47, 29160, 1008, 1642, 1669, 1315, 1954, 3929, 1144, 7038, 1015, 2276, 1669, 6309, 1095, 1015, 18270, 1043, 1564, 95, 51, 37, 1120, 2852, 1039, 1015, 5511, 2146, 9613, 1144, 1324, 2874, 47, 1111, 2641, 3991, 98, 1043, 3598, 1022, 1230, 1292, 1321, 51, 6829, 1123, 47, 2293, 1271, 1023, 18814, 4186, 1401, 1025, 37, 2692, 124, 49, 11613, 9147, 37, 1027, 1043, 1359, 37, 1271, 3433, 52, 47, 52, 1027, 56, 47, 52, 3711, 1039, 1015, 2690, 1043, 1564, 95, 51, 37, 2381, 47, 1111, 1895, 1025, 1015, 2690, 1043, 1564, 95, 51, 37, 1008, 3289, 1823, 1137, 124, 1921, 2203, 126, 1023, 1313, 1638, 1222, 93, 1080, 124, 1803, 126, 93, 1116, 124, 1803, 1565, 117, 94, 93, 1116, 124, 2023, 126, 93, 1116, 124, 2312, 4221, 125, 100, 125, 100, 125, 100, 125, 100, 125, 100, 125, 100, 125, 100, 4293, 126, 39, 37, 50, 48, 103, 95, 51, 37, 39, 37, 50, 5070, 103, 95, 7718, 42, 37, 39, 37, 50, 5070, 103, 95, 7718, 95, 51, 1121, 39, 37, 50, 48, 103, 95, 53, 37, 39, 12663, 39, 4930, 2104, 3289, 1823, 1137, 124, 10015, 2943, 1142, 1148, 93, 1879, 37, 1949, 124, 52, 1317, 50, 1321, 49, 37, 39, 3638, 49, 47, 2917, 39, 3638, 49, 47, 1702, 39, 44, 49, 47, 1615, 39, 3638, 49, 47, 1303, 39, 3638, 49, 47, 1865, 39, 3638, 49, 47, 3239, 1043, 1557, 37, 49, 47, 1722, 1148, 37, 1949, 124, 50, 45, 50, 1321, 49, 37, 39, 50, 47, 1758, 39, 3638, 49, 47, 1864, 39, 3638, 49, 47, 1615, 39, 49, 47, 3004, 39, 50, 47, 1602, 39, 50, 47, 1602, 1043, 1557, 37, 49, 47, 1556, 1148, 37, 1949, 124, 52, 1317, 52, 1321, 50, 37, 39, 1783, 47, 52, 39, 1891, 47, 50, 39, 3638, 50, 47, 55, 39, 3638, 49, 47, 52, 39, 3452, 47, 54, 39, 3452, 47, 54, 1043, 1557, 37, 49, 47, 57, 1148, 37, 1949, 124, 50, 1317, 52, 1321, 50, 37, 39, 3638, 1472, 47, 55, 39, 1404, 47, 52, 39, 3638, 52, 47, 49, 39, 3638, 54, 47, 51, 39, 3638, 1303, 47, 54, 39, 3638, 1303, 47, 54, 1043, 1557, 37, 50, 47, 52, 1148, 37, 1949, 124, 52, 1317, 50, 1321, 50, 37, 39, 3638, 1472, 47, 55, 39, 1303, 47, 57, 39, 3638, 50, 47, 56, 39, 3638, 58, 47, 55, 39, 3638, 1472, 47, 50, 39, 3638, 1472, 47, 51, 1043, 1557, 37, 49, 47, 55, 1148, 37, 1949, 124, 50, 1317, 50, 1321, 50, 37, 39, 3638, 3177, 47, 55, 39, 2691, 47, 58, 39, 3638, 52, 47, 55, 39, 3638, 2646, 47, 50, 39, 3638, 2153, 47, 53, 39, 3638, 2153, 47, 53, 1043, 1557, 37, 49, 47, 58, 1148, 93, 1080, 124, 2312, 126, 93, 1080, 124, 2023, 126, 93, 2762, 92, 26358, 102, 3220, 6010, 15329, 1438, 4506, 45, 6543, 1486, 84, 1027, 81, 2595, 46, 3672, 9221, 45, 37, 1949, 124, 51, 74, 1317, 23477, 1321, 109, 37, 41, 1008, 2060, 1230, 4019, 3209, 9906, 46, 50, 126, 37, 5918, 42, 1054, 1043, 1299, 79, 37, 3672, 1008, 1015, 1008, 15329, 1438, 4506, 45, 12507, 1061, 98, 2037, 1025, 2158, 1025, 1714, 1561, 37, 50, 48, 103, 95, 51, 1183, 2112, 1561, 45, 37, 50, 5070, 103, 95, 51, 78, 42, 1183, 1027, 4704, 1561, 45, 37, 50, 5070, 103, 95, 51, 78, 95, 51, 1121, 1027, 37, 50, 5070, 103, 95, 53, 42, 1186, 1111, 2037, 1025, 1206, 1015, 2158, 3991, 1015, 2690, 3672, 2639, 1669, 47, 1111, 3741, 1895, 2220, 1008, 3289, 1823, 1137, 124, 10015, 2943, 126, 1023, 1525, 10698, 47, 126, 93, 1080, 124, 1803, 126, 93, 1310, 124, 13523, 3567, 2096, 1054, 1015, 37, 2164, 124, 1728, 1082, 2984, 126, 3031, 2874, 3567, 2096, 1023, 1858, 1009, 5512, 45, 1061, 7525, 1144, 1015, 6515, 4318, 45, 1095, 1015, 5669, 3723, 1008, 2158, 37, 50, 48, 103, 95, 51, 1183, 1442, 98, 7453, 3646, 2148, 1667, 1015, 1795, 8263, 1868, 37, 50, 48, 78, 37, 5197, 47, 5107, 98, 5437, 1226, 1582, 1032, 4395, 1144, 4765, 1748, 1366, 1120, 3741, 1432, 45, 1027, 5453, 1530, 1122, 1733, 3406, 4373, 13965, 93, 1137, 124, 5789, 1866, 45, 2321, 1866, 126, 1039, 2573, 98, 5740, 2476, 6624, 1313, 11133, 1015, 5374, 9225, 5598, 27099, 1016, 4454, 1015, 17896, 1025, 11381, 5740, 4135, 1192, 1137, 124, 104, 1243, 2133, 1150, 4457, 45, 1079, 1356, 1095, 98, 5350, 5421, 3705, 1025, 1015, 1901, 46, 2559, 74, 3225, 2089, 5781, 1401, 1039, 3524, 1015, 37, 2164, 124, 1728, 1082, 2032, 46, 3621, 2114, 1690, 1452, 1072, 45, 3545, 1608, 1015, 1208, 1750, 2146, 3500, 1038, 4673, 45, 106, 47, 102, 1171, 3025, 1016, 46, 5472, 2991, 1008, 1015, 2874, 4475, 1428, 47, 1111, 5598, 1023, 1039, 1717, 1015, 3723, 1025, 1015, 4288, 4161, 1008, 2158, 1025, 37, 110, 95, 51, 48, 103, 95, 51, 1186, 1111, 3723, 1025, 1015, 4161, 1226, 1032, 3844, 1061, 93, 1116, 124, 1179, 126, 1939, 124, 52, 1317, 52, 1321, 50, 1068, 1498, 45, 110, 45, 103, 45, 78, 42, 62, 124, 110, 93, 1302, 103, 95, 51, 126, 117, 1704, 50, 1283, 41, 93, 1498, 48, 110, 1317, 45, 1317, 110, 48, 78, 42, 44, 124, 110, 95, 52, 93, 1302, 103, 95, 53, 126, 117, 1704, 52, 1283, 41, 93, 1498, 48, 110, 1317, 45, 1317, 110, 48, 78, 42, 44, 93, 1418, 93, 1080, 124, 1179, 126, 1316, 1079, 1315, 5312, 1817, 1095, 1015, 2072, 37, 117, 1704, 3155, 44, 50, 2871, 1122, 9482, 45, 1027, 3299, 1641, 1582, 1009, 9482, 3186, 45, 1464, 1061, 1043, 1498, 48, 110, 37, 1027, 37, 110, 48, 78, 1186, 1430, 1015, 12356, 1025, 98, 21270, 3964, 45, 1015, 2113, 3855, 37, 109, 37, 37, 74, 37, 1027, 37, 75, 37, 1315, 1954, 6608, 1072, 8445, 47, 28786, 6336, 14103, 1008, 1231, 3723, 4553, 1038, 6543, 1561, 45, 93, 6021, 1022, 1230, 5382, 2480, 117, 1704, 50, 5149, 1498, 48, 110, 45, 110, 48, 78, 42, 2675, 49, 1148, 1022, 1230, 5382, 2480, 117, 1704, 52, 5149, 1498, 48, 110, 45, 110, 48, 78, 42, 2675, 124, 114, 93, 1302, 110, 126, 125, 117, 1704, 50, 5149, 1498, 48, 110, 45, 110, 48, 78, 7127, 51, 93, 6221, 1271, 1008, 3354, 6353, 1012, 3576, 1597, 1025, 2606, 1008, 1015, 37, 50, 48, 78, 37, 3723, 47, 1111, 2072, 37, 117, 1704, 50, 2871, 1027, 37, 117, 1704, 52, 2871, 1122, 1582, 2559, 1008, 98, 2970, 37, 110, 48, 78, 37, 3723, 45, 93, 1116, 124, 1179, 126, 117, 1704, 3155, 44, 50, 1283, 41, 93, 1498, 48, 110, 45, 110, 48, 78, 42, 62, 117, 1704, 3155, 44, 50, 45, 3155, 44, 50, 1283, 41, 93, 1498, 48, 110, 42, 44, 124, 110, 93, 1302, 78, 126, 1317, 117, 1704, 3155, 44, 50, 45, 3155, 44, 51, 1283, 41, 93], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m val2:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mist3_env/lib/python3.10/site-packages/ipykernel/kernelbase.py:1270\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1268\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mist3_env/lib/python3.10/site-packages/ipykernel/kernelbase.py:1313\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "for i in val2:\n",
    "    print(i)\n",
    "    input()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_1.0%_30000_new', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<cls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_1.0%_30000_new')\n",
    "tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the token having the token id as 0\n",
    "tokenizer.decode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<cls>', '<sep>', '<mask>', '<unk>', '<bos>', '<eos>', '\\x00')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(1), tokenizer.decode(2), tokenizer.decode(3), tokenizer.decode(4), tokenizer.decode(5), tokenizer.decode(6), tokenizer.decode(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the id of <pad> token from tokenizer\n",
    "tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '<pad>',\n",
    "                                                'cls_token': '<cls>',\n",
    "                                                'sep_token': '<sep>',\n",
    "                                                'mask_token': '<mask>',\n",
    "                                                'unk_token': '<unk>',\n",
    "                                                'bos_token': '<bos>',\n",
    "                                                'eos_token': '<eos>'\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mist3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
