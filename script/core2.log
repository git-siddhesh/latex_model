______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

test:False,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:0,  max_grad_norm:0.9,  start_month_index:8,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+

+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |  4000  |
|        SAVE STEPS       |  2000  |
|      LOGGING STEPS      |  500   |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
MODEL PATH: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
Tokenizer loaded__________________________________________________________
____________________________________________________________________________________________________
Loading the model

+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED took __________ 0.044197173913319905 minutes_________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|         Training Argument          |                                                    Value                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|             output_dir             |                              /home/iitgn_cse/latex_model/model_main_fp32_2024-                              |
|                                    |                                 04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-                                |
|                                    |                              05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30                             |
|                                    |                              000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4                             |
|                                    |                                                     096                                                     |
|        overwrite_output_dir        |                                                     True                                                    |
|              do_train              |                                                     True                                                    |
|              do_eval               |                                                     True                                                    |
|             do_predict             |                                                    False                                                    |
|        evaluation_strategy         |                                                    steps                                                    |
|        prediction_loss_only        |                                                    False                                                    |
|    per_device_train_batch_size     |                                                      1                                                      |
|     per_device_eval_batch_size     |                                                      1                                                      |
|      per_gpu_train_batch_size      |                                                     None                                                    |
|      per_gpu_eval_batch_size       |                                                     None                                                    |
|    gradient_accumulation_steps     |                                                      32                                                     |
|      eval_accumulation_steps       |                                                      32                                                     |
|             eval_delay             |                                                      0                                                      |
|           learning_rate            |                                                    2e-05                                                    |
|            weight_decay            |                                                     0.1                                                     |
|             adam_beta1             |                                                     0.9                                                     |
|             adam_beta2             |                                                    0.999                                                    |
|            adam_epsilon            |                                                    1e-08                                                    |
|           max_grad_norm            |                                                     0.9                                                     |
|          num_train_epochs          |                                                      1                                                      |
|             max_steps              |                                                      -1                                                     |
|         lr_scheduler_type          |                                                    cosine                                                   |
|        lr_scheduler_kwargs         |                                                      {}                                                     |
|            warmup_ratio            |                                                     0.0                                                     |
|            warmup_steps            |                                                     100                                                     |
|             log_level              |                                                    debug                                                    |
|         log_level_replica          |                                                   warning                                                   |
|          log_on_each_node          |                                                     True                                                    |
|            logging_dir             |                                                    ./logs                                                   |
|          logging_strategy          |                                                    steps                                                    |
|         logging_first_step         |                                                    False                                                    |
|           logging_steps            |                                                     500                                                     |
|       logging_nan_inf_filter       |                                                     True                                                    |
|           save_strategy            |                                                    steps                                                    |
|             save_steps             |                                                     2000                                                    |
|          save_total_limit          |                                                      3                                                      |
|          save_safetensors          |                                                     True                                                    |
|         save_on_each_node          |                                                    False                                                    |
|          save_only_model           |                                                    False                                                    |
|              no_cuda               |                                                    False                                                    |
|              use_cpu               |                                                    False                                                    |
|           use_mps_device           |                                                    False                                                    |
|                seed                |                                                      42                                                     |
|             data_seed              |                                                     None                                                    |
|           jit_mode_eval            |                                                    False                                                    |
|              use_ipex              |                                                    False                                                    |
|                bf16                |                                                    False                                                    |
|                fp16                |                                                    False                                                    |
|           fp16_opt_level           |                                                      O1                                                     |
|       half_precision_backend       |                                                     auto                                                    |
|           bf16_full_eval           |                                                    False                                                    |
|           fp16_full_eval           |                                                    False                                                    |
|                tf32                |                                                     None                                                    |
|             local_rank             |                                                      0                                                      |
|            ddp_backend             |                                                     None                                                    |
|           tpu_num_cores            |                                                     None                                                    |
|         tpu_metrics_debug          |                                                    False                                                    |
|               debug                |                                                      []                                                     |
|        dataloader_drop_last        |                                                    False                                                    |
|             eval_steps             |                                                     4000                                                    |
|       dataloader_num_workers       |                                                      0                                                      |
|     dataloader_prefetch_factor     |                                                     None                                                    |
|             past_index             |                                                      -1                                                     |
|              run_name              |                              run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-                              |
|                                    |                                               07-17_test=False                                              |
|            disable_tqdm            |                                                    False                                                    |
|       remove_unused_columns        |                                                     True                                                    |
|            label_names             |                                                     None                                                    |
|       load_best_model_at_end       |                                                    False                                                    |
|       metric_for_best_model        |                                                     None                                                    |
|         greater_is_better          |                                                     None                                                    |
|          ignore_data_skip          |                                                    False                                                    |
|                fsdp                |                                                      []                                                     |
|        fsdp_min_num_params         |                                                      0                                                      |
|            fsdp_config             |            {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}           |
| fsdp_transformer_layer_cls_to_wrap |                                                     None                                                    |
|         accelerator_config         | AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True) |
|             deepspeed              |                                                     None                                                    |
|       label_smoothing_factor       |                                                     0.0                                                     |
|               optim                |                                                 adamw_torch                                                 |
|             optim_args             |                                                     None                                                    |
|             adafactor              |                                                    False                                                    |
|          group_by_length           |                                                    False                                                    |
|         length_column_name         |                                                    length                                                   |
|             report_to              |                                                  ['wandb']                                                  |
|     ddp_find_unused_parameters     |                                                     None                                                    |
|         ddp_bucket_cap_mb          |                                                     None                                                    |
|       ddp_broadcast_buffers        |                                                     None                                                    |
|       dataloader_pin_memory        |                                                     True                                                    |
|   dataloader_persistent_workers    |                                                    False                                                    |
|        skip_memory_metrics         |                                                     True                                                    |
|     use_legacy_prediction_loop     |                                                    False                                                    |
|            push_to_hub             |                                                    False                                                    |
|       resume_from_checkpoint       |                                                     None                                                    |
|            hub_model_id            |                                                     None                                                    |
|            hub_strategy            |                                                  every_save                                                 |
|             hub_token              |                                                     None                                                    |
|          hub_private_repo          |                                                    False                                                    |
|          hub_always_push           |                                                    False                                                    |
|       gradient_checkpointing       |                                                    False                                                    |
|   gradient_checkpointing_kwargs    |                                                     None                                                    |
|     include_inputs_for_metrics     |                                                    False                                                    |
|            fp16_backend            |                                                     auto                                                    |
|        push_to_hub_model_id        |                                                     None                                                    |
|      push_to_hub_organization      |                                                     None                                                    |
|         push_to_hub_token          |                                                     None                                                    |
|           mp_parameters            |                                                                                                             |
|        auto_find_batch_size        |                                                    False                                                    |
|          full_determinism          |                                                    False                                                    |
|            torchdynamo             |                                                     None                                                    |
|             ray_scope              |                                                     last                                                    |
|            ddp_timeout             |                                                     1800                                                    |
|           torch_compile            |                                                    False                                                    |
|       torch_compile_backend        |                                                     None                                                    |
|         torch_compile_mode         |                                                     None                                                    |
|          dispatch_batches          |                                                     None                                                    |
|           split_batches            |                                                     None                                                    |
|     include_tokens_per_second      |                                                    False                                                    |
|   include_num_input_tokens_seen    |                                                    False                                                    |
|        neftune_noise_alpha         |                                                     None                                                    |
|         distributed_state          |                                 Distributed environment: DistributedType.NO                                 |
|                                    |                                               Num processes: 1                                              |
|                                    |                                               Process index: 0                                              |
|                                    |                                            Local process index: 0                                           |
|                                    |                                                 Device: cuda                                                |
|                                    |                                                                                                             |
|               _n_gpu               |                                                      1                                                      |
|      __cached__setup_devices       |                                                    cuda:0                                                   |
|          deepspeed_plugin          |                                                     None                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+
______________________________________________________________________________________________________________________________________________________
Training for 0-8
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_8_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_8_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 185322
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47481
}) 
 Took0.5766875465710958 minutes
Popen(['git', 'cat-file', '--batch-check'], cwd=/home/iitgn_cse/latex_model, stdin=<valid stream>, shell=False, universal_newlines=False)
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5791, training_loss=2.202342282879684, metrics={'train_runtime': 41694.0175, 'train_samples_per_second': 4.445, 'train_steps_per_second': 0.139, 'train_loss': 2.202342282879684, 'epoch': 1.0})
 Took694.9260531147321 minutes
GPU memory usage before cleaning cache: 35836.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.401905059814453, 'eval_runtime': 1821.9596, 'eval_samples_per_second': 26.06, 'eval_steps_per_second': 26.06, 'epoch': 1.0}
 Took30.368993190924325 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.0079, 'grad_norm': 3.51334285736084, 'learning_rate': 1.9757200802888278e-05, 'epoch': 0.09, 'step': 500}, {'loss': 1.9976, 'grad_norm': 4.018144607543945, 'learning_rate': 1.8791001183919126e-05, 'epoch': 0.17, 'step': 1000}, {'loss': 1.994, 'grad_norm': 3.7757203578948975, 'learning_rate': 1.7159310873021694e-05, 'epoch': 0.26, 'step': 1500}, {'loss': 2.04, 'grad_norm': 3.6978724002838135, 'learning_rate': 1.4985651035907816e-05, 'epoch': 0.35, 'step': 2000}, {'loss': 2.0886, 'grad_norm': 3.601090431213379, 'learning_rate': 1.2434570665206874e-05, 'epoch': 0.43, 'step': 2500}, {'loss': 2.155, 'grad_norm': 3.662611961364746, 'learning_rate': 9.69918999893772e-06, 'epoch': 0.52, 'step': 3000}, {'loss': 2.2358, 'grad_norm': 4.015500068664551, 'learning_rate': 6.986581057021424e-06, 'epoch': 0.6, 'step': 3500}, {'loss': 2.2976, 'grad_norm': 3.844238758087158, 'learning_rate': 4.502092009014481e-06, 'epoch': 0.69, 'step': 4000}, {'eval_loss': 2.4381582736968994, 'eval_runtime': 1818.3585, 'eval_samples_per_second': 26.112, 'eval_steps_per_second': 26.112, 'epoch': 0.69, 'step': 4000}, {'loss': 2.3648, 'grad_norm': 3.7141716480255127, 'learning_rate': 2.433802039648527e-06, 'epoch': 0.78, 'step': 4500}, {'loss': 2.4198, 'grad_norm': 3.48384428024292, 'learning_rate': 9.382834999021373e-07, 'epoch': 0.86, 'step': 5000}, {'loss': 2.4611, 'grad_norm': 4.011415958404541, 'learning_rate': 1.2874916804543557e-07, 'epoch': 0.95, 'step': 5500}, {'train_runtime': 41694.0175, 'train_samples_per_second': 4.445, 'train_steps_per_second': 0.139, 'total_flos': 5.033891193229148e+17, 'train_loss': 2.202342282879684, 'epoch': 1.0, 'step': 5791}, {'eval_loss': 2.401905059814453, 'eval_runtime': 1821.9596, 'eval_samples_per_second': 26.06, 'eval_steps_per_second': 26.06, 'epoch': 1.0, 'step': 5791}]
Model STATE saved successfully 
 Took1.0947386423746744e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8956551829973857 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 0-8 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 0-9
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_9_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_9_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 176889
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47312
}) 
 Took0.6973181049029032 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5527, training_loss=2.5031826032400777, metrics={'train_runtime': 30211.1028, 'train_samples_per_second': 5.855, 'train_steps_per_second': 0.183, 'train_loss': 2.5031826032400777, 'epoch': 1.0})
 Took503.546858839194 minutes
GPU memory usage before cleaning cache: 35876.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.3370137214660645, 'eval_runtime': 1734.8073, 'eval_samples_per_second': 27.272, 'eval_steps_per_second': 27.272, 'epoch': 1.0}
 Took28.928286957740784 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.5789, 'grad_norm': 3.6314802169799805, 'learning_rate': 1.9733112171333944e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.5823, 'grad_norm': 3.01324462890625, 'learning_rate': 1.8673249484642752e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.5707, 'grad_norm': 3.139439821243286, 'learning_rate': 1.689183440322883e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.548, 'grad_norm': 3.528080701828003, 'learning_rate': 1.4537067935291622e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.5237, 'grad_norm': 3.214386463165283, 'learning_rate': 1.1804849828320395e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.5079, 'grad_norm': 3.1010096073150635, 'learning_rate': 8.922481109626837e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.487, 'grad_norm': 2.8776462078094482, 'learning_rate': 6.1297542647297615e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.4507, 'grad_norm': 2.872499465942383, 'learning_rate': 3.659004215656943e-06, 'epoch': 0.72, 'step': 4000}, {'eval_loss': 2.356057643890381, 'eval_runtime': 1735.6159, 'eval_samples_per_second': 27.259, 'eval_steps_per_second': 27.259, 'epoch': 0.72, 'step': 4000}, {'loss': 2.4369, 'grad_norm': 3.0552256107330322, 'learning_rate': 1.715779715251994e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.4326, 'grad_norm': 3.279902219772339, 'learning_rate': 4.6174315939476963e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.419, 'grad_norm': 2.997612953186035, 'learning_rate': 1.2214305934699078e-09, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 30211.1028, 'train_samples_per_second': 5.855, 'train_steps_per_second': 0.183, 'total_flos': 3.318688745374433e+17, 'train_loss': 2.5031826032400777, 'epoch': 1.0, 'step': 5527}, {'eval_loss': 2.3370137214660645, 'eval_runtime': 1734.8073, 'eval_samples_per_second': 27.272, 'eval_steps_per_second': 27.272, 'epoch': 1.0, 'step': 5527}]
Model STATE saved successfully 
 Took1.1078516642252605e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.9435601671536763 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 0-9 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 0-10
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_10_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_10_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180646
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47431
}) 
 Took0.7727700670560201 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5645, training_loss=2.446635535673508, metrics={'train_runtime': 35233.0971, 'train_samples_per_second': 5.127, 'train_steps_per_second': 0.16, 'train_loss': 2.446635535673508, 'epoch': 1.0})
 Took587.2556286891302 minutes
GPU memory usage before cleaning cache: 35836.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.2954585552215576, 'eval_runtime': 1788.3384, 'eval_samples_per_second': 26.522, 'eval_steps_per_second': 26.522, 'epoch': 1.0}
 Took29.80880964199702 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.5092, 'grad_norm': 2.9751784801483154, 'learning_rate': 1.9744302141513234e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.538, 'grad_norm': 2.852600336074829, 'learning_rate': 1.8727899859338823e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.4998, 'grad_norm': 3.1036202907562256, 'learning_rate': 1.7015768467520824e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.4901, 'grad_norm': 2.9644899368286133, 'learning_rate': 1.474438749355379e-05, 'epoch': 0.35, 'step': 2000}, {'loss': 2.4632, 'grad_norm': 3.1875953674316406, 'learning_rate': 1.2094816056830412e-05, 'epoch': 0.44, 'step': 2500}, {'loss': 2.4492, 'grad_norm': 2.6688334941864014, 'learning_rate': 9.278260060983538e-06, 'epoch': 0.53, 'step': 3000}, {'loss': 2.4318, 'grad_norm': 3.0402286052703857, 'learning_rate': 6.519236288610659e-06, 'epoch': 0.62, 'step': 3500}, {'loss': 2.402, 'grad_norm': 2.8612289428710938, 'learning_rate': 4.0376754428103725e-06, 'epoch': 0.71, 'step': 4000}, {'eval_loss': 2.3158397674560547, 'eval_runtime': 1791.7142, 'eval_samples_per_second': 26.472, 'eval_steps_per_second': 26.472, 'epoch': 0.71, 'step': 4000}, {'loss': 2.3886, 'grad_norm': 2.633077383041382, 'learning_rate': 2.031390759939601e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 2.3836, 'grad_norm': 2.841747760772705, 'learning_rate': 6.603096771368234e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 2.3731, 'grad_norm': 2.58789324760437, 'learning_rate': 3.3725499362091685e-08, 'epoch': 0.97, 'step': 5500}, {'train_runtime': 35233.0971, 'train_samples_per_second': 5.127, 'train_steps_per_second': 0.16, 'total_flos': 4.0592440877789184e+17, 'train_loss': 2.446635535673508, 'epoch': 1.0, 'step': 5645}, {'eval_loss': 2.2954585552215576, 'eval_runtime': 1788.3384, 'eval_samples_per_second': 26.522, 'eval_steps_per_second': 26.522, 'epoch': 1.0, 'step': 5645}]
Model STATE saved successfully 
 Took1.060167948404948e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.9345137000083923 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 0-10 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 0-11
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_11_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_11_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178887
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47365
}) 
 Took0.7803868730862935 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5590, training_loss=2.4122124032172407, metrics={'train_runtime': 32905.7305, 'train_samples_per_second': 5.436, 'train_steps_per_second': 0.17, 'train_loss': 2.4122124032172407, 'epoch': 1.0})
 Took548.4553915659586 minutes
GPU memory usage before cleaning cache: 35676.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.2558741569519043, 'eval_runtime': 1759.7544, 'eval_samples_per_second': 26.916, 'eval_steps_per_second': 26.916, 'epoch': 1.0}
 Took29.33250022729238 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.4611, 'grad_norm': 2.8791728019714355, 'learning_rate': 1.973917570554464e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.4706, 'grad_norm': 2.844473361968994, 'learning_rate': 1.8702852410301556e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.4906, 'grad_norm': 2.689962387084961, 'learning_rate': 1.6958922493101844e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.4548, 'grad_norm': 2.687206983566284, 'learning_rate': 1.4649180425396972e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.429, 'grad_norm': 3.295351982116699, 'learning_rate': 1.196142541428197e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.4117, 'grad_norm': 2.732855796813965, 'learning_rate': 9.11419193034155e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.3947, 'grad_norm': 2.632277727127075, 'learning_rate': 6.3389812300037774e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.3868, 'grad_norm': 2.6183342933654785, 'learning_rate': 3.86143858177388e-06, 'epoch': 0.72, 'step': 4000}, {'eval_loss': 2.274614095687866, 'eval_runtime': 1757.9882, 'eval_samples_per_second': 26.943, 'eval_steps_per_second': 26.943, 'epoch': 0.72, 'step': 4000}, {'loss': 2.3583, 'grad_norm': 2.7637362480163574, 'learning_rate': 1.8830066278357395e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 2.3484, 'grad_norm': 2.6849029064178467, 'learning_rate': 5.645465991623167e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 2.3453, 'grad_norm': 2.7352488040924072, 'learning_rate': 1.325910115169471e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 32905.7305, 'train_samples_per_second': 5.436, 'train_steps_per_second': 0.17, 'total_flos': 3.714806355725353e+17, 'train_loss': 2.4122124032172407, 'epoch': 1.0, 'step': 5590}, {'eval_loss': 2.2558741569519043, 'eval_runtime': 1759.7544, 'eval_samples_per_second': 26.916, 'eval_steps_per_second': 26.916, 'epoch': 1.0, 'step': 5590}]
Model STATE saved successfully 
 Took1.1142094930013021e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.0255101680755616 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 0-11 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 0-12
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_12_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_12_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 179194
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47533
}) 
 Took0.7894824266433715 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5599, training_loss=2.3634083620627706, metrics={'train_runtime': 33403.833, 'train_samples_per_second': 5.364, 'train_steps_per_second': 0.168, 'train_loss': 2.3634083620627706, 'epoch': 1.0})
 Took556.7616201996804 minutes
GPU memory usage before cleaning cache: 35836.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.2215023040771484, 'eval_runtime': 1831.5825, 'eval_samples_per_second': 25.952, 'eval_steps_per_second': 25.952, 'epoch': 1.0}
 Took30.554282661279043 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.404, 'grad_norm': 2.5237112045288086, 'learning_rate': 1.9740025053250186e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.4336, 'grad_norm': 2.7545595169067383, 'learning_rate': 1.8707001032347e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.4215, 'grad_norm': 3.0315628051757812, 'learning_rate': 1.6968332742122285e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.404, 'grad_norm': 2.9095113277435303, 'learning_rate': 1.4664927632878405e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.3846, 'grad_norm': 2.7624130249023438, 'learning_rate': 1.1983461301554595e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.3712, 'grad_norm': 2.386338233947754, 'learning_rate': 9.141248681621957e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.3421, 'grad_norm': 2.553156614303589, 'learning_rate': 6.368632119428898e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.3213, 'grad_norm': 2.948885917663574, 'learning_rate': 3.890313665758348e-06, 'epoch': 0.71, 'step': 4000}, {'eval_loss': 2.2395241260528564, 'eval_runtime': 1830.7846, 'eval_samples_per_second': 25.963, 'eval_steps_per_second': 25.963, 'epoch': 0.71, 'step': 4000}, {'loss': 2.3161, 'grad_norm': 2.566927433013916, 'learning_rate': 1.907144475662378e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 2.3001, 'grad_norm': 2.4983372688293457, 'learning_rate': 5.798471640674519e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 2.3104, 'grad_norm': 2.617875814437866, 'learning_rate': 1.599031149093988e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 33403.833, 'train_samples_per_second': 5.364, 'train_steps_per_second': 0.168, 'total_flos': 3.78861451893547e+17, 'train_loss': 2.3634083620627706, 'epoch': 1.0, 'step': 5599}, {'eval_loss': 2.2215023040771484, 'eval_runtime': 1831.5825, 'eval_samples_per_second': 25.952, 'eval_steps_per_second': 25.952, 'epoch': 1.0, 'step': 5599}]
Model STATE saved successfully 
 Took1.0569890340169271e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.9766738454500834 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 0-12 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-8
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_8_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_8_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 176225
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47598
}) 
 Took0.783181611696879 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5507, training_loss=2.327674018897269, metrics={'train_runtime': 29546.3368, 'train_samples_per_second': 5.964, 'train_steps_per_second': 0.186, 'train_loss': 2.327674018897269, 'epoch': 1.0})
 Took492.47552626132966 minutes
GPU memory usage before cleaning cache: 35756.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.2009196281433105, 'eval_runtime': 1865.8666, 'eval_samples_per_second': 25.51, 'eval_steps_per_second': 25.51, 'epoch': 1.0}
 Took31.101356315612794 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.3652, 'grad_norm': 2.5532288551330566, 'learning_rate': 1.973114304365143e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.3846, 'grad_norm': 2.7551584243774414, 'learning_rate': 1.8663641350141968e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.3791, 'grad_norm': 2.9414072036743164, 'learning_rate': 1.6870082350552917e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.3812, 'grad_norm': 2.494396924972534, 'learning_rate': 1.4500775431308162e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.3457, 'grad_norm': 2.4841599464416504, 'learning_rate': 1.1754280589242567e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.3082, 'grad_norm': 2.4345521926879883, 'learning_rate': 8.860768091752868e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.3048, 'grad_norm': 2.7179455757141113, 'learning_rate': 6.062729035149349e-06, 'epoch': 0.64, 'step': 3500}, {'loss': 2.2967, 'grad_norm': 2.6819469928741455, 'learning_rate': 3.594653354741913e-06, 'epoch': 0.73, 'step': 4000}, {'eval_loss': 2.2165277004241943, 'eval_runtime': 1865.9277, 'eval_samples_per_second': 25.509, 'eval_steps_per_second': 25.509, 'epoch': 0.73, 'step': 4000}, {'loss': 2.2844, 'grad_norm': 2.508068084716797, 'learning_rate': 1.6633783730021125e-06, 'epoch': 0.82, 'step': 4500}, {'loss': 2.2721, 'grad_norm': 2.707127094268799, 'learning_rate': 4.307547776338994e-07, 'epoch': 0.91, 'step': 5000}, {'loss': 2.2813, 'grad_norm': 2.191084146499634, 'learning_rate': 8.270894704676124e-11, 'epoch': 1.0, 'step': 5500}, {'train_runtime': 29546.3368, 'train_samples_per_second': 5.964, 'train_steps_per_second': 0.186, 'total_flos': 3.20170952693932e+17, 'train_loss': 2.327674018897269, 'epoch': 1.0, 'step': 5507}, {'eval_loss': 2.2009196281433105, 'eval_runtime': 1865.8666, 'eval_samples_per_second': 25.51, 'eval_steps_per_second': 25.51, 'epoch': 1.0, 'step': 5507}]
Model STATE saved successfully 
 Took1.1030832926432291e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.071294331550598 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-8 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-9
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_9_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_9_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 177393
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47865
}) 
 Took0.8821193218231201 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5543, training_loss=2.3016899806722373, metrics={'train_runtime': 31152.6229, 'train_samples_per_second': 5.694, 'train_steps_per_second': 0.178, 'train_loss': 2.3016899806722373, 'epoch': 1.0})
 Took519.2485886136691 minutes
GPU memory usage before cleaning cache: 35696.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.1726136207580566, 'eval_runtime': 1979.3943, 'eval_samples_per_second': 24.182, 'eval_steps_per_second': 24.182, 'epoch': 1.0}
 Took32.99405865271886 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.3367, 'grad_norm': 2.8316187858581543, 'learning_rate': 1.9734671962705725e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.3589, 'grad_norm': 2.6758902072906494, 'learning_rate': 1.8680862185152324e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.3549, 'grad_norm': 2.696659803390503, 'learning_rate': 1.6909076794243203e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.3541, 'grad_norm': 2.529670476913452, 'learning_rate': 1.4565856384177458e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.324, 'grad_norm': 2.5480873584747314, 'learning_rate': 1.184500372192242e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.3146, 'grad_norm': 2.667915105819702, 'learning_rate': 8.971554733330519e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.2798, 'grad_norm': 2.310657501220703, 'learning_rate': 6.183166261239242e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.2514, 'grad_norm': 2.712125778198242, 'learning_rate': 3.7104599744210768e-06, 'epoch': 0.72, 'step': 4000}, {'eval_loss': 2.1885485649108887, 'eval_runtime': 1981.2339, 'eval_samples_per_second': 24.159, 'eval_steps_per_second': 24.159, 'epoch': 0.72, 'step': 4000}, {'loss': 2.2708, 'grad_norm': 2.796022653579712, 'learning_rate': 1.757948140789142e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.2418, 'grad_norm': 2.8949742317199707, 'learning_rate': 4.871188535116455e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.2402, 'grad_norm': 2.6612937450408936, 'learning_rate': 3.07969493957172e-09, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 31152.6229, 'train_samples_per_second': 5.694, 'train_steps_per_second': 0.178, 'total_flos': 3.423692165273272e+17, 'train_loss': 2.3016899806722373, 'epoch': 1.0, 'step': 5543}, {'eval_loss': 2.1726136207580566, 'eval_runtime': 1979.3943, 'eval_samples_per_second': 24.182, 'eval_steps_per_second': 24.182, 'epoch': 1.0, 'step': 5543}]
Model STATE saved successfully 
 Took1.1289119720458985e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.861203932762146 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-9 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-10
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_10_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_10_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180974
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 48309
}) 
 Took0.8521963119506836 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5655, training_loss=2.299876850374392, metrics={'train_runtime': 35806.509, 'train_samples_per_second': 5.054, 'train_steps_per_second': 0.158, 'train_loss': 2.299876850374392, 'epoch': 1.0})
 Took596.8176219344139 minutes
GPU memory usage before cleaning cache: 35756.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2164.82, 'eval_samples_per_second': 22.315, 'eval_steps_per_second': 22.315, 'epoch': 1.0}
 Took36.08520522514979 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.326, 'grad_norm': 2.379244089126587, 'learning_rate': 1.974521799144785e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.3634, 'grad_norm': 2.4612841606140137, 'learning_rate': 1.8732376531877725e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.3553, 'grad_norm': 2.6148440837860107, 'learning_rate': 1.7025936310103882e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.3359, 'grad_norm': 2.7557895183563232, 'learning_rate': 1.4761437155550342e-05, 'epoch': 0.35, 'step': 2000}, {'loss': 2.3283, 'grad_norm': 2.5546841621398926, 'learning_rate': 1.211874463000042e-05, 'epoch': 0.44, 'step': 2500}, {'loss': 2.2989, 'grad_norm': 2.5871875286102295, 'learning_rate': 9.307763592448131e-06, 'epoch': 0.53, 'step': 3000}, {'loss': 2.2863, 'grad_norm': 2.8898110389709473, 'learning_rate': 6.551765790407689e-06, 'epoch': 0.62, 'step': 3500}, {'loss': 2.2778, 'grad_norm': 2.481220245361328, 'learning_rate': 4.0696557405537005e-06, 'epoch': 0.71, 'step': 4000}, {'eval_loss': nan, 'eval_runtime': 2167.4732, 'eval_samples_per_second': 22.288, 'eval_steps_per_second': 22.288, 'epoch': 0.71, 'step': 4000}, {'loss': 2.2508, 'grad_norm': 2.7009522914886475, 'learning_rate': 2.058583491552465e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 2.242, 'grad_norm': 2.7467055320739746, 'learning_rate': 6.782853096446196e-07, 'epoch': 0.88, 'step': 5000}, {'loss': 2.24, 'grad_norm': 2.400463104248047, 'learning_rate': 3.839608138692219e-08, 'epoch': 0.97, 'step': 5500}, {'train_runtime': 35806.509, 'train_samples_per_second': 5.054, 'train_steps_per_second': 0.158, 'total_flos': 4.093625018527826e+17, 'train_loss': 2.299876850374392, 'epoch': 1.0, 'step': 5655}, {'eval_loss': nan, 'eval_runtime': 2164.82, 'eval_samples_per_second': 22.315, 'eval_steps_per_second': 22.315, 'epoch': 1.0, 'step': 5655}]
Model STATE saved successfully 
 Took1.2882550557454427e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.05472621122996 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-10 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-11
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_11_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_11_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180163
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47729
}) 
 Took0.9497260729471843 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5630, training_loss=2.2680604057244254, metrics={'train_runtime': 34630.3799, 'train_samples_per_second': 5.202, 'train_steps_per_second': 0.163, 'train_loss': 2.2680604057244254, 'epoch': 1.0})
 Took577.2212373455366 minutes
GPU memory usage before cleaning cache: 35676.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.134350538253784, 'eval_runtime': 1912.0582, 'eval_samples_per_second': 24.962, 'eval_steps_per_second': 24.962, 'epoch': 1.0}
 Took31.871062803268433 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.3053, 'grad_norm': 2.3955819606781006, 'learning_rate': 1.974291909318295e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.3307, 'grad_norm': 2.4031593799591064, 'learning_rate': 1.8721140605907694e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.3141, 'grad_norm': 2.645470142364502, 'learning_rate': 1.7000420745694256e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.2944, 'grad_norm': 2.446094036102295, 'learning_rate': 1.4718663756481192e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.2882, 'grad_norm': 2.422811508178711, 'learning_rate': 1.2058737273916023e-05, 'epoch': 0.44, 'step': 2500}, {'loss': 2.2696, 'grad_norm': 2.481868028640747, 'learning_rate': 9.23381670149807e-06, 'epoch': 0.53, 'step': 3000}, {'loss': 2.2551, 'grad_norm': 2.365225315093994, 'learning_rate': 6.470300620852041e-06, 'epoch': 0.62, 'step': 3500}, {'loss': 2.2388, 'grad_norm': 2.767874240875244, 'learning_rate': 3.989666452286358e-06, 'epoch': 0.71, 'step': 4000}, {'eval_loss': 2.150871753692627, 'eval_runtime': 1913.2826, 'eval_samples_per_second': 24.946, 'eval_steps_per_second': 24.946, 'epoch': 0.71, 'step': 4000}, {'loss': 2.2322, 'grad_norm': 2.3349883556365967, 'learning_rate': 1.9907205136764863e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 2.22, 'grad_norm': 2.481208324432373, 'learning_rate': 6.336650173127224e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 2.2164, 'grad_norm': 2.5628645420074463, 'learning_rate': 2.7258928867432978e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 34630.3799, 'train_samples_per_second': 5.202, 'train_steps_per_second': 0.163, 'total_flos': 3.95353682049024e+17, 'train_loss': 2.2680604057244254, 'epoch': 1.0, 'step': 5630}, {'eval_loss': 2.134350538253784, 'eval_runtime': 1912.0582, 'eval_samples_per_second': 24.962, 'eval_steps_per_second': 24.962, 'epoch': 1.0, 'step': 5630}]
Model STATE saved successfully 
 Took1.0704994201660156e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8233345945676168 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-11 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-12
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_12_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_12_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178758
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47461
}) 
 Took0.8783260265986125 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5586, training_loss=2.2380608443585595, metrics={'train_runtime': 32709.3254, 'train_samples_per_second': 5.465, 'train_steps_per_second': 0.171, 'train_loss': 2.2380608443585595, 'epoch': 1.0})
 Took545.2079543153445 minutes
GPU memory usage before cleaning cache: 35596.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.0987389087677, 'eval_runtime': 1794.6782, 'eval_samples_per_second': 26.445, 'eval_steps_per_second': 26.445, 'epoch': 1.0}
 Took29.914823587735494 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.2729, 'grad_norm': 2.454479932785034, 'learning_rate': 1.973879688305121e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.2937, 'grad_norm': 2.5456366539001465, 'learning_rate': 1.8701002217923723e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.2847, 'grad_norm': 2.4223368167877197, 'learning_rate': 1.69547263963467e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.2761, 'grad_norm': 2.514615535736084, 'learning_rate': 1.4642160338695046e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.2469, 'grad_norm': 2.433300018310547, 'learning_rate': 1.1951605281292555e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.2517, 'grad_norm': 2.513852834701538, 'learning_rate': 9.10214030386097e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.2244, 'grad_norm': 2.3436429500579834, 'learning_rate': 6.325783763816022e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.2049, 'grad_norm': 2.729001998901367, 'learning_rate': 3.8486011468273145e-06, 'epoch': 0.72, 'step': 4000}, {'eval_loss': 2.1140148639678955, 'eval_runtime': 1792.2868, 'eval_samples_per_second': 26.481, 'eval_steps_per_second': 26.481, 'epoch': 0.72, 'step': 4000}, {'loss': 2.1881, 'grad_norm': 2.495842695236206, 'learning_rate': 1.8722976313633168e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.1939, 'grad_norm': 2.193859577178955, 'learning_rate': 5.577941969919854e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.1872, 'grad_norm': 2.5182483196258545, 'learning_rate': 1.212459568982638e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 32709.3254, 'train_samples_per_second': 5.465, 'train_steps_per_second': 0.171, 'total_flos': 3.685617920275415e+17, 'train_loss': 2.2380608443585595, 'epoch': 1.0, 'step': 5586}, {'eval_loss': 2.0987389087677, 'eval_runtime': 1794.6782, 'eval_samples_per_second': 26.445, 'eval_steps_per_second': 26.445, 'epoch': 1.0, 'step': 5586}]
Model STATE saved successfully 
 Took1.0402997334798177e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.0570849537849427 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-12 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 2-8
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_2_8_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_2_8_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178439
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47388
}) 
 Took0.8951577266057332 minutes
Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

test:False,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:2,  max_grad_norm:0.9,  start_month_index:1,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+

+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |  6000  |
|        SAVE STEPS       |  2000  |
|      LOGGING STEPS      |  500   |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
MODEL PATH: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
Tokenizer loaded__________________________________________________________
____________________________________________________________________________________________________
Loading the model

+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED took __________ 0.03977888027826945 minutes_________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|         Training Argument          |                                                    Value                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|             output_dir             |                              /home/iitgn_cse/latex_model/model_main_fp32_2024-                              |
|                                    |                                 04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-                                |
|                                    |                              05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30                             |
|                                    |                              000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4                             |
|                                    |                                                     096                                                     |
|        overwrite_output_dir        |                                                     True                                                    |
|              do_train              |                                                     True                                                    |
|              do_eval               |                                                     True                                                    |
|             do_predict             |                                                    False                                                    |
|        evaluation_strategy         |                                                    steps                                                    |
|        prediction_loss_only        |                                                    False                                                    |
|    per_device_train_batch_size     |                                                      1                                                      |
|     per_device_eval_batch_size     |                                                      1                                                      |
|      per_gpu_train_batch_size      |                                                     None                                                    |
|      per_gpu_eval_batch_size       |                                                     None                                                    |
|    gradient_accumulation_steps     |                                                      32                                                     |
|      eval_accumulation_steps       |                                                      32                                                     |
|             eval_delay             |                                                      0                                                      |
|           learning_rate            |                                                    2e-05                                                    |
|            weight_decay            |                                                     0.1                                                     |
|             adam_beta1             |                                                     0.9                                                     |
|             adam_beta2             |                                                    0.999                                                    |
|            adam_epsilon            |                                                    1e-08                                                    |
|           max_grad_norm            |                                                     0.9                                                     |
|          num_train_epochs          |                                                      1                                                      |
|             max_steps              |                                                      -1                                                     |
|         lr_scheduler_type          |                                                    cosine                                                   |
|        lr_scheduler_kwargs         |                                                      {}                                                     |
|            warmup_ratio            |                                                     0.0                                                     |
|            warmup_steps            |                                                     100                                                     |
|             log_level              |                                                    debug                                                    |
|         log_level_replica          |                                                   warning                                                   |
|          log_on_each_node          |                                                     True                                                    |
|            logging_dir             |                                                    ./logs                                                   |
|          logging_strategy          |                                                    steps                                                    |
|         logging_first_step         |                                                    False                                                    |
|           logging_steps            |                                                     500                                                     |
|       logging_nan_inf_filter       |                                                     True                                                    |
|           save_strategy            |                                                    steps                                                    |
|             save_steps             |                                                     2000                                                    |
|          save_total_limit          |                                                      3                                                      |
|          save_safetensors          |                                                     True                                                    |
|         save_on_each_node          |                                                    False                                                    |
|          save_only_model           |                                                    False                                                    |
|              no_cuda               |                                                    False                                                    |
|              use_cpu               |                                                    False                                                    |
|           use_mps_device           |                                                    False                                                    |
|                seed                |                                                      42                                                     |
|             data_seed              |                                                     None                                                    |
|           jit_mode_eval            |                                                    False                                                    |
|              use_ipex              |                                                    False                                                    |
|                bf16                |                                                    False                                                    |
|                fp16                |                                                    False                                                    |
|           fp16_opt_level           |                                                      O1                                                     |
|       half_precision_backend       |                                                     auto                                                    |
|           bf16_full_eval           |                                                    False                                                    |
|           fp16_full_eval           |                                                    False                                                    |
|                tf32                |                                                     None                                                    |
|             local_rank             |                                                      0                                                      |
|            ddp_backend             |                                                     None                                                    |
|           tpu_num_cores            |                                                     None                                                    |
|         tpu_metrics_debug          |                                                    False                                                    |
|               debug                |                                                      []                                                     |
|        dataloader_drop_last        |                                                    False                                                    |
|             eval_steps             |                                                     6000                                                    |
|       dataloader_num_workers       |                                                      0                                                      |
|     dataloader_prefetch_factor     |                                                     None                                                    |
|             past_index             |                                                      -1                                                     |
|              run_name              |                              run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-                              |
|                                    |                                               07-17_test=False                                              |
|            disable_tqdm            |                                                    False                                                    |
|       remove_unused_columns        |                                                     True                                                    |
|            label_names             |                                                     None                                                    |
|       load_best_model_at_end       |                                                    False                                                    |
|       metric_for_best_model        |                                                     None                                                    |
|         greater_is_better          |                                                     None                                                    |
|          ignore_data_skip          |                                                    False                                                    |
|                fsdp                |                                                      []                                                     |
|        fsdp_min_num_params         |                                                      0                                                      |
|            fsdp_config             |            {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}           |
| fsdp_transformer_layer_cls_to_wrap |                                                     None                                                    |
|         accelerator_config         | AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True) |
|             deepspeed              |                                                     None                                                    |
|       label_smoothing_factor       |                                                     0.0                                                     |
|               optim                |                                                 adamw_torch                                                 |
|             optim_args             |                                                     None                                                    |
|             adafactor              |                                                    False                                                    |
|          group_by_length           |                                                    False                                                    |
|         length_column_name         |                                                    length                                                   |
|             report_to              |                                                  ['wandb']                                                  |
|     ddp_find_unused_parameters     |                                                     None                                                    |
|         ddp_bucket_cap_mb          |                                                     None                                                    |
|       ddp_broadcast_buffers        |                                                     None                                                    |
|       dataloader_pin_memory        |                                                     True                                                    |
|   dataloader_persistent_workers    |                                                    False                                                    |
|        skip_memory_metrics         |                                                     True                                                    |
|     use_legacy_prediction_loop     |                                                    False                                                    |
|            push_to_hub             |                                                    False                                                    |
|       resume_from_checkpoint       |                                                     None                                                    |
|            hub_model_id            |                                                     None                                                    |
|            hub_strategy            |                                                  every_save                                                 |
|             hub_token              |                                                     None                                                    |
|          hub_private_repo          |                                                    False                                                    |
|          hub_always_push           |                                                    False                                                    |
|       gradient_checkpointing       |                                                    False                                                    |
|   gradient_checkpointing_kwargs    |                                                     None                                                    |
|     include_inputs_for_metrics     |                                                    False                                                    |
|            fp16_backend            |                                                     auto                                                    |
|        push_to_hub_model_id        |                                                     None                                                    |
|      push_to_hub_organization      |                                                     None                                                    |
|         push_to_hub_token          |                                                     None                                                    |
|           mp_parameters            |                                                                                                             |
|        auto_find_batch_size        |                                                    False                                                    |
|          full_determinism          |                                                    False                                                    |
|            torchdynamo             |                                                     None                                                    |
|             ray_scope              |                                                     last                                                    |
|            ddp_timeout             |                                                     1800                                                    |
|           torch_compile            |                                                    False                                                    |
|       torch_compile_backend        |                                                     None                                                    |
|         torch_compile_mode         |                                                     None                                                    |
|          dispatch_batches          |                                                     None                                                    |
|           split_batches            |                                                     None                                                    |
|     include_tokens_per_second      |                                                    False                                                    |
|   include_num_input_tokens_seen    |                                                    False                                                    |
|        neftune_noise_alpha         |                                                     None                                                    |
|         distributed_state          |                                 Distributed environment: DistributedType.NO                                 |
|                                    |                                               Num processes: 1                                              |
|                                    |                                               Process index: 0                                              |
|                                    |                                            Local process index: 0                                           |
|                                    |                                                 Device: cuda                                                |
|                                    |                                                                                                             |
|               _n_gpu               |                                                      1                                                      |
|      __cached__setup_devices       |                                                    cuda:0                                                   |
|          deepspeed_plugin          |                                                     None                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+

______________________________________________________________________________________________________________________________________________________
Training for 1-1
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_1_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_1_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 177455
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47360
}) 
 Took0.6084494829177857 minutes
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-
Error occured while training the model
Artifact name may only contain alphanumeric characters, dashes, underscores, and dots. Invalid name: checkpoint-run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-07-17_test=False 
 Took177.17602943579357 minutes
Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

test:False,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:2,  max_grad_norm:0.9,  start_month_index:1,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+

+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |  6000  |
|        SAVE STEPS       |  2000  |
|      LOGGING STEPS      |  500   |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
MODEL PATH: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
Tokenizer loaded__________________________________________________________
____________________________________________________________________________________________________
Loading the model

+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED took __________ 0.0465574582417806 minutes_________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|         Training Argument          |                                                    Value                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|             output_dir             |                              /home/iitgn_cse/latex_model/model_main_fp32_2024-                              |
|                                    |                                 04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-                                |
|                                    |                              05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30                             |
|                                    |                              000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4                             |
|                                    |                                                     096                                                     |
|        overwrite_output_dir        |                                                     True                                                    |
|              do_train              |                                                     True                                                    |
|              do_eval               |                                                     True                                                    |
|             do_predict             |                                                    False                                                    |
|        evaluation_strategy         |                                                    steps                                                    |
|        prediction_loss_only        |                                                    False                                                    |
|    per_device_train_batch_size     |                                                      1                                                      |
|     per_device_eval_batch_size     |                                                      1                                                      |
|      per_gpu_train_batch_size      |                                                     None                                                    |
|      per_gpu_eval_batch_size       |                                                     None                                                    |
|    gradient_accumulation_steps     |                                                      32                                                     |
|      eval_accumulation_steps       |                                                      32                                                     |
|             eval_delay             |                                                      0                                                      |
|           learning_rate            |                                                    2e-05                                                    |
|            weight_decay            |                                                     0.1                                                     |
|             adam_beta1             |                                                     0.9                                                     |
|             adam_beta2             |                                                    0.999                                                    |
|            adam_epsilon            |                                                    1e-08                                                    |
|           max_grad_norm            |                                                     0.9                                                     |
|          num_train_epochs          |                                                      1                                                      |
|             max_steps              |                                                      -1                                                     |
|         lr_scheduler_type          |                                                    cosine                                                   |
|        lr_scheduler_kwargs         |                                                      {}                                                     |
|            warmup_ratio            |                                                     0.0                                                     |
|            warmup_steps            |                                                     100                                                     |
|             log_level              |                                                    debug                                                    |
|         log_level_replica          |                                                   warning                                                   |
|          log_on_each_node          |                                                     True                                                    |
|            logging_dir             |                                                    ./logs                                                   |
|          logging_strategy          |                                                    steps                                                    |
|         logging_first_step         |                                                    False                                                    |
|           logging_steps            |                                                     500                                                     |
|       logging_nan_inf_filter       |                                                     True                                                    |
|           save_strategy            |                                                    steps                                                    |
|             save_steps             |                                                     2000                                                    |
|          save_total_limit          |                                                      3                                                      |
|          save_safetensors          |                                                     True                                                    |
|         save_on_each_node          |                                                    False                                                    |
|          save_only_model           |                                                    False                                                    |
|              no_cuda               |                                                    False                                                    |
|              use_cpu               |                                                    False                                                    |
|           use_mps_device           |                                                    False                                                    |
|                seed                |                                                      42                                                     |
|             data_seed              |                                                     None                                                    |
|           jit_mode_eval            |                                                    False                                                    |
|              use_ipex              |                                                    False                                                    |
|                bf16                |                                                    False                                                    |
|                fp16                |                                                    False                                                    |
|           fp16_opt_level           |                                                      O1                                                     |
|       half_precision_backend       |                                                     auto                                                    |
|           bf16_full_eval           |                                                    False                                                    |
|           fp16_full_eval           |                                                    False                                                    |
|                tf32                |                                                     None                                                    |
|             local_rank             |                                                      0                                                      |
|            ddp_backend             |                                                     None                                                    |
|           tpu_num_cores            |                                                     None                                                    |
|         tpu_metrics_debug          |                                                    False                                                    |
|               debug                |                                                      []                                                     |
|        dataloader_drop_last        |                                                    False                                                    |
|             eval_steps             |                                                     6000                                                    |
|       dataloader_num_workers       |                                                      0                                                      |
|     dataloader_prefetch_factor     |                                                     None                                                    |
|             past_index             |                                                      -1                                                     |
|              run_name              |                              run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-                              |
|                                    |                                               07-17_test_False                                              |
|            disable_tqdm            |                                                    False                                                    |
|       remove_unused_columns        |                                                     True                                                    |
|            label_names             |                                                     None                                                    |
|       load_best_model_at_end       |                                                    False                                                    |
|       metric_for_best_model        |                                                     None                                                    |
|         greater_is_better          |                                                     None                                                    |
|          ignore_data_skip          |                                                    False                                                    |
|                fsdp                |                                                      []                                                     |
|        fsdp_min_num_params         |                                                      0                                                      |
|            fsdp_config             |            {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}           |
| fsdp_transformer_layer_cls_to_wrap |                                                     None                                                    |
|         accelerator_config         | AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True) |
|             deepspeed              |                                                     None                                                    |
|       label_smoothing_factor       |                                                     0.0                                                     |
|               optim                |                                                 adamw_torch                                                 |
|             optim_args             |                                                     None                                                    |
|             adafactor              |                                                    False                                                    |
|          group_by_length           |                                                    False                                                    |
|         length_column_name         |                                                    length                                                   |
|             report_to              |                                                  ['wandb']                                                  |
|     ddp_find_unused_parameters     |                                                     None                                                    |
|         ddp_bucket_cap_mb          |                                                     None                                                    |
|       ddp_broadcast_buffers        |                                                     None                                                    |
|       dataloader_pin_memory        |                                                     True                                                    |
|   dataloader_persistent_workers    |                                                    False                                                    |
|        skip_memory_metrics         |                                                     True                                                    |
|     use_legacy_prediction_loop     |                                                    False                                                    |
|            push_to_hub             |                                                    False                                                    |
|       resume_from_checkpoint       |                                                     None                                                    |
|            hub_model_id            |                                                     None                                                    |
|            hub_strategy            |                                                  every_save                                                 |
|             hub_token              |                                                     None                                                    |
|          hub_private_repo          |                                                    False                                                    |
|          hub_always_push           |                                                    False                                                    |
|       gradient_checkpointing       |                                                    False                                                    |
|   gradient_checkpointing_kwargs    |                                                     None                                                    |
|     include_inputs_for_metrics     |                                                    False                                                    |
|            fp16_backend            |                                                     auto                                                    |
|        push_to_hub_model_id        |                                                     None                                                    |
|      push_to_hub_organization      |                                                     None                                                    |
|         push_to_hub_token          |                                                     None                                                    |
|           mp_parameters            |                                                                                                             |
|        auto_find_batch_size        |                                                    False                                                    |
|          full_determinism          |                                                    False                                                    |
|            torchdynamo             |                                                     None                                                    |
|             ray_scope              |                                                     last                                                    |
|            ddp_timeout             |                                                     1800                                                    |
|           torch_compile            |                                                    False                                                    |
|       torch_compile_backend        |                                                     None                                                    |
|         torch_compile_mode         |                                                     None                                                    |
|          dispatch_batches          |                                                     None                                                    |
|           split_batches            |                                                     None                                                    |
|     include_tokens_per_second      |                                                    False                                                    |
|   include_num_input_tokens_seen    |                                                    False                                                    |
|        neftune_noise_alpha         |                                                     None                                                    |
|         distributed_state          |                                 Distributed environment: DistributedType.NO                                 |
|                                    |                                               Num processes: 1                                              |
|                                    |                                               Process index: 0                                              |
|                                    |                                            Local process index: 0                                           |
|                                    |                                                 Device: cuda                                                |
|                                    |                                                                                                             |
|               _n_gpu               |                                                      1                                                      |
|      __cached__setup_devices       |                                                    cuda:0                                                   |
|          deepspeed_plugin          |                                                     None                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+

______________________________________________________________________________________________________________________________________________________
Training for 1-1
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_1_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_1_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 177455
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47360
}) 
 Took0.5635519067446391 minutes
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-
Error occured while training the model
Artifact name may only contain alphanumeric characters, dashes, underscores, and dots. Invalid name: checkpoint-run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-07-17_test=False 
 Took177.1708010673523 minutes
Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

test:False,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:2,  max_grad_norm:0.9,  start_month_index:1,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+

+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |  6000  |
|        SAVE STEPS       |  2000  |
|      LOGGING STEPS      |  500   |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
MODEL PATH: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
Tokenizer loaded__________________________________________________________
____________________________________________________________________________________________________
Loading the model

+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED took __________ 0.044564000765482586 minutes_________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|         Training Argument          |                                                    Value                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|             output_dir             |                              /home/iitgn_cse/latex_model/model_main_fp32_2024-                              |
|                                    |                                 04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-                                |
|                                    |                              05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30                             |
|                                    |                              000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4                             |
|                                    |                                                     096                                                     |
|        overwrite_output_dir        |                                                     True                                                    |
|              do_train              |                                                     True                                                    |
|              do_eval               |                                                     True                                                    |
|             do_predict             |                                                    False                                                    |
|        evaluation_strategy         |                                                    steps                                                    |
|        prediction_loss_only        |                                                    False                                                    |
|    per_device_train_batch_size     |                                                      1                                                      |
|     per_device_eval_batch_size     |                                                      1                                                      |
|      per_gpu_train_batch_size      |                                                     None                                                    |
|      per_gpu_eval_batch_size       |                                                     None                                                    |
|    gradient_accumulation_steps     |                                                      32                                                     |
|      eval_accumulation_steps       |                                                      32                                                     |
|             eval_delay             |                                                      0                                                      |
|           learning_rate            |                                                    2e-05                                                    |
|            weight_decay            |                                                     0.1                                                     |
|             adam_beta1             |                                                     0.9                                                     |
|             adam_beta2             |                                                    0.999                                                    |
|            adam_epsilon            |                                                    1e-08                                                    |
|           max_grad_norm            |                                                     0.9                                                     |
|          num_train_epochs          |                                                      1                                                      |
|             max_steps              |                                                      -1                                                     |
|         lr_scheduler_type          |                                                    cosine                                                   |
|        lr_scheduler_kwargs         |                                                      {}                                                     |
|            warmup_ratio            |                                                     0.0                                                     |
|            warmup_steps            |                                                     100                                                     |
|             log_level              |                                                    debug                                                    |
|         log_level_replica          |                                                   warning                                                   |
|          log_on_each_node          |                                                     True                                                    |
|            logging_dir             |                                                    ./logs                                                   |
|          logging_strategy          |                                                    steps                                                    |
|         logging_first_step         |                                                    False                                                    |
|           logging_steps            |                                                     500                                                     |
|       logging_nan_inf_filter       |                                                     True                                                    |
|           save_strategy            |                                                    steps                                                    |
|             save_steps             |                                                     2000                                                    |
|          save_total_limit          |                                                      3                                                      |
|          save_safetensors          |                                                     True                                                    |
|         save_on_each_node          |                                                    False                                                    |
|          save_only_model           |                                                    False                                                    |
|              no_cuda               |                                                    False                                                    |
|              use_cpu               |                                                    False                                                    |
|           use_mps_device           |                                                    False                                                    |
|                seed                |                                                      42                                                     |
|             data_seed              |                                                     None                                                    |
|           jit_mode_eval            |                                                    False                                                    |
|              use_ipex              |                                                    False                                                    |
|                bf16                |                                                    False                                                    |
|                fp16                |                                                    False                                                    |
|           fp16_opt_level           |                                                      O1                                                     |
|       half_precision_backend       |                                                     auto                                                    |
|           bf16_full_eval           |                                                    False                                                    |
|           fp16_full_eval           |                                                    False                                                    |
|                tf32                |                                                     None                                                    |
|             local_rank             |                                                      0                                                      |
|            ddp_backend             |                                                     None                                                    |
|           tpu_num_cores            |                                                     None                                                    |
|         tpu_metrics_debug          |                                                    False                                                    |
|               debug                |                                                      []                                                     |
|        dataloader_drop_last        |                                                    False                                                    |
|             eval_steps             |                                                     6000                                                    |
|       dataloader_num_workers       |                                                      0                                                      |
|     dataloader_prefetch_factor     |                                                     None                                                    |
|             past_index             |                                                      -1                                                     |
|              run_name              |                              run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-                              |
|                                    |                                               07-17_test_False                                              |
|            disable_tqdm            |                                                    False                                                    |
|       remove_unused_columns        |                                                     True                                                    |
|            label_names             |                                                     None                                                    |
|       load_best_model_at_end       |                                                    False                                                    |
|       metric_for_best_model        |                                                     None                                                    |
|         greater_is_better          |                                                     None                                                    |
|          ignore_data_skip          |                                                    False                                                    |
|                fsdp                |                                                      []                                                     |
|        fsdp_min_num_params         |                                                      0                                                      |
|            fsdp_config             |            {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}           |
| fsdp_transformer_layer_cls_to_wrap |                                                     None                                                    |
|         accelerator_config         | AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True) |
|             deepspeed              |                                                     None                                                    |
|       label_smoothing_factor       |                                                     0.0                                                     |
|               optim                |                                                 adamw_torch                                                 |
|             optim_args             |                                                     None                                                    |
|             adafactor              |                                                    False                                                    |
|          group_by_length           |                                                    False                                                    |
|         length_column_name         |                                                    length                                                   |
|             report_to              |                                                  ['wandb']                                                  |
|     ddp_find_unused_parameters     |                                                     None                                                    |
|         ddp_bucket_cap_mb          |                                                     None                                                    |
|       ddp_broadcast_buffers        |                                                     None                                                    |
|       dataloader_pin_memory        |                                                     True                                                    |
|   dataloader_persistent_workers    |                                                    False                                                    |
|        skip_memory_metrics         |                                                     True                                                    |
|     use_legacy_prediction_loop     |                                                    False                                                    |
|            push_to_hub             |                                                    False                                                    |
|       resume_from_checkpoint       |                                                     None                                                    |
|            hub_model_id            |                                                     None                                                    |
|            hub_strategy            |                                                  every_save                                                 |
|             hub_token              |                                                     None                                                    |
|          hub_private_repo          |                                                    False                                                    |
|          hub_always_push           |                                                    False                                                    |
|       gradient_checkpointing       |                                                    False                                                    |
|   gradient_checkpointing_kwargs    |                                                     None                                                    |
|     include_inputs_for_metrics     |                                                    False                                                    |
|            fp16_backend            |                                                     auto                                                    |
|        push_to_hub_model_id        |                                                     None                                                    |
|      push_to_hub_organization      |                                                     None                                                    |
|         push_to_hub_token          |                                                     None                                                    |
|           mp_parameters            |                                                                                                             |
|        auto_find_batch_size        |                                                    False                                                    |
|          full_determinism          |                                                    False                                                    |
|            torchdynamo             |                                                     None                                                    |
|             ray_scope              |                                                     last                                                    |
|            ddp_timeout             |                                                     1800                                                    |
|           torch_compile            |                                                    False                                                    |
|       torch_compile_backend        |                                                     None                                                    |
|         torch_compile_mode         |                                                     None                                                    |
|          dispatch_batches          |                                                     None                                                    |
|           split_batches            |                                                     None                                                    |
|     include_tokens_per_second      |                                                    False                                                    |
|   include_num_input_tokens_seen    |                                                    False                                                    |
|        neftune_noise_alpha         |                                                     None                                                    |
|         distributed_state          |                                 Distributed environment: DistributedType.NO                                 |
|                                    |                                               Num processes: 1                                              |
|                                    |                                               Process index: 0                                              |
|                                    |                                            Local process index: 0                                           |
|                                    |                                                 Device: cuda                                                |
|                                    |                                                                                                             |
|               _n_gpu               |                                                      1                                                      |
|      __cached__setup_devices       |                                                    cuda:0                                                   |
|          deepspeed_plugin          |                                                     None                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+

______________________________________________________________________________________________________________________________________________________
Training for 1-1
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_1_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_1_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 177455
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47360
}) 
 Took0.5463281949361165 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5545, training_loss=2.2122017780438106, metrics={'train_runtime': 515077.9003, 'train_samples_per_second': 0.345, 'train_steps_per_second': 0.011, 'train_loss': 2.2122017780438106, 'epoch': 1.0})
 Took8584.656435537338 minutes
GPU memory usage before cleaning cache: 35736.88 MB
GPU memory usage after cleaning cache: 21818.88 MB
GPU memory occupied from nvmlInit: 22439 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.085578203201294, 'eval_runtime': 1757.1381, 'eval_samples_per_second': 26.953, 'eval_steps_per_second': 26.953, 'epoch': 1.0}
 Took29.29632157087326 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.2461, 'grad_norm': 2.417651414871216, 'learning_rate': 1.9734865976149145e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.2723, 'grad_norm': 2.4601728916168213, 'learning_rate': 1.868180920098644e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.2561, 'grad_norm': 2.40944766998291, 'learning_rate': 1.691122222319118e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.236, 'grad_norm': 2.39021635055542, 'learning_rate': 1.45694397068345e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.2235, 'grad_norm': 2.42747163772583, 'learning_rate': 1.1850004224044315e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.2253, 'grad_norm': 2.277052164077759, 'learning_rate': 8.977670436877812e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.2102, 'grad_norm': 2.3966214656829834, 'learning_rate': 6.189829676637182e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.1791, 'grad_norm': 2.538438320159912, 'learning_rate': 3.7168901335157313e-06, 'epoch': 0.72, 'step': 4000}, {'loss': 2.1727, 'grad_norm': 2.2943174839019775, 'learning_rate': 1.7632341857016733e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.1568, 'grad_norm': 2.2097373008728027, 'learning_rate': 4.90326697908925e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.1618, 'grad_norm': 2.16998028755188, 'learning_rate': 3.3703469648760367e-09, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 515077.9003, 'train_samples_per_second': 0.345, 'train_steps_per_second': 0.011, 'total_flos': 3.428278704487219e+17, 'train_loss': 2.2122017780438106, 'epoch': 1.0, 'step': 5545}, {'eval_loss': 2.085578203201294, 'eval_runtime': 1757.1381, 'eval_samples_per_second': 26.953, 'eval_steps_per_second': 26.953, 'epoch': 1.0, 'step': 5545}]
Model STATE saved successfully 
 Took1.0267893473307292e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8472955822944641 minutes
GPU memory usage before cleaning cache: 22514.88 MB
GPU memory usage after cleaning cache: 21818.88 MB
GPU memory occupied from nvmlInit: 22439 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-1 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-2
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_2_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_2_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 177207
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47423
}) 
 Took0.7747462312380473 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5537, training_loss=2.185958104463439, metrics={'train_runtime': 28950.8839, 'train_samples_per_second': 6.121, 'train_steps_per_second': 0.191, 'train_loss': 2.185958104463439, 'epoch': 1.0})
 Took482.54335277080537 minutes
GPU memory usage before cleaning cache: 35656.88 MB
GPU memory usage after cleaning cache: 21818.88 MB
GPU memory occupied from nvmlInit: 22439 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.057264804840088, 'eval_runtime': 1789.7821, 'eval_samples_per_second': 26.497, 'eval_steps_per_second': 26.497, 'epoch': 1.0}
 Took29.832814943790435 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.217, 'grad_norm': 2.419914960861206, 'learning_rate': 1.9734088644855326e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.2272, 'grad_norm': 2.5259273052215576, 'learning_rate': 1.867801505637654e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.2302, 'grad_norm': 2.437401056289673, 'learning_rate': 1.6902627376535165e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.2202, 'grad_norm': 2.6773681640625, 'learning_rate': 1.4555086144630231e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.2186, 'grad_norm': 2.243520975112915, 'learning_rate': 1.1829977274282976e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.1906, 'grad_norm': 2.535682439804077, 'learning_rate': 8.953182974524321e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.183, 'grad_norm': 2.6032204627990723, 'learning_rate': 6.163158544673838e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.1497, 'grad_norm': 2.5713324546813965, 'learning_rate': 3.6911669952932137e-06, 'epoch': 0.72, 'step': 4000}, {'loss': 2.1364, 'grad_norm': 2.542867422103882, 'learning_rate': 1.7421098325041908e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.1459, 'grad_norm': 2.1535887718200684, 'learning_rate': 4.775429274313748e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.131, 'grad_norm': 2.3715004920959473, 'learning_rate': 2.2852724803190673e-09, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 28950.8839, 'train_samples_per_second': 6.121, 'train_steps_per_second': 0.191, 'total_flos': 3.39375167913173e+17, 'train_loss': 2.185958104463439, 'epoch': 1.0, 'step': 5537}, {'eval_loss': 2.057264804840088, 'eval_runtime': 1789.7821, 'eval_samples_per_second': 26.497, 'eval_steps_per_second': 26.497, 'epoch': 1.0, 'step': 5537}]
Model STATE saved successfully 
 Took1.0422865549723308e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.0585310419400533 minutes
GPU memory usage before cleaning cache: 22514.88 MB
GPU memory usage after cleaning cache: 21818.88 MB
GPU memory occupied from nvmlInit: 22439 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-2 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-3
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_3_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_3_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180393
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47763
}) 
 Took0.7807570695877075 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5637, training_loss=2.1739718559174457, metrics={'train_runtime': 33160.2428, 'train_samples_per_second': 5.44, 'train_steps_per_second': 0.17, 'train_loss': 2.1739718559174457, 'epoch': 1.0})
 Took552.7067375421524 minutes
GPU memory usage before cleaning cache: 35656.88 MB
GPU memory usage after cleaning cache: 21818.88 MB
GPU memory occupied from nvmlInit: 22439 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.0495598316192627, 'eval_runtime': 1931.3194, 'eval_samples_per_second': 24.731, 'eval_steps_per_second': 24.731, 'epoch': 1.0}
 Took32.19294463793437 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.2044, 'grad_norm': 2.444118022918701, 'learning_rate': 1.9743565906530045e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.2335, 'grad_norm': 2.389880895614624, 'learning_rate': 1.8724301558943566e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.2216, 'grad_norm': 2.3973264694213867, 'learning_rate': 1.7007597401755277e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.2156, 'grad_norm': 2.469482183456421, 'learning_rate': 1.4730690532106167e-05, 'epoch': 0.35, 'step': 2000}, {'loss': 2.1944, 'grad_norm': 2.422027111053467, 'learning_rate': 1.2075601881175683e-05, 'epoch': 0.44, 'step': 2500}, {'loss': 2.1776, 'grad_norm': 2.1091268062591553, 'learning_rate': 9.254585061550439e-06, 'epoch': 0.53, 'step': 3000}, {'loss': 2.1547, 'grad_norm': 2.5761985778808594, 'learning_rate': 6.493158347704341e-06, 'epoch': 0.62, 'step': 3500}, {'loss': 2.1457, 'grad_norm': 2.24475359916687, 'learning_rate': 4.012076250301893e-06, 'epoch': 0.71, 'step': 4000}, {'loss': 2.1312, 'grad_norm': 2.4672293663024902, 'learning_rate': 2.0096819162331637e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 2.1231, 'grad_norm': 2.2536768913269043, 'learning_rate': 6.460511422441984e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 2.1249, 'grad_norm': 2.432588577270508, 'learning_rate': 3.019556450466588e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 33160.2428, 'train_samples_per_second': 5.44, 'train_steps_per_second': 0.17, 'total_flos': 4.0234388335959245e+17, 'train_loss': 2.1739718559174457, 'epoch': 1.0, 'step': 5637}, {'eval_loss': 2.0495598316192627, 'eval_runtime': 1931.3194, 'eval_samples_per_second': 24.731, 'eval_steps_per_second': 24.731, 'epoch': 1.0, 'step': 5637}]
Model STATE saved successfully 
 Took9.874502817789713e-06 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8769266923268636 minutes
GPU memory usage before cleaning cache: 22514.88 MB
GPU memory usage after cleaning cache: 21818.88 MB
GPU memory occupied from nvmlInit: 22439 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-3 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-4
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_4_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_4_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 177867
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47377
}) 
 Took0.8046058893203736 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5558, training_loss=2.1543644757183302, metrics={'train_runtime': 29833.4864, 'train_samples_per_second': 5.962, 'train_steps_per_second': 0.186, 'train_loss': 2.1543644757183302, 'epoch': 1.0})
 Took497.251185242335 minutes
GPU memory usage before cleaning cache: 35796.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.024535655975342, 'eval_runtime': 1771.3145, 'eval_samples_per_second': 26.747, 'eval_steps_per_second': 26.747, 'epoch': 1.0}
 Took29.546795558929443 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.1805, 'grad_norm': 2.434303045272827, 'learning_rate': 1.9736121900013042e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.1933, 'grad_norm': 2.1543056964874268, 'learning_rate': 1.868794022046693e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.1956, 'grad_norm': 2.4244463443756104, 'learning_rate': 1.69251144134421e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.1951, 'grad_norm': 2.4080514907836914, 'learning_rate': 1.459264928776429e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.1722, 'grad_norm': 2.2860162258148193, 'learning_rate': 1.188240648869295e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.1561, 'grad_norm': 2.029737710952759, 'learning_rate': 9.017322529515198e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.1402, 'grad_norm': 2.3687548637390137, 'learning_rate': 6.2330706969021774e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.1337, 'grad_norm': 2.251997709274292, 'learning_rate': 3.7586752671617933e-06, 'epoch': 0.72, 'step': 4000}, {'loss': 2.1193, 'grad_norm': 2.538069248199463, 'learning_rate': 1.7976726503119602e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.1152, 'grad_norm': 2.493809938430786, 'learning_rate': 5.11369090242424e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.1021, 'grad_norm': 2.2570769786834717, 'learning_rate': 5.572092763721504e-09, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 29833.4864, 'train_samples_per_second': 5.962, 'train_steps_per_second': 0.186, 'total_flos': 3.5229971036747366e+17, 'train_loss': 2.1543644757183302, 'epoch': 1.0, 'step': 5558}, {'eval_loss': 2.024535655975342, 'eval_runtime': 1771.3145, 'eval_samples_per_second': 26.747, 'eval_steps_per_second': 26.747, 'epoch': 1.0, 'step': 5558}]
Model STATE saved successfully 
 Took9.552637736002605e-06 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.865959640343984 minutes
GPU memory usage before cleaning cache: 22514.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-4 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-5
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_5_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_5_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178783
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47617
}) 
 Took0.8554382960001627 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5586, training_loss=2.1520564253083534, metrics={'train_runtime': 30902.898, 'train_samples_per_second': 5.785, 'train_steps_per_second': 0.181, 'train_loss': 2.1520564253083534, 'epoch': 1.0})
 Took515.0811696410179 minutes
GPU memory usage before cleaning cache: 35696.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 1867.7725, 'eval_samples_per_second': 25.494, 'eval_steps_per_second': 25.494, 'epoch': 1.0}
 Took31.133308056990305 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.178, 'grad_norm': 2.4422101974487305, 'learning_rate': 1.973879688305121e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.2055, 'grad_norm': 2.240560531616211, 'learning_rate': 1.8701002217923723e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.1979, 'grad_norm': 2.42606520652771, 'learning_rate': 1.69547263963467e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.1715, 'grad_norm': 2.498940944671631, 'learning_rate': 1.4642160338695046e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.1645, 'grad_norm': 2.449733257293701, 'learning_rate': 1.1951605281292555e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.1532, 'grad_norm': 2.5615928173065186, 'learning_rate': 9.10214030386097e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.1527, 'grad_norm': 2.4729831218719482, 'learning_rate': 6.325783763816022e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.1272, 'grad_norm': 2.32254695892334, 'learning_rate': 3.8486011468273145e-06, 'epoch': 0.72, 'step': 4000}, {'loss': 2.1201, 'grad_norm': 2.4068374633789062, 'learning_rate': 1.8722976313633168e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.0958, 'grad_norm': 2.456693410873413, 'learning_rate': 5.577941969919854e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 2.1136, 'grad_norm': 2.2376551628112793, 'learning_rate': 1.212459568982638e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 30902.898, 'train_samples_per_second': 5.785, 'train_steps_per_second': 0.181, 'total_flos': 3.6820779964882944e+17, 'train_loss': 2.1520564253083534, 'epoch': 1.0, 'step': 5586}, {'eval_loss': nan, 'eval_runtime': 1867.7725, 'eval_samples_per_second': 25.494, 'eval_steps_per_second': 25.494, 'epoch': 1.0, 'step': 5586}]
Model STATE saved successfully 
 Took1.0275840759277344e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.9435764114061992 minutes
GPU memory usage before cleaning cache: 22514.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-5 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-6
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_6_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_6_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178406
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 48064
}) 
 Took0.87895827293396 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5575, training_loss=2.1324407137883616, metrics={'train_runtime': 30480.3162, 'train_samples_per_second': 5.853, 'train_steps_per_second': 0.183, 'train_loss': 2.1324407137883616, 'epoch': 1.0})
 Took508.04369748830794 minutes
GPU memory usage before cleaning cache: 35896.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 1.9953444004058838, 'eval_runtime': 2073.4686, 'eval_samples_per_second': 23.18, 'eval_steps_per_second': 23.18, 'epoch': 1.0}
 Took34.56153966188431 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.1421, 'grad_norm': 2.2177305221557617, 'learning_rate': 1.9737750861740434e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.1806, 'grad_norm': 2.1668355464935303, 'learning_rate': 1.869589389346611e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.1774, 'grad_norm': 2.2772257328033447, 'learning_rate': 1.6943143226185252e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.1672, 'grad_norm': 2.4317715167999268, 'learning_rate': 1.4622787108416585e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.1597, 'grad_norm': 2.3840279579162598, 'learning_rate': 1.19245158197083e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.1387, 'grad_norm': 2.4440674781799316, 'learning_rate': 9.068914394888651e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.1187, 'grad_norm': 2.3207898139953613, 'learning_rate': 6.289429686535226e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.1227, 'grad_norm': 2.5031301975250244, 'learning_rate': 3.8132859673749688e-06, 'epoch': 0.72, 'step': 4000}, {'loss': 2.0894, 'grad_norm': 2.105083703994751, 'learning_rate': 1.8429092317163244e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.0883, 'grad_norm': 2.286904811859131, 'learning_rate': 5.393787686133234e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.0824, 'grad_norm': 2.3351399898529053, 'learning_rate': 9.25884897770013e-09, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 30480.3162, 'train_samples_per_second': 5.853, 'train_steps_per_second': 0.183, 'total_flos': 3.61707807110996e+17, 'train_loss': 2.1324407137883616, 'epoch': 1.0, 'step': 5575}, {'eval_loss': 1.9953444004058838, 'eval_runtime': 2073.4686, 'eval_samples_per_second': 23.18, 'eval_steps_per_second': 23.18, 'epoch': 1.0, 'step': 5575}]
Model STATE saved successfully 
 Took9.838740030924479e-06 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8889161586761475 minutes
GPU memory usage before cleaning cache: 22514.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-6 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-7
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_7_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178538
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47534
}) 
 Took0.9257875204086303 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5579, training_loss=2.122692599777766, metrics={'train_runtime': 30573.9471, 'train_samples_per_second': 5.84, 'train_steps_per_second': 0.182, 'train_loss': 2.122692599777766, 'epoch': 1.0})
 Took509.6055520653725 minutes
GPU memory usage before cleaning cache: 35736.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 1.997084617614746, 'eval_runtime': 1838.6386, 'eval_samples_per_second': 25.853, 'eval_steps_per_second': 25.853, 'epoch': 1.0}
 Took30.647822086016337 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.1507, 'grad_norm': 2.4832353591918945, 'learning_rate': 1.9738131957863652e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.1581, 'grad_norm': 2.5263311862945557, 'learning_rate': 1.8697754918934484e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.17, 'grad_norm': 2.2813355922698975, 'learning_rate': 1.694736275643646e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.1523, 'grad_norm': 2.2914633750915527, 'learning_rate': 1.4629843482176444e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.1366, 'grad_norm': 2.3387115001678467, 'learning_rate': 1.193438082795917e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.1219, 'grad_norm': 2.236344337463379, 'learning_rate': 9.08101079932246e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.1213, 'grad_norm': 2.3783118724823, 'learning_rate': 6.3026596964843e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.0991, 'grad_norm': 2.4680845737457275, 'learning_rate': 3.8261298747634065e-06, 'epoch': 0.72, 'step': 4000}, {'loss': 2.0852, 'grad_norm': 2.168861150741577, 'learning_rate': 1.8535854148656295e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.083, 'grad_norm': 1.8845460414886475, 'learning_rate': 5.46049064786599e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.0781, 'grad_norm': 2.4862987995147705, 'learning_rate': 1.0257630957956067e-08, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 30573.9471, 'train_samples_per_second': 5.84, 'train_steps_per_second': 0.182, 'total_flos': 3.634087798847816e+17, 'train_loss': 2.122692599777766, 'epoch': 1.0, 'step': 5579}, {'eval_loss': 1.997084617614746, 'eval_runtime': 1838.6386, 'eval_samples_per_second': 25.853, 'eval_steps_per_second': 25.853, 'epoch': 1.0, 'step': 5579}]
Model STATE saved successfully 
 Took9.584426879882813e-06 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.095926821231842 minutes
GPU memory usage before cleaning cache: 22514.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-7 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 2-1
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_2_1_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_2_1_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 177572
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47351
}) 
 Took0.9035135229428609 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5549, training_loss=2.103799627114683, metrics={'train_runtime': 29349.0189, 'train_samples_per_second': 6.05, 'train_steps_per_second': 0.189, 'train_loss': 2.103799627114683, 'epoch': 1.0})
 Took489.1931251366933 minutes
GPU memory usage before cleaning cache: 35656.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 1.9691309928894043, 'eval_runtime': 1746.5254, 'eval_samples_per_second': 27.112, 'eval_steps_per_second': 27.112, 'epoch': 1.0}
 Took29.112557351589203 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.1228, 'grad_norm': 2.548765182495117, 'learning_rate': 1.973525336613693e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.1462, 'grad_norm': 2.164414644241333, 'learning_rate': 1.8683700200596154e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.1455, 'grad_norm': 2.263211488723755, 'learning_rate': 1.6915506533003116e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.1294, 'grad_norm': 2.4256930351257324, 'learning_rate': 1.457659624009212e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.126, 'grad_norm': 2.165717601776123, 'learning_rate': 1.1859992780509503e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.1128, 'grad_norm': 2.329037666320801, 'learning_rate': 8.989889530601742e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.0927, 'grad_norm': 2.3685803413391113, 'learning_rate': 6.2031477576215395e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.0833, 'grad_norm': 2.353145122528076, 'learning_rate': 3.729749148769639e-06, 'epoch': 0.72, 'step': 4000}, {'loss': 2.0782, 'grad_norm': 2.4225707054138184, 'learning_rate': 1.7738160919540593e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.0521, 'grad_norm': 2.3426434993743896, 'learning_rate': 4.967660492517035e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.0572, 'grad_norm': 2.2664384841918945, 'learning_rate': 3.990243598303023e-09, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 29349.0189, 'train_samples_per_second': 6.05, 'train_steps_per_second': 0.189, 'total_flos': 3.4505396227149005e+17, 'train_loss': 2.103799627114683, 'epoch': 1.0, 'step': 5549}, {'eval_loss': 1.9691309928894043, 'eval_runtime': 1746.5254, 'eval_samples_per_second': 27.112, 'eval_steps_per_second': 27.112, 'epoch': 1.0, 'step': 5549}]
Model STATE saved successfully 
 Took1.0351339975992838e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.0754933953285217 minutes
GPU memory usage before cleaning cache: 22514.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
______________________________________________________________________________________________________________________________________________________
Training for 2-1 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 2-2
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_2_2_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_2_2_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 177372
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47596
}) 
 Took0.8803889513015747 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5542, training_loss=2.0812561657402067, metrics={'train_runtime': 29166.0334, 'train_samples_per_second': 6.081, 'train_steps_per_second': 0.19, 'train_loss': 2.0812561657402067, 'epoch': 1.0})
 Took486.15094577471416 minutes
GPU memory usage before cleaning cache: 35736.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 1.9556951522827148, 'eval_runtime': 1865.5174, 'eval_samples_per_second': 25.514, 'eval_steps_per_second': 25.514, 'epoch': 1.0}
 Took31.0984263976415 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.1034, 'grad_norm': 2.547100782394409, 'learning_rate': 1.9734574876236087e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.1285, 'grad_norm': 2.370898485183716, 'learning_rate': 1.868038829760314e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.1087, 'grad_norm': 2.197932243347168, 'learning_rate': 1.6908003259995014e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.1206, 'grad_norm': 2.3773531913757324, 'learning_rate': 1.4564063457112867e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.093, 'grad_norm': 2.286982297897339, 'learning_rate': 1.1842501913223066e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.0765, 'grad_norm': 2.4958741664886475, 'learning_rate': 8.968495341603221e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.0753, 'grad_norm': 2.157916784286499, 'learning_rate': 6.179833460875931e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.0671, 'grad_norm': 1.9698100090026855, 'learning_rate': 3.7072447357913477e-06, 'epoch': 0.72, 'step': 4000}, {'loss': 2.049, 'grad_norm': 2.296445846557617, 'learning_rate': 1.7553063524328973e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.0397, 'grad_norm': 2.0310897827148438, 'learning_rate': 4.855178993995868e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.0368, 'grad_norm': 2.459698438644409, 'learning_rate': 2.9392056906352162e-09, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 29166.0334, 'train_samples_per_second': 6.081, 'train_steps_per_second': 0.19, 'total_flos': 3.420037919487099e+17, 'train_loss': 2.0812561657402067, 'epoch': 1.0, 'step': 5542}, {'eval_loss': 1.9556951522827148, 'eval_runtime': 1865.5174, 'eval_samples_per_second': 25.514, 'eval_steps_per_second': 25.514, 'epoch': 1.0, 'step': 5542}]
Model STATE saved successfully 
 Took1.3025601704915364e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8550023396809896 minutes
GPU memory usage before cleaning cache: 22514.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
______________________________________________________________________________________________________________________________________________________
Training for 2-2 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 2-3
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_2_3_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_2_3_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178444
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47664
}) 
 Took1.0105302850405375 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5576, training_loss=2.070882624840976, metrics={'train_runtime': 30586.0661, 'train_samples_per_second': 5.834, 'train_steps_per_second': 0.182, 'train_loss': 2.070882624840976, 'epoch': 1.0})
 Took509.8246695915858 minutes
GPU memory usage before cleaning cache: 35656.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 1.9461714029312134, 'eval_runtime': 1897.1973, 'eval_samples_per_second': 25.123, 'eval_steps_per_second': 25.123, 'epoch': 1.0}
 Took31.62368694941203 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.0864, 'grad_norm': 2.337153196334839, 'learning_rate': 1.9737846213609392e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.1183, 'grad_norm': 2.6580543518066406, 'learning_rate': 1.869635952065721e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.1121, 'grad_norm': 2.275486707687378, 'learning_rate': 1.694419891068761e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.0929, 'grad_norm': 2.067491054534912, 'learning_rate': 1.462455244313774e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.1042, 'grad_norm': 2.4244089126586914, 'learning_rate': 1.1926983605726049e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.0766, 'grad_norm': 2.183974504470825, 'learning_rate': 9.07194002456018e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.0685, 'grad_norm': 2.2821478843688965, 'learning_rate': 6.292738296115124e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.0472, 'grad_norm': 2.191265344619751, 'learning_rate': 3.816497155977188e-06, 'epoch': 0.72, 'step': 4000}, {'loss': 2.0322, 'grad_norm': 2.558680295944214, 'learning_rate': 1.845577142065873e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.0206, 'grad_norm': 2.2287001609802246, 'learning_rate': 5.410435188849228e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.0283, 'grad_norm': 2.307213068008423, 'learning_rate': 9.503886737061151e-09, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 30586.0661, 'train_samples_per_second': 5.834, 'train_steps_per_second': 0.182, 'total_flos': 3.635743438742323e+17, 'train_loss': 2.070882624840976, 'epoch': 1.0, 'step': 5576}, {'eval_loss': 1.9461714029312134, 'eval_runtime': 1897.1973, 'eval_samples_per_second': 25.123, 'eval_steps_per_second': 25.123, 'epoch': 1.0, 'step': 5576}]
Model STATE saved successfully 
 Took1.0768572489420572e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.047837245464325 minutes
GPU memory usage before cleaning cache: 22514.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
______________________________________________________________________________________________________________________________________________________
Training for 2-3 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 2-4
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_2_4_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_2_4_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178683
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47606
}) 
 Took0.97740771373113 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5583, training_loss=2.0634209251779607, metrics={'train_runtime': 30816.5312, 'train_samples_per_second': 5.798, 'train_steps_per_second': 0.181, 'train_loss': 2.0634209251779607, 'epoch': 1.0})
 Took513.6694999138514 minutes
GPU memory usage before cleaning cache: 35656.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 1.943244218826294, 'eval_runtime': 1871.9391, 'eval_samples_per_second': 25.431, 'eval_steps_per_second': 25.431, 'epoch': 1.0}
 Took31.20356531937917 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.0719, 'grad_norm': 2.191150426864624, 'learning_rate': 1.973851222511574e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.1118, 'grad_norm': 2.3402953147888184, 'learning_rate': 1.8699611995419587e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.1021, 'grad_norm': 2.298126220703125, 'learning_rate': 1.6951573745789466e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.097, 'grad_norm': 2.383479356765747, 'learning_rate': 1.4636886633645755e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.0811, 'grad_norm': 2.1532881259918213, 'learning_rate': 1.1944229491600356e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.0743, 'grad_norm': 2.146345615386963, 'learning_rate': 9.09309090777381e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.0508, 'grad_norm': 2.542090892791748, 'learning_rate': 6.315877887324114e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.0425, 'grad_norm': 2.256035327911377, 'learning_rate': 3.8389714853251495e-06, 'epoch': 0.72, 'step': 4000}, {'loss': 2.0335, 'grad_norm': 2.193110466003418, 'learning_rate': 1.8642736374494686e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.0258, 'grad_norm': 2.1559205055236816, 'learning_rate': 5.527493755338553e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.0099, 'grad_norm': 2.193859338760376, 'learning_rate': 1.1305963796519603e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 30816.5312, 'train_samples_per_second': 5.798, 'train_steps_per_second': 0.181, 'total_flos': 3.6681073533036134e+17, 'train_loss': 2.0634209251779607, 'epoch': 1.0, 'step': 5583}, {'eval_loss': 1.943244218826294, 'eval_runtime': 1871.9391, 'eval_samples_per_second': 25.431, 'eval_steps_per_second': 25.431, 'epoch': 1.0, 'step': 5583}]
Model STATE saved successfully 
 Took1.0943412780761718e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8886840462684631 minutes
GPU memory usage before cleaning cache: 22514.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
______________________________________________________________________________________________________________________________________________________
Training for 2-4 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 2-5
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_2_5_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_2_5_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180118
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 48018
}) 
 Took0.9936020493507385 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5628, training_loss=2.066188153427547, metrics={'train_runtime': 33654.6779, 'train_samples_per_second': 5.352, 'train_steps_per_second': 0.167, 'train_loss': 2.066188153427547, 'epoch': 1.0})
 Took560.9819830258688 minutes
GPU memory usage before cleaning cache: 35876.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 1.9304555654525757, 'eval_runtime': 2044.9656, 'eval_samples_per_second': 23.481, 'eval_steps_per_second': 23.481, 'epoch': 1.0}
 Took34.08643001317978 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.087, 'grad_norm': 2.328289270401001, 'learning_rate': 1.974273384050835e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.1045, 'grad_norm': 2.4160821437835693, 'learning_rate': 1.8720235335446345e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.1066, 'grad_norm': 2.337193727493286, 'learning_rate': 1.6998365631364526e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.1076, 'grad_norm': 2.4280200004577637, 'learning_rate': 1.471522032238116e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.0841, 'grad_norm': 2.4205334186553955, 'learning_rate': 1.2053909841971547e-05, 'epoch': 0.44, 'step': 2500}, {'loss': 2.0737, 'grad_norm': 2.1336135864257812, 'learning_rate': 9.227873831389676e-06, 'epoch': 0.53, 'step': 3000}, {'loss': 2.0582, 'grad_norm': 2.328242540359497, 'learning_rate': 6.4637630766710925e-06, 'epoch': 0.62, 'step': 3500}, {'loss': 2.0377, 'grad_norm': 2.2361042499542236, 'learning_rate': 3.983261899211708e-06, 'epoch': 0.71, 'step': 4000}, {'loss': 2.0347, 'grad_norm': 2.067373514175415, 'learning_rate': 1.9853088615521665e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 2.0216, 'grad_norm': 1.9254131317138672, 'learning_rate': 6.301417047375347e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 2.0214, 'grad_norm': 2.318135976791382, 'learning_rate': 2.6446128508180734e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 33654.6779, 'train_samples_per_second': 5.352, 'train_steps_per_second': 0.167, 'total_flos': 3.940590720173261e+17, 'train_loss': 2.066188153427547, 'epoch': 1.0, 'step': 5628}, {'eval_loss': 1.9304555654525757, 'eval_runtime': 2044.9656, 'eval_samples_per_second': 23.481, 'eval_steps_per_second': 23.481, 'epoch': 1.0, 'step': 5628}]
Model STATE saved successfully 
 Took1.201629638671875e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.046410326162974 minutes
GPU memory usage before cleaning cache: 22514.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
______________________________________________________________________________________________________________________________________________________
Training for 2-5 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 2-6
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_2_6_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_2_6_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178057
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47435
}) 
 Took1.0070355534553528 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5564, training_loss=2.0463635289009146, metrics={'train_runtime': 29988.8881, 'train_samples_per_second': 5.937, 'train_steps_per_second': 0.186, 'train_loss': 2.0463635289009146, 'epoch': 1.0})
 Took499.8851144194603 minutes
GPU memory usage before cleaning cache: 35656.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 1.9185757637023926, 'eval_runtime': 1786.1538, 'eval_samples_per_second': 26.557, 'eval_steps_per_second': 26.557, 'epoch': 1.0}
 Took29.77304885784785 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.0597, 'grad_norm': 2.191850423812866, 'learning_rate': 1.973669855385235e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.0853, 'grad_norm': 2.4250552654266357, 'learning_rate': 1.8690755620655773e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.0818, 'grad_norm': 2.4412004947662354, 'learning_rate': 1.693149529487441e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.0786, 'grad_norm': 2.599059581756592, 'learning_rate': 1.4603313650134074e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.0666, 'grad_norm': 2.0559680461883545, 'learning_rate': 1.1897302523874405e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.0456, 'grad_norm': 2.259901762008667, 'learning_rate': 9.035565135325415e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.0229, 'grad_norm': 2.258157253265381, 'learning_rate': 6.2529863600220285e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.0193, 'grad_norm': 2.1511001586914062, 'learning_rate': 3.77795391513163e-06, 'epoch': 0.72, 'step': 4000}, {'loss': 2.0294, 'grad_norm': 2.4420394897460938, 'learning_rate': 1.813612816716558e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.0148, 'grad_norm': 2.556863784790039, 'learning_rate': 5.211917003097544e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.0096, 'grad_norm': 2.329664468765259, 'learning_rate': 6.7695423681901625e-09, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 29988.8881, 'train_samples_per_second': 5.937, 'train_steps_per_second': 0.186, 'total_flos': 3.543882495650611e+17, 'train_loss': 2.0463635289009146, 'epoch': 1.0, 'step': 5564}, {'eval_loss': 1.9185757637023926, 'eval_runtime': 1786.1538, 'eval_samples_per_second': 26.557, 'eval_steps_per_second': 26.557, 'epoch': 1.0, 'step': 5564}]
Model STATE saved successfully 
 Took1.127322514851888e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8613489071528116 minutes
GPU memory usage before cleaning cache: 22514.81 MB
GPU memory usage after cleaning cache: 21818.81 MB
GPU memory occupied from nvmlInit: 22439 MB.
______________________________________________________________________________________________________________________________________________________
Training for 2-6 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 2-7
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_2_7_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_2_7_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180142
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47732
}) 
 Took0.9536028663317363 minutes
