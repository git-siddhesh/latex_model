______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

test:False,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:0,  max_grad_norm:0.9,  start_month_index:8,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+

+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |  4000  |
|        SAVE STEPS       |  2000  |
|      LOGGING STEPS      |  500   |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
MODEL PATH: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
Tokenizer loaded__________________________________________________________
____________________________________________________________________________________________________
Loading the model

+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED took __________ 0.044197173913319905 minutes_________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|         Training Argument          |                                                    Value                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|             output_dir             |                              /home/iitgn_cse/latex_model/model_main_fp32_2024-                              |
|                                    |                                 04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-                                |
|                                    |                              05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30                             |
|                                    |                              000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4                             |
|                                    |                                                     096                                                     |
|        overwrite_output_dir        |                                                     True                                                    |
|              do_train              |                                                     True                                                    |
|              do_eval               |                                                     True                                                    |
|             do_predict             |                                                    False                                                    |
|        evaluation_strategy         |                                                    steps                                                    |
|        prediction_loss_only        |                                                    False                                                    |
|    per_device_train_batch_size     |                                                      1                                                      |
|     per_device_eval_batch_size     |                                                      1                                                      |
|      per_gpu_train_batch_size      |                                                     None                                                    |
|      per_gpu_eval_batch_size       |                                                     None                                                    |
|    gradient_accumulation_steps     |                                                      32                                                     |
|      eval_accumulation_steps       |                                                      32                                                     |
|             eval_delay             |                                                      0                                                      |
|           learning_rate            |                                                    2e-05                                                    |
|            weight_decay            |                                                     0.1                                                     |
|             adam_beta1             |                                                     0.9                                                     |
|             adam_beta2             |                                                    0.999                                                    |
|            adam_epsilon            |                                                    1e-08                                                    |
|           max_grad_norm            |                                                     0.9                                                     |
|          num_train_epochs          |                                                      1                                                      |
|             max_steps              |                                                      -1                                                     |
|         lr_scheduler_type          |                                                    cosine                                                   |
|        lr_scheduler_kwargs         |                                                      {}                                                     |
|            warmup_ratio            |                                                     0.0                                                     |
|            warmup_steps            |                                                     100                                                     |
|             log_level              |                                                    debug                                                    |
|         log_level_replica          |                                                   warning                                                   |
|          log_on_each_node          |                                                     True                                                    |
|            logging_dir             |                                                    ./logs                                                   |
|          logging_strategy          |                                                    steps                                                    |
|         logging_first_step         |                                                    False                                                    |
|           logging_steps            |                                                     500                                                     |
|       logging_nan_inf_filter       |                                                     True                                                    |
|           save_strategy            |                                                    steps                                                    |
|             save_steps             |                                                     2000                                                    |
|          save_total_limit          |                                                      3                                                      |
|          save_safetensors          |                                                     True                                                    |
|         save_on_each_node          |                                                    False                                                    |
|          save_only_model           |                                                    False                                                    |
|              no_cuda               |                                                    False                                                    |
|              use_cpu               |                                                    False                                                    |
|           use_mps_device           |                                                    False                                                    |
|                seed                |                                                      42                                                     |
|             data_seed              |                                                     None                                                    |
|           jit_mode_eval            |                                                    False                                                    |
|              use_ipex              |                                                    False                                                    |
|                bf16                |                                                    False                                                    |
|                fp16                |                                                    False                                                    |
|           fp16_opt_level           |                                                      O1                                                     |
|       half_precision_backend       |                                                     auto                                                    |
|           bf16_full_eval           |                                                    False                                                    |
|           fp16_full_eval           |                                                    False                                                    |
|                tf32                |                                                     None                                                    |
|             local_rank             |                                                      0                                                      |
|            ddp_backend             |                                                     None                                                    |
|           tpu_num_cores            |                                                     None                                                    |
|         tpu_metrics_debug          |                                                    False                                                    |
|               debug                |                                                      []                                                     |
|        dataloader_drop_last        |                                                    False                                                    |
|             eval_steps             |                                                     4000                                                    |
|       dataloader_num_workers       |                                                      0                                                      |
|     dataloader_prefetch_factor     |                                                     None                                                    |
|             past_index             |                                                      -1                                                     |
|              run_name              |                              run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-                              |
|                                    |                                               07-17_test=False                                              |
|            disable_tqdm            |                                                    False                                                    |
|       remove_unused_columns        |                                                     True                                                    |
|            label_names             |                                                     None                                                    |
|       load_best_model_at_end       |                                                    False                                                    |
|       metric_for_best_model        |                                                     None                                                    |
|         greater_is_better          |                                                     None                                                    |
|          ignore_data_skip          |                                                    False                                                    |
|                fsdp                |                                                      []                                                     |
|        fsdp_min_num_params         |                                                      0                                                      |
|            fsdp_config             |            {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}           |
| fsdp_transformer_layer_cls_to_wrap |                                                     None                                                    |
|         accelerator_config         | AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True) |
|             deepspeed              |                                                     None                                                    |
|       label_smoothing_factor       |                                                     0.0                                                     |
|               optim                |                                                 adamw_torch                                                 |
|             optim_args             |                                                     None                                                    |
|             adafactor              |                                                    False                                                    |
|          group_by_length           |                                                    False                                                    |
|         length_column_name         |                                                    length                                                   |
|             report_to              |                                                  ['wandb']                                                  |
|     ddp_find_unused_parameters     |                                                     None                                                    |
|         ddp_bucket_cap_mb          |                                                     None                                                    |
|       ddp_broadcast_buffers        |                                                     None                                                    |
|       dataloader_pin_memory        |                                                     True                                                    |
|   dataloader_persistent_workers    |                                                    False                                                    |
|        skip_memory_metrics         |                                                     True                                                    |
|     use_legacy_prediction_loop     |                                                    False                                                    |
|            push_to_hub             |                                                    False                                                    |
|       resume_from_checkpoint       |                                                     None                                                    |
|            hub_model_id            |                                                     None                                                    |
|            hub_strategy            |                                                  every_save                                                 |
|             hub_token              |                                                     None                                                    |
|          hub_private_repo          |                                                    False                                                    |
|          hub_always_push           |                                                    False                                                    |
|       gradient_checkpointing       |                                                    False                                                    |
|   gradient_checkpointing_kwargs    |                                                     None                                                    |
|     include_inputs_for_metrics     |                                                    False                                                    |
|            fp16_backend            |                                                     auto                                                    |
|        push_to_hub_model_id        |                                                     None                                                    |
|      push_to_hub_organization      |                                                     None                                                    |
|         push_to_hub_token          |                                                     None                                                    |
|           mp_parameters            |                                                                                                             |
|        auto_find_batch_size        |                                                    False                                                    |
|          full_determinism          |                                                    False                                                    |
|            torchdynamo             |                                                     None                                                    |
|             ray_scope              |                                                     last                                                    |
|            ddp_timeout             |                                                     1800                                                    |
|           torch_compile            |                                                    False                                                    |
|       torch_compile_backend        |                                                     None                                                    |
|         torch_compile_mode         |                                                     None                                                    |
|          dispatch_batches          |                                                     None                                                    |
|           split_batches            |                                                     None                                                    |
|     include_tokens_per_second      |                                                    False                                                    |
|   include_num_input_tokens_seen    |                                                    False                                                    |
|        neftune_noise_alpha         |                                                     None                                                    |
|         distributed_state          |                                 Distributed environment: DistributedType.NO                                 |
|                                    |                                               Num processes: 1                                              |
|                                    |                                               Process index: 0                                              |
|                                    |                                            Local process index: 0                                           |
|                                    |                                                 Device: cuda                                                |
|                                    |                                                                                                             |
|               _n_gpu               |                                                      1                                                      |
|      __cached__setup_devices       |                                                    cuda:0                                                   |
|          deepspeed_plugin          |                                                     None                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+
______________________________________________________________________________________________________________________________________________________
Training for 0-8
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_8_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_8_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 185322
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47481
}) 
 Took0.5766875465710958 minutes
Popen(['git', 'cat-file', '--batch-check'], cwd=/home/iitgn_cse/latex_model, stdin=<valid stream>, shell=False, universal_newlines=False)
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5791, training_loss=2.202342282879684, metrics={'train_runtime': 41694.0175, 'train_samples_per_second': 4.445, 'train_steps_per_second': 0.139, 'train_loss': 2.202342282879684, 'epoch': 1.0})
 Took694.9260531147321 minutes
GPU memory usage before cleaning cache: 35836.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.401905059814453, 'eval_runtime': 1821.9596, 'eval_samples_per_second': 26.06, 'eval_steps_per_second': 26.06, 'epoch': 1.0}
 Took30.368993190924325 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.0079, 'grad_norm': 3.51334285736084, 'learning_rate': 1.9757200802888278e-05, 'epoch': 0.09, 'step': 500}, {'loss': 1.9976, 'grad_norm': 4.018144607543945, 'learning_rate': 1.8791001183919126e-05, 'epoch': 0.17, 'step': 1000}, {'loss': 1.994, 'grad_norm': 3.7757203578948975, 'learning_rate': 1.7159310873021694e-05, 'epoch': 0.26, 'step': 1500}, {'loss': 2.04, 'grad_norm': 3.6978724002838135, 'learning_rate': 1.4985651035907816e-05, 'epoch': 0.35, 'step': 2000}, {'loss': 2.0886, 'grad_norm': 3.601090431213379, 'learning_rate': 1.2434570665206874e-05, 'epoch': 0.43, 'step': 2500}, {'loss': 2.155, 'grad_norm': 3.662611961364746, 'learning_rate': 9.69918999893772e-06, 'epoch': 0.52, 'step': 3000}, {'loss': 2.2358, 'grad_norm': 4.015500068664551, 'learning_rate': 6.986581057021424e-06, 'epoch': 0.6, 'step': 3500}, {'loss': 2.2976, 'grad_norm': 3.844238758087158, 'learning_rate': 4.502092009014481e-06, 'epoch': 0.69, 'step': 4000}, {'eval_loss': 2.4381582736968994, 'eval_runtime': 1818.3585, 'eval_samples_per_second': 26.112, 'eval_steps_per_second': 26.112, 'epoch': 0.69, 'step': 4000}, {'loss': 2.3648, 'grad_norm': 3.7141716480255127, 'learning_rate': 2.433802039648527e-06, 'epoch': 0.78, 'step': 4500}, {'loss': 2.4198, 'grad_norm': 3.48384428024292, 'learning_rate': 9.382834999021373e-07, 'epoch': 0.86, 'step': 5000}, {'loss': 2.4611, 'grad_norm': 4.011415958404541, 'learning_rate': 1.2874916804543557e-07, 'epoch': 0.95, 'step': 5500}, {'train_runtime': 41694.0175, 'train_samples_per_second': 4.445, 'train_steps_per_second': 0.139, 'total_flos': 5.033891193229148e+17, 'train_loss': 2.202342282879684, 'epoch': 1.0, 'step': 5791}, {'eval_loss': 2.401905059814453, 'eval_runtime': 1821.9596, 'eval_samples_per_second': 26.06, 'eval_steps_per_second': 26.06, 'epoch': 1.0, 'step': 5791}]
Model STATE saved successfully 
 Took1.0947386423746744e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8956551829973857 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 0-8 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 0-9
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_9_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_9_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 176889
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47312
}) 
 Took0.6973181049029032 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5527, training_loss=2.5031826032400777, metrics={'train_runtime': 30211.1028, 'train_samples_per_second': 5.855, 'train_steps_per_second': 0.183, 'train_loss': 2.5031826032400777, 'epoch': 1.0})
 Took503.546858839194 minutes
GPU memory usage before cleaning cache: 35876.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.3370137214660645, 'eval_runtime': 1734.8073, 'eval_samples_per_second': 27.272, 'eval_steps_per_second': 27.272, 'epoch': 1.0}
 Took28.928286957740784 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.5789, 'grad_norm': 3.6314802169799805, 'learning_rate': 1.9733112171333944e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.5823, 'grad_norm': 3.01324462890625, 'learning_rate': 1.8673249484642752e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.5707, 'grad_norm': 3.139439821243286, 'learning_rate': 1.689183440322883e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.548, 'grad_norm': 3.528080701828003, 'learning_rate': 1.4537067935291622e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.5237, 'grad_norm': 3.214386463165283, 'learning_rate': 1.1804849828320395e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.5079, 'grad_norm': 3.1010096073150635, 'learning_rate': 8.922481109626837e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.487, 'grad_norm': 2.8776462078094482, 'learning_rate': 6.1297542647297615e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.4507, 'grad_norm': 2.872499465942383, 'learning_rate': 3.659004215656943e-06, 'epoch': 0.72, 'step': 4000}, {'eval_loss': 2.356057643890381, 'eval_runtime': 1735.6159, 'eval_samples_per_second': 27.259, 'eval_steps_per_second': 27.259, 'epoch': 0.72, 'step': 4000}, {'loss': 2.4369, 'grad_norm': 3.0552256107330322, 'learning_rate': 1.715779715251994e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.4326, 'grad_norm': 3.279902219772339, 'learning_rate': 4.6174315939476963e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.419, 'grad_norm': 2.997612953186035, 'learning_rate': 1.2214305934699078e-09, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 30211.1028, 'train_samples_per_second': 5.855, 'train_steps_per_second': 0.183, 'total_flos': 3.318688745374433e+17, 'train_loss': 2.5031826032400777, 'epoch': 1.0, 'step': 5527}, {'eval_loss': 2.3370137214660645, 'eval_runtime': 1734.8073, 'eval_samples_per_second': 27.272, 'eval_steps_per_second': 27.272, 'epoch': 1.0, 'step': 5527}]
Model STATE saved successfully 
 Took1.1078516642252605e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.9435601671536763 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 0-9 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 0-10
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_10_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_10_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180646
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47431
}) 
 Took0.7727700670560201 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5645, training_loss=2.446635535673508, metrics={'train_runtime': 35233.0971, 'train_samples_per_second': 5.127, 'train_steps_per_second': 0.16, 'train_loss': 2.446635535673508, 'epoch': 1.0})
 Took587.2556286891302 minutes
GPU memory usage before cleaning cache: 35836.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.2954585552215576, 'eval_runtime': 1788.3384, 'eval_samples_per_second': 26.522, 'eval_steps_per_second': 26.522, 'epoch': 1.0}
 Took29.80880964199702 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.5092, 'grad_norm': 2.9751784801483154, 'learning_rate': 1.9744302141513234e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.538, 'grad_norm': 2.852600336074829, 'learning_rate': 1.8727899859338823e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.4998, 'grad_norm': 3.1036202907562256, 'learning_rate': 1.7015768467520824e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.4901, 'grad_norm': 2.9644899368286133, 'learning_rate': 1.474438749355379e-05, 'epoch': 0.35, 'step': 2000}, {'loss': 2.4632, 'grad_norm': 3.1875953674316406, 'learning_rate': 1.2094816056830412e-05, 'epoch': 0.44, 'step': 2500}, {'loss': 2.4492, 'grad_norm': 2.6688334941864014, 'learning_rate': 9.278260060983538e-06, 'epoch': 0.53, 'step': 3000}, {'loss': 2.4318, 'grad_norm': 3.0402286052703857, 'learning_rate': 6.519236288610659e-06, 'epoch': 0.62, 'step': 3500}, {'loss': 2.402, 'grad_norm': 2.8612289428710938, 'learning_rate': 4.0376754428103725e-06, 'epoch': 0.71, 'step': 4000}, {'eval_loss': 2.3158397674560547, 'eval_runtime': 1791.7142, 'eval_samples_per_second': 26.472, 'eval_steps_per_second': 26.472, 'epoch': 0.71, 'step': 4000}, {'loss': 2.3886, 'grad_norm': 2.633077383041382, 'learning_rate': 2.031390759939601e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 2.3836, 'grad_norm': 2.841747760772705, 'learning_rate': 6.603096771368234e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 2.3731, 'grad_norm': 2.58789324760437, 'learning_rate': 3.3725499362091685e-08, 'epoch': 0.97, 'step': 5500}, {'train_runtime': 35233.0971, 'train_samples_per_second': 5.127, 'train_steps_per_second': 0.16, 'total_flos': 4.0592440877789184e+17, 'train_loss': 2.446635535673508, 'epoch': 1.0, 'step': 5645}, {'eval_loss': 2.2954585552215576, 'eval_runtime': 1788.3384, 'eval_samples_per_second': 26.522, 'eval_steps_per_second': 26.522, 'epoch': 1.0, 'step': 5645}]
Model STATE saved successfully 
 Took1.060167948404948e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.9345137000083923 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 0-10 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 0-11
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_11_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_11_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178887
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47365
}) 
 Took0.7803868730862935 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5590, training_loss=2.4122124032172407, metrics={'train_runtime': 32905.7305, 'train_samples_per_second': 5.436, 'train_steps_per_second': 0.17, 'train_loss': 2.4122124032172407, 'epoch': 1.0})
 Took548.4553915659586 minutes
GPU memory usage before cleaning cache: 35676.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.2558741569519043, 'eval_runtime': 1759.7544, 'eval_samples_per_second': 26.916, 'eval_steps_per_second': 26.916, 'epoch': 1.0}
 Took29.33250022729238 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.4611, 'grad_norm': 2.8791728019714355, 'learning_rate': 1.973917570554464e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.4706, 'grad_norm': 2.844473361968994, 'learning_rate': 1.8702852410301556e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.4906, 'grad_norm': 2.689962387084961, 'learning_rate': 1.6958922493101844e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.4548, 'grad_norm': 2.687206983566284, 'learning_rate': 1.4649180425396972e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.429, 'grad_norm': 3.295351982116699, 'learning_rate': 1.196142541428197e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.4117, 'grad_norm': 2.732855796813965, 'learning_rate': 9.11419193034155e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.3947, 'grad_norm': 2.632277727127075, 'learning_rate': 6.3389812300037774e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.3868, 'grad_norm': 2.6183342933654785, 'learning_rate': 3.86143858177388e-06, 'epoch': 0.72, 'step': 4000}, {'eval_loss': 2.274614095687866, 'eval_runtime': 1757.9882, 'eval_samples_per_second': 26.943, 'eval_steps_per_second': 26.943, 'epoch': 0.72, 'step': 4000}, {'loss': 2.3583, 'grad_norm': 2.7637362480163574, 'learning_rate': 1.8830066278357395e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 2.3484, 'grad_norm': 2.6849029064178467, 'learning_rate': 5.645465991623167e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 2.3453, 'grad_norm': 2.7352488040924072, 'learning_rate': 1.325910115169471e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 32905.7305, 'train_samples_per_second': 5.436, 'train_steps_per_second': 0.17, 'total_flos': 3.714806355725353e+17, 'train_loss': 2.4122124032172407, 'epoch': 1.0, 'step': 5590}, {'eval_loss': 2.2558741569519043, 'eval_runtime': 1759.7544, 'eval_samples_per_second': 26.916, 'eval_steps_per_second': 26.916, 'epoch': 1.0, 'step': 5590}]
Model STATE saved successfully 
 Took1.1142094930013021e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.0255101680755616 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 0-11 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 0-12
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_12_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_12_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 179194
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47533
}) 
 Took0.7894824266433715 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5599, training_loss=2.3634083620627706, metrics={'train_runtime': 33403.833, 'train_samples_per_second': 5.364, 'train_steps_per_second': 0.168, 'train_loss': 2.3634083620627706, 'epoch': 1.0})
 Took556.7616201996804 minutes
GPU memory usage before cleaning cache: 35836.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.2215023040771484, 'eval_runtime': 1831.5825, 'eval_samples_per_second': 25.952, 'eval_steps_per_second': 25.952, 'epoch': 1.0}
 Took30.554282661279043 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.404, 'grad_norm': 2.5237112045288086, 'learning_rate': 1.9740025053250186e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.4336, 'grad_norm': 2.7545595169067383, 'learning_rate': 1.8707001032347e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.4215, 'grad_norm': 3.0315628051757812, 'learning_rate': 1.6968332742122285e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.404, 'grad_norm': 2.9095113277435303, 'learning_rate': 1.4664927632878405e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.3846, 'grad_norm': 2.7624130249023438, 'learning_rate': 1.1983461301554595e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.3712, 'grad_norm': 2.386338233947754, 'learning_rate': 9.141248681621957e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.3421, 'grad_norm': 2.553156614303589, 'learning_rate': 6.368632119428898e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.3213, 'grad_norm': 2.948885917663574, 'learning_rate': 3.890313665758348e-06, 'epoch': 0.71, 'step': 4000}, {'eval_loss': 2.2395241260528564, 'eval_runtime': 1830.7846, 'eval_samples_per_second': 25.963, 'eval_steps_per_second': 25.963, 'epoch': 0.71, 'step': 4000}, {'loss': 2.3161, 'grad_norm': 2.566927433013916, 'learning_rate': 1.907144475662378e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 2.3001, 'grad_norm': 2.4983372688293457, 'learning_rate': 5.798471640674519e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 2.3104, 'grad_norm': 2.617875814437866, 'learning_rate': 1.599031149093988e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 33403.833, 'train_samples_per_second': 5.364, 'train_steps_per_second': 0.168, 'total_flos': 3.78861451893547e+17, 'train_loss': 2.3634083620627706, 'epoch': 1.0, 'step': 5599}, {'eval_loss': 2.2215023040771484, 'eval_runtime': 1831.5825, 'eval_samples_per_second': 25.952, 'eval_steps_per_second': 25.952, 'epoch': 1.0, 'step': 5599}]
Model STATE saved successfully 
 Took1.0569890340169271e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.9766738454500834 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 0-12 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-8
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_8_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_8_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 176225
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47598
}) 
 Took0.783181611696879 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5507, training_loss=2.327674018897269, metrics={'train_runtime': 29546.3368, 'train_samples_per_second': 5.964, 'train_steps_per_second': 0.186, 'train_loss': 2.327674018897269, 'epoch': 1.0})
 Took492.47552626132966 minutes
GPU memory usage before cleaning cache: 35756.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.2009196281433105, 'eval_runtime': 1865.8666, 'eval_samples_per_second': 25.51, 'eval_steps_per_second': 25.51, 'epoch': 1.0}
 Took31.101356315612794 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.3652, 'grad_norm': 2.5532288551330566, 'learning_rate': 1.973114304365143e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.3846, 'grad_norm': 2.7551584243774414, 'learning_rate': 1.8663641350141968e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.3791, 'grad_norm': 2.9414072036743164, 'learning_rate': 1.6870082350552917e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.3812, 'grad_norm': 2.494396924972534, 'learning_rate': 1.4500775431308162e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.3457, 'grad_norm': 2.4841599464416504, 'learning_rate': 1.1754280589242567e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.3082, 'grad_norm': 2.4345521926879883, 'learning_rate': 8.860768091752868e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.3048, 'grad_norm': 2.7179455757141113, 'learning_rate': 6.062729035149349e-06, 'epoch': 0.64, 'step': 3500}, {'loss': 2.2967, 'grad_norm': 2.6819469928741455, 'learning_rate': 3.594653354741913e-06, 'epoch': 0.73, 'step': 4000}, {'eval_loss': 2.2165277004241943, 'eval_runtime': 1865.9277, 'eval_samples_per_second': 25.509, 'eval_steps_per_second': 25.509, 'epoch': 0.73, 'step': 4000}, {'loss': 2.2844, 'grad_norm': 2.508068084716797, 'learning_rate': 1.6633783730021125e-06, 'epoch': 0.82, 'step': 4500}, {'loss': 2.2721, 'grad_norm': 2.707127094268799, 'learning_rate': 4.307547776338994e-07, 'epoch': 0.91, 'step': 5000}, {'loss': 2.2813, 'grad_norm': 2.191084146499634, 'learning_rate': 8.270894704676124e-11, 'epoch': 1.0, 'step': 5500}, {'train_runtime': 29546.3368, 'train_samples_per_second': 5.964, 'train_steps_per_second': 0.186, 'total_flos': 3.20170952693932e+17, 'train_loss': 2.327674018897269, 'epoch': 1.0, 'step': 5507}, {'eval_loss': 2.2009196281433105, 'eval_runtime': 1865.8666, 'eval_samples_per_second': 25.51, 'eval_steps_per_second': 25.51, 'epoch': 1.0, 'step': 5507}]
Model STATE saved successfully 
 Took1.1030832926432291e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.071294331550598 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-8 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-9
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_9_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_9_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 177393
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47865
}) 
 Took0.8821193218231201 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5543, training_loss=2.3016899806722373, metrics={'train_runtime': 31152.6229, 'train_samples_per_second': 5.694, 'train_steps_per_second': 0.178, 'train_loss': 2.3016899806722373, 'epoch': 1.0})
 Took519.2485886136691 minutes
GPU memory usage before cleaning cache: 35696.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.1726136207580566, 'eval_runtime': 1979.3943, 'eval_samples_per_second': 24.182, 'eval_steps_per_second': 24.182, 'epoch': 1.0}
 Took32.99405865271886 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.3367, 'grad_norm': 2.8316187858581543, 'learning_rate': 1.9734671962705725e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.3589, 'grad_norm': 2.6758902072906494, 'learning_rate': 1.8680862185152324e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.3549, 'grad_norm': 2.696659803390503, 'learning_rate': 1.6909076794243203e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.3541, 'grad_norm': 2.529670476913452, 'learning_rate': 1.4565856384177458e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.324, 'grad_norm': 2.5480873584747314, 'learning_rate': 1.184500372192242e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.3146, 'grad_norm': 2.667915105819702, 'learning_rate': 8.971554733330519e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.2798, 'grad_norm': 2.310657501220703, 'learning_rate': 6.183166261239242e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.2514, 'grad_norm': 2.712125778198242, 'learning_rate': 3.7104599744210768e-06, 'epoch': 0.72, 'step': 4000}, {'eval_loss': 2.1885485649108887, 'eval_runtime': 1981.2339, 'eval_samples_per_second': 24.159, 'eval_steps_per_second': 24.159, 'epoch': 0.72, 'step': 4000}, {'loss': 2.2708, 'grad_norm': 2.796022653579712, 'learning_rate': 1.757948140789142e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.2418, 'grad_norm': 2.8949742317199707, 'learning_rate': 4.871188535116455e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.2402, 'grad_norm': 2.6612937450408936, 'learning_rate': 3.07969493957172e-09, 'epoch': 0.99, 'step': 5500}, {'train_runtime': 31152.6229, 'train_samples_per_second': 5.694, 'train_steps_per_second': 0.178, 'total_flos': 3.423692165273272e+17, 'train_loss': 2.3016899806722373, 'epoch': 1.0, 'step': 5543}, {'eval_loss': 2.1726136207580566, 'eval_runtime': 1979.3943, 'eval_samples_per_second': 24.182, 'eval_steps_per_second': 24.182, 'epoch': 1.0, 'step': 5543}]
Model STATE saved successfully 
 Took1.1289119720458985e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.861203932762146 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-9 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-10
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_10_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_10_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180974
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 48309
}) 
 Took0.8521963119506836 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5655, training_loss=2.299876850374392, metrics={'train_runtime': 35806.509, 'train_samples_per_second': 5.054, 'train_steps_per_second': 0.158, 'train_loss': 2.299876850374392, 'epoch': 1.0})
 Took596.8176219344139 minutes
GPU memory usage before cleaning cache: 35756.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': nan, 'eval_runtime': 2164.82, 'eval_samples_per_second': 22.315, 'eval_steps_per_second': 22.315, 'epoch': 1.0}
 Took36.08520522514979 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.326, 'grad_norm': 2.379244089126587, 'learning_rate': 1.974521799144785e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.3634, 'grad_norm': 2.4612841606140137, 'learning_rate': 1.8732376531877725e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.3553, 'grad_norm': 2.6148440837860107, 'learning_rate': 1.7025936310103882e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.3359, 'grad_norm': 2.7557895183563232, 'learning_rate': 1.4761437155550342e-05, 'epoch': 0.35, 'step': 2000}, {'loss': 2.3283, 'grad_norm': 2.5546841621398926, 'learning_rate': 1.211874463000042e-05, 'epoch': 0.44, 'step': 2500}, {'loss': 2.2989, 'grad_norm': 2.5871875286102295, 'learning_rate': 9.307763592448131e-06, 'epoch': 0.53, 'step': 3000}, {'loss': 2.2863, 'grad_norm': 2.8898110389709473, 'learning_rate': 6.551765790407689e-06, 'epoch': 0.62, 'step': 3500}, {'loss': 2.2778, 'grad_norm': 2.481220245361328, 'learning_rate': 4.0696557405537005e-06, 'epoch': 0.71, 'step': 4000}, {'eval_loss': nan, 'eval_runtime': 2167.4732, 'eval_samples_per_second': 22.288, 'eval_steps_per_second': 22.288, 'epoch': 0.71, 'step': 4000}, {'loss': 2.2508, 'grad_norm': 2.7009522914886475, 'learning_rate': 2.058583491552465e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 2.242, 'grad_norm': 2.7467055320739746, 'learning_rate': 6.782853096446196e-07, 'epoch': 0.88, 'step': 5000}, {'loss': 2.24, 'grad_norm': 2.400463104248047, 'learning_rate': 3.839608138692219e-08, 'epoch': 0.97, 'step': 5500}, {'train_runtime': 35806.509, 'train_samples_per_second': 5.054, 'train_steps_per_second': 0.158, 'total_flos': 4.093625018527826e+17, 'train_loss': 2.299876850374392, 'epoch': 1.0, 'step': 5655}, {'eval_loss': nan, 'eval_runtime': 2164.82, 'eval_samples_per_second': 22.315, 'eval_steps_per_second': 22.315, 'epoch': 1.0, 'step': 5655}]
Model STATE saved successfully 
 Took1.2882550557454427e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.05472621122996 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-10 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-11
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_11_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_11_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 180163
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47729
}) 
 Took0.9497260729471843 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5630, training_loss=2.2680604057244254, metrics={'train_runtime': 34630.3799, 'train_samples_per_second': 5.202, 'train_steps_per_second': 0.163, 'train_loss': 2.2680604057244254, 'epoch': 1.0})
 Took577.2212373455366 minutes
GPU memory usage before cleaning cache: 35676.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.134350538253784, 'eval_runtime': 1912.0582, 'eval_samples_per_second': 24.962, 'eval_steps_per_second': 24.962, 'epoch': 1.0}
 Took31.871062803268433 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.3053, 'grad_norm': 2.3955819606781006, 'learning_rate': 1.974291909318295e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.3307, 'grad_norm': 2.4031593799591064, 'learning_rate': 1.8721140605907694e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.3141, 'grad_norm': 2.645470142364502, 'learning_rate': 1.7000420745694256e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.2944, 'grad_norm': 2.446094036102295, 'learning_rate': 1.4718663756481192e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.2882, 'grad_norm': 2.422811508178711, 'learning_rate': 1.2058737273916023e-05, 'epoch': 0.44, 'step': 2500}, {'loss': 2.2696, 'grad_norm': 2.481868028640747, 'learning_rate': 9.23381670149807e-06, 'epoch': 0.53, 'step': 3000}, {'loss': 2.2551, 'grad_norm': 2.365225315093994, 'learning_rate': 6.470300620852041e-06, 'epoch': 0.62, 'step': 3500}, {'loss': 2.2388, 'grad_norm': 2.767874240875244, 'learning_rate': 3.989666452286358e-06, 'epoch': 0.71, 'step': 4000}, {'eval_loss': 2.150871753692627, 'eval_runtime': 1913.2826, 'eval_samples_per_second': 24.946, 'eval_steps_per_second': 24.946, 'epoch': 0.71, 'step': 4000}, {'loss': 2.2322, 'grad_norm': 2.3349883556365967, 'learning_rate': 1.9907205136764863e-06, 'epoch': 0.8, 'step': 4500}, {'loss': 2.22, 'grad_norm': 2.481208324432373, 'learning_rate': 6.336650173127224e-07, 'epoch': 0.89, 'step': 5000}, {'loss': 2.2164, 'grad_norm': 2.5628645420074463, 'learning_rate': 2.7258928867432978e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 34630.3799, 'train_samples_per_second': 5.202, 'train_steps_per_second': 0.163, 'total_flos': 3.95353682049024e+17, 'train_loss': 2.2680604057244254, 'epoch': 1.0, 'step': 5630}, {'eval_loss': 2.134350538253784, 'eval_runtime': 1912.0582, 'eval_samples_per_second': 24.962, 'eval_steps_per_second': 24.962, 'epoch': 1.0, 'step': 5630}]
Model STATE saved successfully 
 Took1.0704994201660156e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took0.8233345945676168 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-11 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 1-12
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_12_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_12_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178758
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47461
}) 
 Took0.8783260265986125 minutes
____________________________________________________________________________________________________
***** Train results ***** TrainOutput(global_step=5586, training_loss=2.2380608443585595, metrics={'train_runtime': 32709.3254, 'train_samples_per_second': 5.465, 'train_steps_per_second': 0.171, 'train_loss': 2.2380608443585595, 'epoch': 1.0})
 Took545.2079543153445 minutes
GPU memory usage before cleaning cache: 35596.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
----------------------------------------------------------------------------------------------------
***** Eval results ***** {'eval_loss': 2.0987389087677, 'eval_runtime': 1794.6782, 'eval_samples_per_second': 26.445, 'eval_steps_per_second': 26.445, 'epoch': 1.0}
 Took29.914823587735494 minutes
----------------------------------------------------------------------------------------------------
***** Log history ***** [{'loss': 2.2729, 'grad_norm': 2.454479932785034, 'learning_rate': 1.973879688305121e-05, 'epoch': 0.09, 'step': 500}, {'loss': 2.2937, 'grad_norm': 2.5456366539001465, 'learning_rate': 1.8701002217923723e-05, 'epoch': 0.18, 'step': 1000}, {'loss': 2.2847, 'grad_norm': 2.4223368167877197, 'learning_rate': 1.69547263963467e-05, 'epoch': 0.27, 'step': 1500}, {'loss': 2.2761, 'grad_norm': 2.514615535736084, 'learning_rate': 1.4642160338695046e-05, 'epoch': 0.36, 'step': 2000}, {'loss': 2.2469, 'grad_norm': 2.433300018310547, 'learning_rate': 1.1951605281292555e-05, 'epoch': 0.45, 'step': 2500}, {'loss': 2.2517, 'grad_norm': 2.513852834701538, 'learning_rate': 9.10214030386097e-06, 'epoch': 0.54, 'step': 3000}, {'loss': 2.2244, 'grad_norm': 2.3436429500579834, 'learning_rate': 6.325783763816022e-06, 'epoch': 0.63, 'step': 3500}, {'loss': 2.2049, 'grad_norm': 2.729001998901367, 'learning_rate': 3.8486011468273145e-06, 'epoch': 0.72, 'step': 4000}, {'eval_loss': 2.1140148639678955, 'eval_runtime': 1792.2868, 'eval_samples_per_second': 26.481, 'eval_steps_per_second': 26.481, 'epoch': 0.72, 'step': 4000}, {'loss': 2.1881, 'grad_norm': 2.495842695236206, 'learning_rate': 1.8722976313633168e-06, 'epoch': 0.81, 'step': 4500}, {'loss': 2.1939, 'grad_norm': 2.193859577178955, 'learning_rate': 5.577941969919854e-07, 'epoch': 0.9, 'step': 5000}, {'loss': 2.1872, 'grad_norm': 2.5182483196258545, 'learning_rate': 1.212459568982638e-08, 'epoch': 0.98, 'step': 5500}, {'train_runtime': 32709.3254, 'train_samples_per_second': 5.465, 'train_steps_per_second': 0.171, 'total_flos': 3.685617920275415e+17, 'train_loss': 2.2380608443585595, 'epoch': 1.0, 'step': 5586}, {'eval_loss': 2.0987389087677, 'eval_runtime': 1794.6782, 'eval_samples_per_second': 26.445, 'eval_steps_per_second': 26.445, 'epoch': 1.0, 'step': 5586}]
Model STATE saved successfully 
 Took1.0402997334798177e-05 minutes
Model saved successfully @ /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096 
 Took1.0570849537849427 minutes
GPU memory usage before cleaning cache: 22534.81 MB
GPU memory usage after cleaning cache: 21776.81 MB
GPU memory occupied from nvmlInit: 22397 MB.
______________________________________________________________________________________________________________________________________________________
Training for 1-12 completed
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
Training for 2-8
______________________________________________________________________________________________________________________________________________________
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_2_8_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_2_8_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 178439
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47388
}) 
 Took0.8951577266057332 minutes
Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

test:False,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:2,  max_grad_norm:0.9,  start_month_index:1,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+

+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |  6000  |
|        SAVE STEPS       |  2000  |
|      LOGGING STEPS      |  500   |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
MODEL PATH: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
Tokenizer loaded__________________________________________________________
____________________________________________________________________________________________________
Loading the model

+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED took __________ 0.03977888027826945 minutes_________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|         Training Argument          |                                                    Value                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|             output_dir             |                              /home/iitgn_cse/latex_model/model_main_fp32_2024-                              |
|                                    |                                 04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-                                |
|                                    |                              05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30                             |
|                                    |                              000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4                             |
|                                    |                                                     096                                                     |
|        overwrite_output_dir        |                                                     True                                                    |
|              do_train              |                                                     True                                                    |
|              do_eval               |                                                     True                                                    |
|             do_predict             |                                                    False                                                    |
|        evaluation_strategy         |                                                    steps                                                    |
|        prediction_loss_only        |                                                    False                                                    |
|    per_device_train_batch_size     |                                                      1                                                      |
|     per_device_eval_batch_size     |                                                      1                                                      |
|      per_gpu_train_batch_size      |                                                     None                                                    |
|      per_gpu_eval_batch_size       |                                                     None                                                    |
|    gradient_accumulation_steps     |                                                      32                                                     |
|      eval_accumulation_steps       |                                                      32                                                     |
|             eval_delay             |                                                      0                                                      |
|           learning_rate            |                                                    2e-05                                                    |
|            weight_decay            |                                                     0.1                                                     |
|             adam_beta1             |                                                     0.9                                                     |
|             adam_beta2             |                                                    0.999                                                    |
|            adam_epsilon            |                                                    1e-08                                                    |
|           max_grad_norm            |                                                     0.9                                                     |
|          num_train_epochs          |                                                      1                                                      |
|             max_steps              |                                                      -1                                                     |
|         lr_scheduler_type          |                                                    cosine                                                   |
|        lr_scheduler_kwargs         |                                                      {}                                                     |
|            warmup_ratio            |                                                     0.0                                                     |
|            warmup_steps            |                                                     100                                                     |
|             log_level              |                                                    debug                                                    |
|         log_level_replica          |                                                   warning                                                   |
|          log_on_each_node          |                                                     True                                                    |
|            logging_dir             |                                                    ./logs                                                   |
|          logging_strategy          |                                                    steps                                                    |
|         logging_first_step         |                                                    False                                                    |
|           logging_steps            |                                                     500                                                     |
|       logging_nan_inf_filter       |                                                     True                                                    |
|           save_strategy            |                                                    steps                                                    |
|             save_steps             |                                                     2000                                                    |
|          save_total_limit          |                                                      3                                                      |
|          save_safetensors          |                                                     True                                                    |
|         save_on_each_node          |                                                    False                                                    |
|          save_only_model           |                                                    False                                                    |
|              no_cuda               |                                                    False                                                    |
|              use_cpu               |                                                    False                                                    |
|           use_mps_device           |                                                    False                                                    |
|                seed                |                                                      42                                                     |
|             data_seed              |                                                     None                                                    |
|           jit_mode_eval            |                                                    False                                                    |
|              use_ipex              |                                                    False                                                    |
|                bf16                |                                                    False                                                    |
|                fp16                |                                                    False                                                    |
|           fp16_opt_level           |                                                      O1                                                     |
|       half_precision_backend       |                                                     auto                                                    |
|           bf16_full_eval           |                                                    False                                                    |
|           fp16_full_eval           |                                                    False                                                    |
|                tf32                |                                                     None                                                    |
|             local_rank             |                                                      0                                                      |
|            ddp_backend             |                                                     None                                                    |
|           tpu_num_cores            |                                                     None                                                    |
|         tpu_metrics_debug          |                                                    False                                                    |
|               debug                |                                                      []                                                     |
|        dataloader_drop_last        |                                                    False                                                    |
|             eval_steps             |                                                     6000                                                    |
|       dataloader_num_workers       |                                                      0                                                      |
|     dataloader_prefetch_factor     |                                                     None                                                    |
|             past_index             |                                                      -1                                                     |
|              run_name              |                              run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-                              |
|                                    |                                               07-17_test=False                                              |
|            disable_tqdm            |                                                    False                                                    |
|       remove_unused_columns        |                                                     True                                                    |
|            label_names             |                                                     None                                                    |
|       load_best_model_at_end       |                                                    False                                                    |
|       metric_for_best_model        |                                                     None                                                    |
|         greater_is_better          |                                                     None                                                    |
|          ignore_data_skip          |                                                    False                                                    |
|                fsdp                |                                                      []                                                     |
|        fsdp_min_num_params         |                                                      0                                                      |
|            fsdp_config             |            {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}           |
| fsdp_transformer_layer_cls_to_wrap |                                                     None                                                    |
|         accelerator_config         | AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True) |
|             deepspeed              |                                                     None                                                    |
|       label_smoothing_factor       |                                                     0.0                                                     |
|               optim                |                                                 adamw_torch                                                 |
|             optim_args             |                                                     None                                                    |
|             adafactor              |                                                    False                                                    |
|          group_by_length           |                                                    False                                                    |
|         length_column_name         |                                                    length                                                   |
|             report_to              |                                                  ['wandb']                                                  |
|     ddp_find_unused_parameters     |                                                     None                                                    |
|         ddp_bucket_cap_mb          |                                                     None                                                    |
|       ddp_broadcast_buffers        |                                                     None                                                    |
|       dataloader_pin_memory        |                                                     True                                                    |
|   dataloader_persistent_workers    |                                                    False                                                    |
|        skip_memory_metrics         |                                                     True                                                    |
|     use_legacy_prediction_loop     |                                                    False                                                    |
|            push_to_hub             |                                                    False                                                    |
|       resume_from_checkpoint       |                                                     None                                                    |
|            hub_model_id            |                                                     None                                                    |
|            hub_strategy            |                                                  every_save                                                 |
|             hub_token              |                                                     None                                                    |
|          hub_private_repo          |                                                    False                                                    |
|          hub_always_push           |                                                    False                                                    |
|       gradient_checkpointing       |                                                    False                                                    |
|   gradient_checkpointing_kwargs    |                                                     None                                                    |
|     include_inputs_for_metrics     |                                                    False                                                    |
|            fp16_backend            |                                                     auto                                                    |
|        push_to_hub_model_id        |                                                     None                                                    |
|      push_to_hub_organization      |                                                     None                                                    |
|         push_to_hub_token          |                                                     None                                                    |
|           mp_parameters            |                                                                                                             |
|        auto_find_batch_size        |                                                    False                                                    |
|          full_determinism          |                                                    False                                                    |
|            torchdynamo             |                                                     None                                                    |
|             ray_scope              |                                                     last                                                    |
|            ddp_timeout             |                                                     1800                                                    |
|           torch_compile            |                                                    False                                                    |
|       torch_compile_backend        |                                                     None                                                    |
|         torch_compile_mode         |                                                     None                                                    |
|          dispatch_batches          |                                                     None                                                    |
|           split_batches            |                                                     None                                                    |
|     include_tokens_per_second      |                                                    False                                                    |
|   include_num_input_tokens_seen    |                                                    False                                                    |
|        neftune_noise_alpha         |                                                     None                                                    |
|         distributed_state          |                                 Distributed environment: DistributedType.NO                                 |
|                                    |                                               Num processes: 1                                              |
|                                    |                                               Process index: 0                                              |
|                                    |                                            Local process index: 0                                           |
|                                    |                                                 Device: cuda                                                |
|                                    |                                                                                                             |
|               _n_gpu               |                                                      1                                                      |
|      __cached__setup_devices       |                                                    cuda:0                                                   |
|          deepspeed_plugin          |                                                     None                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+

______________________________________________________________________________________________________________________________________________________
Training for 1-1
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_1_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_1_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 177455
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47360
}) 
 Took0.6084494829177857 minutes
#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-
Error occured while training the model
Artifact name may only contain alphanumeric characters, dashes, underscores, and dots. Invalid name: checkpoint-run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-07-17_test=False 
 Took177.17602943579357 minutes
Starting new HTTPS connection (1): o151352.ingest.sentry.io:443
https://o151352.ingest.sentry.io:443 "POST /api/4504800232407040/envelope/ HTTP/1.1" 200 0


______________________________________________________________________________________________________________________________________________________
+++++++++++++++++++++++++++++++++++++ NEW RUN ++++++++++++++++++++++++++++++++++++++++

test:False,  layer:7,  float16:False,  adafactor:False,  vocab:30000,  batch_size:32,  seq_len:2048,  start_year_index:2,  max_grad_norm:0.9,  start_month_index:1,  device:0,  checkpoint:None,  enb_grad_checkpoint:False,  local_model_path:/home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+-------------------------+-------+
|        Parameter        | Value |
+-------------------------+-------+
|   MODEL EMBEDDING DIM   |  4096 |
|        VOCABULARY       | 30000 |
|       PER HEAD DIM      |  128  |
| FEED-FORWARD HIDDEN DIM | 14336 |
|     NUMBER OF LAYERS    |   7   |
|     NUMBER OF Q-HEAD    |   32  |
|    NUMBER OF KV-HEAD    |   8   |
|       WINDOW SIZE       |  4096 |
|        use_cache        |  True |
+-------------------------+-------+

+-------------------------+--------+
|      HyperParameter     | Value  |
+-------------------------+--------+
|          EPOCHS         |   1    |
|      LEARNING RATE      | 2e-05  |
|       WEIGHT DECAY      |  0.1   |
|       WARMUP STEPS      |  100   |
|    LR SCHEDULER TYPE    | cosine |
|     TRAIN BATCH SIZE    |   32   |
|     EVAL BATCH SIZE     |   1    |
|   TOKENIZER BATCH SIZE  |   16   |
| EVAL ACCUMULATION STEPS |   32   |
|        EVAL STEPS       |  6000  |
|        SAVE STEPS       |  2000  |
|      LOGGING STEPS      |  500   |
|     SAVE TOTAL LIMIT    |   3    |
|      MAX SEQ LENGTH     |  2048  |
|        EVAL FRAC        |  0.1   |
+-------------------------+--------+
MODEL PATH: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096
Tokenizer loaded__________________________________________________________
____________________________________________________________________________________________________
Loading the model

+------------------------------------------------+------------+
|                    Modules                     | Parameters |
+------------------------------------------------+------------+
|           model.embed_tokens.weight            | 122880000  |
|     model.layers.0.self_attn.q_proj.weight     |  16777216  |
|     model.layers.0.self_attn.k_proj.weight     |  4194304   |
|     model.layers.0.self_attn.v_proj.weight     |  4194304   |
|     model.layers.0.self_attn.o_proj.weight     |  16777216  |
|      model.layers.0.mlp.gate_proj.weight       |  58720256  |
|       model.layers.0.mlp.up_proj.weight        |  58720256  |
|      model.layers.0.mlp.down_proj.weight       |  58720256  |
|     model.layers.0.input_layernorm.weight      |    4096    |
| model.layers.0.post_attention_layernorm.weight |    4096    |
|     model.layers.1.self_attn.q_proj.weight     |  16777216  |
|     model.layers.1.self_attn.k_proj.weight     |  4194304   |
|     model.layers.1.self_attn.v_proj.weight     |  4194304   |
|     model.layers.1.self_attn.o_proj.weight     |  16777216  |
|      model.layers.1.mlp.gate_proj.weight       |  58720256  |
|       model.layers.1.mlp.up_proj.weight        |  58720256  |
|      model.layers.1.mlp.down_proj.weight       |  58720256  |
|     model.layers.1.input_layernorm.weight      |    4096    |
| model.layers.1.post_attention_layernorm.weight |    4096    |
|     model.layers.2.self_attn.q_proj.weight     |  16777216  |
|     model.layers.2.self_attn.k_proj.weight     |  4194304   |
|     model.layers.2.self_attn.v_proj.weight     |  4194304   |
|     model.layers.2.self_attn.o_proj.weight     |  16777216  |
|      model.layers.2.mlp.gate_proj.weight       |  58720256  |
|       model.layers.2.mlp.up_proj.weight        |  58720256  |
|      model.layers.2.mlp.down_proj.weight       |  58720256  |
|     model.layers.2.input_layernorm.weight      |    4096    |
| model.layers.2.post_attention_layernorm.weight |    4096    |
|     model.layers.3.self_attn.q_proj.weight     |  16777216  |
|     model.layers.3.self_attn.k_proj.weight     |  4194304   |
|     model.layers.3.self_attn.v_proj.weight     |  4194304   |
|     model.layers.3.self_attn.o_proj.weight     |  16777216  |
|      model.layers.3.mlp.gate_proj.weight       |  58720256  |
|       model.layers.3.mlp.up_proj.weight        |  58720256  |
|      model.layers.3.mlp.down_proj.weight       |  58720256  |
|     model.layers.3.input_layernorm.weight      |    4096    |
| model.layers.3.post_attention_layernorm.weight |    4096    |
|     model.layers.4.self_attn.q_proj.weight     |  16777216  |
|     model.layers.4.self_attn.k_proj.weight     |  4194304   |
|     model.layers.4.self_attn.v_proj.weight     |  4194304   |
|     model.layers.4.self_attn.o_proj.weight     |  16777216  |
|      model.layers.4.mlp.gate_proj.weight       |  58720256  |
|       model.layers.4.mlp.up_proj.weight        |  58720256  |
|      model.layers.4.mlp.down_proj.weight       |  58720256  |
|     model.layers.4.input_layernorm.weight      |    4096    |
| model.layers.4.post_attention_layernorm.weight |    4096    |
|     model.layers.5.self_attn.q_proj.weight     |  16777216  |
|     model.layers.5.self_attn.k_proj.weight     |  4194304   |
|     model.layers.5.self_attn.v_proj.weight     |  4194304   |
|     model.layers.5.self_attn.o_proj.weight     |  16777216  |
|      model.layers.5.mlp.gate_proj.weight       |  58720256  |
|       model.layers.5.mlp.up_proj.weight        |  58720256  |
|      model.layers.5.mlp.down_proj.weight       |  58720256  |
|     model.layers.5.input_layernorm.weight      |    4096    |
| model.layers.5.post_attention_layernorm.weight |    4096    |
|     model.layers.6.self_attn.q_proj.weight     |  16777216  |
|     model.layers.6.self_attn.k_proj.weight     |  4194304   |
|     model.layers.6.self_attn.v_proj.weight     |  4194304   |
|     model.layers.6.self_attn.o_proj.weight     |  16777216  |
|      model.layers.6.mlp.gate_proj.weight       |  58720256  |
|       model.layers.6.mlp.up_proj.weight        |  58720256  |
|      model.layers.6.mlp.down_proj.weight       |  58720256  |
|     model.layers.6.input_layernorm.weight      |    4096    |
| model.layers.6.post_attention_layernorm.weight |    4096    |
|               model.norm.weight                |    4096    |
|                 lm_head.weight                 | 122880000  |
+------------------------------------------------+------------+
MISTRAL model size: 1772.5M parameters
Total Trainable Params: 1772.5481M
Total Trainable Params in one layer: 218.1120M
Original Model type:torch.float32
GPU memory usage before cleaning cache: 440.88 MB
GPU memory usage after cleaning cache: 440.88 MB
GPU memory occupied from nvmlInit: 1061 MB.
MODEL LOADED took __________ 0.0465574582417806 minutes_________________
______________________________________________________________________________________________________________________________________________________
MODEL SAVED AT: /home/iitgn_cse/latex_model/model_main_fp32_2024-04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096

+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|         Training Argument          |                                                    Value                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+
|             output_dir             |                              /home/iitgn_cse/latex_model/model_main_fp32_2024-                              |
|                                    |                                 04-10/latex/main_fp32_2024-04-10_ep_1_lr_2e-                                |
|                                    |                              05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30                             |
|                                    |                              000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4                             |
|                                    |                                                     096                                                     |
|        overwrite_output_dir        |                                                     True                                                    |
|              do_train              |                                                     True                                                    |
|              do_eval               |                                                     True                                                    |
|             do_predict             |                                                    False                                                    |
|        evaluation_strategy         |                                                    steps                                                    |
|        prediction_loss_only        |                                                    False                                                    |
|    per_device_train_batch_size     |                                                      1                                                      |
|     per_device_eval_batch_size     |                                                      1                                                      |
|      per_gpu_train_batch_size      |                                                     None                                                    |
|      per_gpu_eval_batch_size       |                                                     None                                                    |
|    gradient_accumulation_steps     |                                                      32                                                     |
|      eval_accumulation_steps       |                                                      32                                                     |
|             eval_delay             |                                                      0                                                      |
|           learning_rate            |                                                    2e-05                                                    |
|            weight_decay            |                                                     0.1                                                     |
|             adam_beta1             |                                                     0.9                                                     |
|             adam_beta2             |                                                    0.999                                                    |
|            adam_epsilon            |                                                    1e-08                                                    |
|           max_grad_norm            |                                                     0.9                                                     |
|          num_train_epochs          |                                                      1                                                      |
|             max_steps              |                                                      -1                                                     |
|         lr_scheduler_type          |                                                    cosine                                                   |
|        lr_scheduler_kwargs         |                                                      {}                                                     |
|            warmup_ratio            |                                                     0.0                                                     |
|            warmup_steps            |                                                     100                                                     |
|             log_level              |                                                    debug                                                    |
|         log_level_replica          |                                                   warning                                                   |
|          log_on_each_node          |                                                     True                                                    |
|            logging_dir             |                                                    ./logs                                                   |
|          logging_strategy          |                                                    steps                                                    |
|         logging_first_step         |                                                    False                                                    |
|           logging_steps            |                                                     500                                                     |
|       logging_nan_inf_filter       |                                                     True                                                    |
|           save_strategy            |                                                    steps                                                    |
|             save_steps             |                                                     2000                                                    |
|          save_total_limit          |                                                      3                                                      |
|          save_safetensors          |                                                     True                                                    |
|         save_on_each_node          |                                                    False                                                    |
|          save_only_model           |                                                    False                                                    |
|              no_cuda               |                                                    False                                                    |
|              use_cpu               |                                                    False                                                    |
|           use_mps_device           |                                                    False                                                    |
|                seed                |                                                      42                                                     |
|             data_seed              |                                                     None                                                    |
|           jit_mode_eval            |                                                    False                                                    |
|              use_ipex              |                                                    False                                                    |
|                bf16                |                                                    False                                                    |
|                fp16                |                                                    False                                                    |
|           fp16_opt_level           |                                                      O1                                                     |
|       half_precision_backend       |                                                     auto                                                    |
|           bf16_full_eval           |                                                    False                                                    |
|           fp16_full_eval           |                                                    False                                                    |
|                tf32                |                                                     None                                                    |
|             local_rank             |                                                      0                                                      |
|            ddp_backend             |                                                     None                                                    |
|           tpu_num_cores            |                                                     None                                                    |
|         tpu_metrics_debug          |                                                    False                                                    |
|               debug                |                                                      []                                                     |
|        dataloader_drop_last        |                                                    False                                                    |
|             eval_steps             |                                                     6000                                                    |
|       dataloader_num_workers       |                                                      0                                                      |
|     dataloader_prefetch_factor     |                                                     None                                                    |
|             past_index             |                                                      -1                                                     |
|              run_name              |                              run_latex_fp32_7_2048_32_0.9_30000_2024-04-23_17-                              |
|                                    |                                               07-17_test_False                                              |
|            disable_tqdm            |                                                    False                                                    |
|       remove_unused_columns        |                                                     True                                                    |
|            label_names             |                                                     None                                                    |
|       load_best_model_at_end       |                                                    False                                                    |
|       metric_for_best_model        |                                                     None                                                    |
|         greater_is_better          |                                                     None                                                    |
|          ignore_data_skip          |                                                    False                                                    |
|                fsdp                |                                                      []                                                     |
|        fsdp_min_num_params         |                                                      0                                                      |
|            fsdp_config             |            {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}           |
| fsdp_transformer_layer_cls_to_wrap |                                                     None                                                    |
|         accelerator_config         | AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True) |
|             deepspeed              |                                                     None                                                    |
|       label_smoothing_factor       |                                                     0.0                                                     |
|               optim                |                                                 adamw_torch                                                 |
|             optim_args             |                                                     None                                                    |
|             adafactor              |                                                    False                                                    |
|          group_by_length           |                                                    False                                                    |
|         length_column_name         |                                                    length                                                   |
|             report_to              |                                                  ['wandb']                                                  |
|     ddp_find_unused_parameters     |                                                     None                                                    |
|         ddp_bucket_cap_mb          |                                                     None                                                    |
|       ddp_broadcast_buffers        |                                                     None                                                    |
|       dataloader_pin_memory        |                                                     True                                                    |
|   dataloader_persistent_workers    |                                                    False                                                    |
|        skip_memory_metrics         |                                                     True                                                    |
|     use_legacy_prediction_loop     |                                                    False                                                    |
|            push_to_hub             |                                                    False                                                    |
|       resume_from_checkpoint       |                                                     None                                                    |
|            hub_model_id            |                                                     None                                                    |
|            hub_strategy            |                                                  every_save                                                 |
|             hub_token              |                                                     None                                                    |
|          hub_private_repo          |                                                    False                                                    |
|          hub_always_push           |                                                    False                                                    |
|       gradient_checkpointing       |                                                    False                                                    |
|   gradient_checkpointing_kwargs    |                                                     None                                                    |
|     include_inputs_for_metrics     |                                                    False                                                    |
|            fp16_backend            |                                                     auto                                                    |
|        push_to_hub_model_id        |                                                     None                                                    |
|      push_to_hub_organization      |                                                     None                                                    |
|         push_to_hub_token          |                                                     None                                                    |
|           mp_parameters            |                                                                                                             |
|        auto_find_batch_size        |                                                    False                                                    |
|          full_determinism          |                                                    False                                                    |
|            torchdynamo             |                                                     None                                                    |
|             ray_scope              |                                                     last                                                    |
|            ddp_timeout             |                                                     1800                                                    |
|           torch_compile            |                                                    False                                                    |
|       torch_compile_backend        |                                                     None                                                    |
|         torch_compile_mode         |                                                     None                                                    |
|          dispatch_batches          |                                                     None                                                    |
|           split_batches            |                                                     None                                                    |
|     include_tokens_per_second      |                                                    False                                                    |
|   include_num_input_tokens_seen    |                                                    False                                                    |
|        neftune_noise_alpha         |                                                     None                                                    |
|         distributed_state          |                                 Distributed environment: DistributedType.NO                                 |
|                                    |                                               Num processes: 1                                              |
|                                    |                                               Process index: 0                                              |
|                                    |                                            Local process index: 0                                           |
|                                    |                                                 Device: cuda                                                |
|                                    |                                                                                                             |
|               _n_gpu               |                                                      1                                                      |
|      __cached__setup_devices       |                                                    cuda:0                                                   |
|          deepspeed_plugin          |                                                     None                                                    |
+------------------------------------+-------------------------------------------------------------------------------------------------------------+

______________________________________________________________________________________________________________________________________________________
Training for 1-1
______________________________________________________________________________________________________________________________________________________

val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_1_1_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_1_1_datasets.pkl
trainer.train_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 177455
})
trainer.eval_dataset: Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 47360
}) 
 Took0.5635519067446391 minutes
