









 33%|██████████████████████████████████████████████████████████████████████████                                                                                                                                                    | 10/30 [06:49<13:35, 40.76s/it]
  0%|                                                                                                                                                                                                                                       | 0/10 [00:00<?, ?it/s]


 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                         | 6/10 [00:02<00:01,  2.44it/s]









 63%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 19/30 [13:26<07:42, 42.04s/it]
 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                          | 20/30 [14:09<07:03, 42.38s/it]

 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                   | 7/10 [00:02<00:01,  2.37it/s]










100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [21:26<00:00, 40.45s/it]
  0%|                                                                                                                                                                                                                                       | 0/10 [00:00<?, ?it/s]


 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                               | 5/10 [00:01<00:01,  2.55it/s]
{'eval_loss': 10.579565048217773, 'eval_runtime': 4.2382, 'eval_samples_per_second': 2.359, 'eval_steps_per_second': 2.359, 'epoch': 1.0}
{'train_runtime': 1318.298, 'train_samples_per_second': 0.728, 'train_steps_per_second': 0.023, 'train_loss': 10.878241221110025, 'epoch': 1.0}
***** Train results ***** TrainOutput(global_step=30, training_loss=10.878241221110025, metrics={'train_runtime': 1318.298, 'train_samples_per_second': 0.728, 'train_steps_per_second': 0.023, 'train_loss': 10.878241221110025, 'epoch': 1.0})
+----------------------------------------------------------------------------------+
GPU memory usage before cleaning cache: 39924.81 MB
GPU memory usage after cleaning cache: 23124.81 MB
GPU memory occupied from nvmlInit: 23745 MB.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [21:30<00:00, 40.45s/it]Could not locate the best model at /home/iitgn_cse/latex_model/model_exp_fp322024-04-10/latex/exp_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096/checkpoint-30/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [21:55<00:00, 43.84s/it]

 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                            | 8/10 [00:02<00:00,  2.83it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.67it/s]
Model saved
+----------------------------------------------------------------------------------+
GPU memory usage before cleaning cache: 23934.81 MB
GPU memory usage after cleaning cache: 23124.81 MB
GPU memory occupied from nvmlInit: 23745 MB.
+----------------------------------------------------------------------------------+
------------------------------------------------------------------------------------------
Training for 0-2
------------------------------------------------------------------------------------------
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_2_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_2_datasets.pkl
type <class 'datasets.arrow_dataset.Dataset'>
  0%|                                                                                                                                                                                                                                       | 0/30 [00:00<?, ?it/s]
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10









 33%|██████████████████████████████████████████████████████████████████████████                                                                                                                                                    | 10/30 [06:49<13:36, 40.84s/it]
 20%|████████████████████████████████████████████▌                                                                                                                                                                                  | 2/10 [00:00<00:00, 11.17it/s]


 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                   | 7/10 [00:02<00:01,  2.53it/s]
{'eval_loss': 10.532491683959961, 'eval_runtime': 4.1961, 'eval_samples_per_second': 2.383, 'eval_steps_per_second': 2.383, 'epoch': 0.33}








 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                          | 20/30 [16:35<07:16, 43.64s/it]
 20%|████████████████████████████████████████████▌                                                                                                                                                                                  | 2/10 [00:00<00:00, 11.26it/s]


 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                         | 6/10 [00:01<00:01,  2.68it/s]
{'eval_loss': 10.343780517578125, 'eval_runtime': 4.1991, 'eval_samples_per_second': 2.381, 'eval_steps_per_second': 2.381, 'epoch': 0.67}








100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [26:10<00:00, 43.52s/it]
 20%|████████████████████████████████████████████▌                                                                                                                                                                                  | 2/10 [00:00<00:00, 11.22it/s]


 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                   | 7/10 [00:02<00:01,  2.53it/s]
{'eval_loss': 9.990306854248047, 'eval_runtime': 4.2041, 'eval_samples_per_second': 2.379, 'eval_steps_per_second': 2.379, 'epoch': 1.0}
Could not locate the best model at /home/iitgn_cse/latex_model/model_exp_fp322024-04-10/latex/exp_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096/checkpoint-30/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [29:00<00:00, 58.03s/it]
{'train_runtime': 1740.9566, 'train_samples_per_second': 0.551, 'train_steps_per_second': 0.017, 'train_loss': 9.932413736979166, 'epoch': 1.0}
***** Train results ***** TrainOutput(global_step=30, training_loss=9.932413736979166, metrics={'train_runtime': 1740.9566, 'train_samples_per_second': 0.551, 'train_steps_per_second': 0.017, 'train_loss': 9.932413736979166, 'epoch': 1.0})
+----------------------------------------------------------------------------------+
GPU memory usage before cleaning cache: 38392.81 MB
GPU memory usage after cleaning cache: 23124.81 MB
GPU memory occupied from nvmlInit: 23745 MB.
+----------------------------------------------------------------------------------+

 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 9/10 [00:03<00:00,  2.41it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.73it/s]
Model saved
+----------------------------------------------------------------------------------+
GPU memory usage before cleaning cache: 23934.81 MB
GPU memory usage after cleaning cache: 23124.81 MB
GPU memory occupied from nvmlInit: 23745 MB.
+----------------------------------------------------------------------------------+
------------------------------------------------------------------------------------------
Training for 0-3
------------------------------------------------------------------------------------------
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_3_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_3_datasets.pkl
type <class 'datasets.arrow_dataset.Dataset'>
  0%|                                                                                                                                                                                                                                       | 0/30 [00:00<?, ?it/s]
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10









 33%|██████████████████████████████████████████████████████████████████████████                                                                                                                                                    | 10/30 [06:53<13:56, 41.84s/it]
 30%|██████████████████████████████████████████████████████████████████▉                                                                                                                                                            | 3/10 [00:00<00:02,  3.14it/s]


 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                   | 7/10 [00:02<00:01,  2.37it/s]
{'eval_loss': 9.089744567871094, 'eval_runtime': 4.4632, 'eval_samples_per_second': 2.241, 'eval_steps_per_second': 2.241, 'epoch': 0.33}








 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                          | 20/30 [16:36<07:13, 43.37s/it]
 20%|████████████████████████████████████████████▌                                                                                                                                                                                  | 2/10 [00:00<00:01,  4.39it/s]


 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                   | 7/10 [00:02<00:01,  2.37it/s]
{'eval_loss': 8.781901359558105, 'eval_runtime': 4.4603, 'eval_samples_per_second': 2.242, 'eval_steps_per_second': 2.242, 'epoch': 0.67}








100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [26:12<00:00, 42.64s/it]
 20%|████████████████████████████████████████████▌                                                                                                                                                                                  | 2/10 [00:00<00:01,  4.39it/s]


 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                   | 7/10 [00:02<00:01,  2.37it/s]
{'eval_loss': 8.366008758544922, 'eval_runtime': 4.4646, 'eval_samples_per_second': 2.24, 'eval_steps_per_second': 2.24, 'epoch': 1.0}
Could not locate the best model at /home/iitgn_cse/latex_model/model_exp_fp322024-04-10/latex/exp_fp32_2024-04-10_ep_1_lr_2e-05_cosine_wt_decay_0.1_warmup_st_100_emb_4096_V_30000_Dhead_128_FF_14336_L_7_N_Head_32_KV_Head_8_W_4096/checkpoint-30/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [29:00<00:00, 58.03s/it]
{'train_runtime': 1740.8557, 'train_samples_per_second': 0.551, 'train_steps_per_second': 0.017, 'train_loss': 9.349660491943359, 'epoch': 1.0}
***** Train results ***** TrainOutput(global_step=30, training_loss=9.349660491943359, metrics={'train_runtime': 1740.8557, 'train_samples_per_second': 0.551, 'train_steps_per_second': 0.017, 'train_loss': 9.349660491943359, 'epoch': 1.0})
+----------------------------------------------------------------------------------+
GPU memory usage before cleaning cache: 38390.81 MB
GPU memory usage after cleaning cache: 23124.81 MB
GPU memory occupied from nvmlInit: 23745 MB.
+----------------------------------------------------------------------------------+

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.55it/s]
***** Eval results ***** {'eval_loss': 8.366008758544922, 'eval_runtime': 4.4907, 'eval_samples_per_second': 2.227, 'eval_steps_per_second': 2.227, 'epoch': 1.0}
Model saved
+----------------------------------------------------------------------------------+
GPU memory usage before cleaning cache: 23934.81 MB
GPU memory usage after cleaning cache: 23124.81 MB
GPU memory occupied from nvmlInit: 23745 MB.
+----------------------------------------------------------------------------------+
------------------------------------------------------------------------------------------
Training for 0-4
------------------------------------------------------------------------------------------
val_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/val_0_4_datasets.pkl
train_local_pickel_path: /home/iitgn_cse/siddhesh_tokenize_data_9-4-24/DATA_TKNZD_10-4-24/train_0_4_datasets.pkl
type <class 'datasets.arrow_dataset.Dataset'>
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 960
})
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'num_chunks'],
    num_rows: 10
})









 30%|██████████████████████████████████████████████████████████████████▉                                                                                                                                                            | 9/30 [06:10<14:28, 41.37s/it]
 33%|██████████████████████████████████████████████████████████████████████████                                                                                                                                                    | 10/30 [06:49<13:36, 40.81s/it]

 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                         | 6/10 [00:02<00:01,  2.44it/s]

{'eval_loss': 8.545217514038086, 'eval_runtime': 4.4468, 'eval_samples_per_second': 2.249, 'eval_steps_per_second': 2.249, 'epoch': 0.33}








 63%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 19/30 [16:08<08:15, 45.00s/it]
 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                          | 20/30 [16:48<07:16, 43.69s/it]


 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 9/10 [00:03<00:00,  2.31it/s]

{'eval_loss': 8.386857986450195, 'eval_runtime': 4.4423, 'eval_samples_per_second': 2.251, 'eval_steps_per_second': 2.251, 'epoch': 0.67}




 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                     | 25/30 [23:05<04:30, 54.10s/it]Traceback (most recent call last):
  File "/home/iitgn_cse/latex_model/script/v3_training.py", line 327, in <module>
    train_result = trainer.train()
                   ^^^^^^^^^^^^^^^
  File "/home/iitgn_cse/miniconda3/envs/env_mist/lib/python3.11/site-packages/transformers/trainer.py", line 1624, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/iitgn_cse/miniconda3/envs/env_mist/lib/python3.11/site-packages/transformers/trainer.py", line 1961, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/iitgn_cse/miniconda3/envs/env_mist/lib/python3.11/site-packages/transformers/trainer.py", line 2911, in training_step
    self.accelerator.backward(loss)
  File "/home/iitgn_cse/miniconda3/envs/env_mist/lib/python3.11/site-packages/accelerate/accelerator.py", line 1966, in backward
    loss.backward(**kwargs)
  File "/home/iitgn_cse/miniconda3/envs/env_mist/lib/python3.11/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/iitgn_cse/miniconda3/envs/env_mist/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt