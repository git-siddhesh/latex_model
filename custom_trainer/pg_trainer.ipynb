{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trainer_env conda evn  \n",
    "set the path for the transformer library to the env\n",
    "\n",
    "`//home/dosisiddhesh/transformers/src/transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dosisiddhesh/latex_model/custom_trainer\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/dosisiddhesh/latex_model/custom_trainer', '/home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python312.zip', '/home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12', '/home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/lib-dynload', '', '/home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages', '/home/dosisiddhesh/transformers/src']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "# add the 'home/dosisiddhesh/transformers' path to the system path\n",
    "sys.path.append('/home/dosisiddhesh/transformers/src')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GenerationMixin' from 'transformers.generation' (/home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages/transformers/generation/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trainer\n",
      "File \u001b[0;32m~/miniconda3/envs/trainer_env/lib/python3.12/site-packages/transformers/trainer.py:65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtpu\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tpu_spmd_dataloader\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelcard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingSummary\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, load_sharded_checkpoint, unwrap_model\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     67\u001b[0m     MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n\u001b[1;32m     68\u001b[0m     MODEL_MAPPING_NAMES,\n\u001b[1;32m     69\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adafactor, get_scheduler\n",
      "File \u001b[0;32m~/miniconda3/envs/trainer_env/lib/python3.12/site-packages/transformers/modeling_utils.py:44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerationConfig, GenerationMixin\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     Conv1D,\n\u001b[1;32m     48\u001b[0m     apply_chunking_to_forward,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     prune_linear_layer,\n\u001b[1;32m     55\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'GenerationMixin' from 'transformers.generation' (/home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages/transformers/generation/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: fsspec in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.2.1-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
      "Successfully installed jinja2-3.1.3 nvidia-cudnn-cu12-8.9.2.26 nvidia-cusolver-cu12-11.4.5.107 torch-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub -q\n",
    "!pip install transformers -q\n",
    "!pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notebook version of run_clm_no_trainer.py to debug the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...)\n",
    "on a text file or a dataset without using HuggingFace Trainer.\n",
    "\n",
    "Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n",
    "https://huggingface.co/models?filter=text-generation\n",
    "\"\"\"\n",
    "# You can also adapt this script on your own causal language modeling task. Pointers for this are left as comments.\n",
    "\n",
    "''' To Run this file, use the following command:\n",
    "------------------------------------------------\n",
    "Original\n",
    "________________________________________________\n",
    "CUDA_VISIBLE_DEVICES=2 python run_clm_no_trainer.py \\\n",
    "    --dataset_name wikitext \\\n",
    "    --dataset_config_name wikitext-2-raw-v1 \\\n",
    "    --model_name_or_path openai-community/gpt2 \\\n",
    "    --output_dir /tmp/test-clm\n",
    "\n",
    "    \n",
    "\n",
    "----------------------------------------------\n",
    "Modified\n",
    "______________________________________________\n",
    "CUDA_VISIBLE_DEVICES=2 python run_clm_no_trainer.py \\\n",
    "    --dataset_name wikitext \\\n",
    "    --dataset_config_name wikitext-2-raw-v1 \\\n",
    "    --model_name_or_path custom_mistral \\\n",
    "    --output_dir /tmp/test-clm \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --block_size 128 \\\n",
    "    --float16\n",
    "\n",
    "------------------------------------------------\n",
    "With Gpt model architecture\n",
    "______________________________________________\n",
    "python run_clm_no_trainer.py \\\n",
    "    \n",
    "------------------------------------------------\n",
    "'''\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from accelerate import Accelerator, DistributedType\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import HfApi\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    MistralForCausalLM,\n",
    "    MistralConfig,\n",
    "    AutoTokenizer,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    ")\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "from pynvml import *\n",
    "def gpu_usage():\n",
    "    def print_gpu_utilization():\n",
    "        nvmlInit()\n",
    "        handle = nvmlDeviceGetHandleByIndex(0)\n",
    "        info = nvmlDeviceGetMemoryInfo(handle)\n",
    "        print(f\"GPU memory occupied from nvmlInit: {info.used//1024**2} MB.\")\n",
    "        logger.info(f\"GPU memory occupied from nvmlInit: {info.used//1024**2} MB.\")\n",
    "\n",
    "    print(\"+----------------------------------------------------------------------------------+\")\n",
    "    a,b = torch.cuda.mem_get_info()\n",
    "    gpu_mem_usage = (b-a)/(2**20)\n",
    "    print(f\"GPU memory usage before cleaning cache: {gpu_mem_usage:.2f} MB\")\n",
    "    logger.info(f\"GPU memory usage before cleaning cache: {gpu_mem_usage:.2f} MB\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    a,b = torch.cuda.mem_get_info()\n",
    "    gpu_mem_usage = (b-a)/(2**20)\n",
    "    print(f\"GPU memory usage after cleaning cache: {gpu_mem_usage:.2f} MB\")\n",
    "    logger.info(f\"GPU memory usage after cleaning cache: {gpu_mem_usage:.2f} MB\")\n",
    "    print_gpu_utilization()\n",
    "    print(\"+----------------------------------------------------------------------------------+\")\n",
    "\n",
    "class Parameter:\n",
    "    def __init__(self, name, value, use_cache=True):\n",
    "        self.name = name\n",
    "        self.D_emb,self.Vocal,self.d_head,self.d_FF,self.N_Layer,self.N_Head,self.KV_Head,self.Window = value\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "def model_size_and_parameters(model, logger = None):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    model_size = sum(t.numel() for t in model.parameters())\n",
    "    total_params = 0\n",
    "    one_layer_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "        if \"layers.0\" in name:\n",
    "            one_layer_params += params\n",
    "    print(table)\n",
    "    print(f\"MISTRAL model size: {model_size/1000**2:.1f}M parameters\")\n",
    "    print(f\"Total Trainable Params: {total_params/10**6:.4f}M\")\n",
    "    print(f\"Total Trainable Params in one layer: {one_layer_params/10**6:.4f}M\")\n",
    "    print(\"Original Model type:\",model.dtype)\n",
    "\n",
    "    if logger:\n",
    "        logger.info(table)\n",
    "        logger.info(f\"MISTRAL model size: {model_size/1000**2:.1f}M parameters\")\n",
    "        logger.info(f\"Total Trainable Params: {total_params/10**6:.4f}M\")\n",
    "        logger.info(f\"Total Trainable Params in one layer: {one_layer_params/10**6:.4f}M\")\n",
    "        logger.info(f\"Original Model type:{model.dtype}\")\n",
    "        \n",
    "    return total_params, one_layer_params\n",
    "\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.40.0.dev0\")\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/language-modeling/requirements.txt\")\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    --dataset_name wikitext \\\n",
    "    --dataset_config_name wikitext-2-raw-v1 \\\n",
    "    --model_name_or_path custom_mistral \\\n",
    "    --output_dir /tmp/test-clm \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --block_size 128 \\\n",
    "    --float16\n",
    "'''\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Finetune a transformers model on a causal language modeling task\")\n",
    "    parser.add_argument(\n",
    "        \"--dataset_name\",\n",
    "        type=str,\n",
    "        default='wikitext',\n",
    "        help=\"The name of the dataset to use (via the datasets library).\",\n",
    "    )\n",
    "    # add argument whether to load the model in float 16\n",
    "    parser.add_argument(\n",
    "        \"--float16\",\n",
    "        action=\"store_false\",\n",
    "        help=\"Whether or not to use float16 (mixed precision).\",\n",
    "    )        \n",
    "    parser.add_argument(\n",
    "        \"--dataset_config_name\",\n",
    "        type=str,\n",
    "        default='wikitext-2-raw-v1',\n",
    "        help=\"The configuration name of the dataset to use (via the datasets library).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_file\", type=str, default=None, help=\"A csv, txt or a json file containing the training data.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_file\", type=str, default=None, help=\"A csv, txt or a json file containing the validation data.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_split_percentage\",\n",
    "        default=5,\n",
    "        help=\"The percentage of the train set used as validation set in case there's no validation split\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        default='custom_mistral',\n",
    "        # default='openai-community/gpt2',\n",
    "        # default='facebook/galactica-125m',\n",
    "        type=str,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Pretrained config name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_slow_tokenizer\",\n",
    "        action=\"store_true\",\n",
    "        help=\"If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_device_train_batch_size\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Batch size (per device) for the training dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_device_eval_batch_size\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Batch size (per device) for the evaluation dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=5e-5,\n",
    "        help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    "    )\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"Weight decay to use.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=1, help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\n",
    "        \"--max_train_steps\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_scheduler_type\",\n",
    "        type=SchedulerType,\n",
    "        default=\"linear\",\n",
    "        help=\"The scheduler type to use.\",\n",
    "        choices=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_warmup_steps\", type=int, default=0, help=\"Number of steps for the warmup in the lr scheduler.\"\n",
    "    )\n",
    "    parser.add_argument(\"--output_dir\", type=str, default='./test-clm', help=\"Where to store the final model.\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n",
    "    parser.add_argument(\n",
    "        \"--model_type\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Model type to use if training from scratch.\",\n",
    "        choices=MODEL_TYPES,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--block_size\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=(\n",
    "            \"Optional input sequence length after tokenization. The training dataset will be truncated in block of\"\n",
    "            \" this size for training. Default to the model max input length for single sentence inputs (take into\"\n",
    "            \" account special tokens).\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--preprocessing_num_workers\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"The number of processes to use for the preprocessing.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--no_keep_linebreaks\", action=\"store_true\", help=\"Do not keep line breaks when using TXT files.\"\n",
    "    )\n",
    "    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the model to the Hub.\")\n",
    "    parser.add_argument(\n",
    "        \"--hub_model_id\", type=str, help=\"The name of the repository to keep in sync with the local `output_dir`.\"\n",
    "    )\n",
    "    parser.add_argument(\"--hub_token\", type=str, help=\"The token to use to push to the Model Hub.\")\n",
    "    parser.add_argument(\n",
    "        \"--trust_remote_code\",\n",
    "        type=bool,\n",
    "        default=False,\n",
    "        help=(\n",
    "            \"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option \"\n",
    "            \"should only be set to `True` for repositories you trust and in which you have read the code, as it will \"\n",
    "            \"execute code present on the Hub on your local machine.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpointing_steps\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_from_checkpoint\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"If the training should continue from a checkpoint folder.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--with_tracking\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to enable experiment trackers for logging.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--report_to\",\n",
    "        type=str,\n",
    "        default=\"all\",\n",
    "        help=(\n",
    "            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`,'\n",
    "            ' `\"wandb\"`, `\"comet_ml\"` and `\"clearml\"`. Use `\"all\"` (default) to report to all integrations. '\n",
    "            \"Only applicable when `--with_tracking` is passed.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--low_cpu_mem_usage\",\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. \"\n",
    "            \"If passed, LLM loading time and RAM consumption will be benefited.\"\n",
    "        ),\n",
    "    )\n",
    "    args = parser.parse_args('')\n",
    "\n",
    "    # Sanity checks\n",
    "    if args.dataset_name is None and args.train_file is None and args.validation_file is None:\n",
    "        raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "    else:\n",
    "        if args.train_file is not None:\n",
    "            extension = args.train_file.split(\".\")[-1]\n",
    "            if extension not in [\"csv\", \"json\", \"txt\"]:\n",
    "                raise ValueError(\"`train_file` should be a csv, json or txt file.\")\n",
    "        if args.validation_file is not None:\n",
    "            extension = args.validation_file.split(\".\")[-1]\n",
    "            if extension not in [\"csv\", \"json\", \"txt\"]:\n",
    "                raise ValueError(\"`validation_file` should be a csv, json or txt file.\")\n",
    "\n",
    "    if args.push_to_hub:\n",
    "        if args.output_dir is None:\n",
    "            raise ValueError(\"Need an `output_dir` to create a repo when `--push_to_hub` is passed.\")\n",
    "\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(dataset_name='wikitext', float16=True, dataset_config_name='wikitext-2-raw-v1', train_file=None, validation_file=None, validation_split_percentage=5, model_name_or_path='custom_mistral', config_name=None, tokenizer_name=None, use_slow_tokenizer=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=5e-05, weight_decay=0.0, num_train_epochs=1, max_train_steps=None, gradient_accumulation_steps=1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, num_warmup_steps=0, output_dir='./test-clm', seed=None, model_type=None, block_size=128, preprocessing_num_workers=None, overwrite_cache=False, no_keep_linebreaks=False, push_to_hub=False, hub_model_id=None, hub_token=None, trust_remote_code=False, checkpointing_steps=None, resume_from_checkpoint=None, with_tracking=False, report_to='all', low_cpu_mem_usage=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = parse_args()\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/01/2024 15:22:27 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw datasets:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 367\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 436\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 376\n",
      "    })\n",
      "})\n",
      "---------------------------------------------------\n",
      "--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n",
      "Loading local custom config\n",
      "CONFIG:  MistralConfig {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"relu\",\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 128,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 1,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.40.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n",
      "Loading local custom tokenizer\n",
      "Len of tokenizer before adding special tokens 30000\n",
      "Len of tokenizer after adding special tokens 30000\n",
      "--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\n",
      "Loading local custom model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/01/2024 15:23:05 - INFO - __main__ - +------------------------------------------------+------------+\n",
      "|                    Modules                     | Parameters |\n",
      "+------------------------------------------------+------------+\n",
      "|           model.embed_tokens.weight            |  3840000   |\n",
      "|     model.layers.0.self_attn.q_proj.weight     |   16384    |\n",
      "|     model.layers.0.self_attn.k_proj.weight     |    4096    |\n",
      "|     model.layers.0.self_attn.v_proj.weight     |    4096    |\n",
      "|     model.layers.0.self_attn.o_proj.weight     |   16384    |\n",
      "|      model.layers.0.mlp.gate_proj.weight       |   16384    |\n",
      "|       model.layers.0.mlp.up_proj.weight        |   16384    |\n",
      "|      model.layers.0.mlp.down_proj.weight       |   16384    |\n",
      "|     model.layers.0.input_layernorm.weight      |    128     |\n",
      "| model.layers.0.post_attention_layernorm.weight |    128     |\n",
      "|               model.norm.weight                |    128     |\n",
      "|                 lm_head.weight                 |  3840000   |\n",
      "+------------------------------------------------+------------+\n",
      "04/01/2024 15:23:05 - INFO - __main__ - MISTRAL model size: 7.8M parameters\n",
      "04/01/2024 15:23:05 - INFO - __main__ - Total Trainable Params: 7.7705M\n",
      "04/01/2024 15:23:05 - INFO - __main__ - Total Trainable Params in one layer: 0.0904M\n",
      "04/01/2024 15:23:05 - INFO - __main__ - Original Model type:torch.float16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in torch.float32 with config\n",
      "Model reduced to half precision in torch.float16\n",
      "+------------------------------------------------+------------+\n",
      "|                    Modules                     | Parameters |\n",
      "+------------------------------------------------+------------+\n",
      "|           model.embed_tokens.weight            |  3840000   |\n",
      "|     model.layers.0.self_attn.q_proj.weight     |   16384    |\n",
      "|     model.layers.0.self_attn.k_proj.weight     |    4096    |\n",
      "|     model.layers.0.self_attn.v_proj.weight     |    4096    |\n",
      "|     model.layers.0.self_attn.o_proj.weight     |   16384    |\n",
      "|      model.layers.0.mlp.gate_proj.weight       |   16384    |\n",
      "|       model.layers.0.mlp.up_proj.weight        |   16384    |\n",
      "|      model.layers.0.mlp.down_proj.weight       |   16384    |\n",
      "|     model.layers.0.input_layernorm.weight      |    128     |\n",
      "| model.layers.0.post_attention_layernorm.weight |    128     |\n",
      "|               model.norm.weight                |    128     |\n",
      "|                 lm_head.weight                 |  3840000   |\n",
      "+------------------------------------------------+------------+\n",
      "MISTRAL model size: 7.8M parameters\n",
      "Total Trainable Params: 7.7705M\n",
      "Total Trainable Params in one layer: 0.0904M\n",
      "Original Model type: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "send_example_telemetry(\"run_clm_no_trainer\", args)\n",
    "\n",
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "# If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n",
    "# in the environment\n",
    "accelerator_log_kwargs = {}\n",
    "\n",
    "if args.with_tracking:\n",
    "    accelerator_log_kwargs[\"log_with\"] = args.report_to\n",
    "    accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps, **accelerator_log_kwargs)\n",
    "\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if args.push_to_hub:\n",
    "        # Retrieve of infer repo_name\n",
    "        repo_name = args.hub_model_id\n",
    "        if repo_name is None:\n",
    "            repo_name = Path(args.output_dir).absolute().name\n",
    "        # Create repo and retrieve repo_id\n",
    "        api = HfApi()\n",
    "        repo_id = api.create_repo(repo_name, exist_ok=True, token=args.hub_token).repo_id\n",
    "\n",
    "        with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
    "            if \"step_*\" not in gitignore:\n",
    "                gitignore.write(\"step_*\\n\")\n",
    "            if \"epoch_*\" not in gitignore:\n",
    "                gitignore.write(\"epoch_*\\n\")\n",
    "    elif args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "# (the dataset will be downloaded automatically from the datasets Hub).\n",
    "#\n",
    "# For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
    "# 'text' is found. You can easily tweak this behavior (see below).\n",
    "#\n",
    "# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    # raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n",
    "    # Load 10% of each split using slicing\n",
    "    train_dataset = load_dataset(args.dataset_name, args.dataset_config_name, split='train[:1%]')\n",
    "    test_dataset = load_dataset(args.dataset_name, args.dataset_config_name, split='test[:10%]')\n",
    "    validation_dataset = load_dataset(args.dataset_name, args.dataset_config_name, split='validation[:10%]')\n",
    "\n",
    "    # Combine them into a DatasetDict\n",
    "    raw_datasets = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset,\n",
    "        'validation': validation_dataset\n",
    "    })\n",
    "    print(\"Raw datasets: \", raw_datasets)\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "    if \"validation\" not in raw_datasets.keys():\n",
    "        raw_datasets[\"validation\"] = load_dataset(\n",
    "            args.dataset_name,\n",
    "            args.dataset_config_name,\n",
    "            split=f\"train[:{args.validation_split_percentage}%]\",\n",
    "        )\n",
    "        raw_datasets[\"train\"] = load_dataset(\n",
    "            args.dataset_name,\n",
    "            args.dataset_config_name,\n",
    "            split=f\"train[{args.validation_split_percentage}%:]\",\n",
    "        )\n",
    "else:\n",
    "    data_files = {}\n",
    "    dataset_args = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "        extension = args.train_file.split(\".\")[-1]\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "        extension = args.validation_file.split(\".\")[-1]\n",
    "    if extension == \"txt\":\n",
    "        extension = \"text\"\n",
    "        dataset_args[\"keep_linebreaks\"] = not args.no_keep_linebreaks\n",
    "    raw_datasets = load_dataset(extension, data_files=data_files, **dataset_args)\n",
    "    # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n",
    "    if \"validation\" not in raw_datasets.keys():\n",
    "        raw_datasets[\"validation\"] = load_dataset(\n",
    "            extension,\n",
    "            data_files=data_files,\n",
    "            split=f\"train[:{args.validation_split_percentage}%]\",\n",
    "            **dataset_args,\n",
    "        )\n",
    "        raw_datasets[\"train\"] = load_dataset(\n",
    "            extension,\n",
    "            data_files=data_files,\n",
    "            split=f\"train[{args.validation_split_percentage}%:]\",\n",
    "            **dataset_args,\n",
    "        )\n",
    "\n",
    "# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "# https://huggingface.co/docs/datasets/loading_datasets.\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "        \n",
    "##################################################################################################\n",
    "if args.config_name:\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        args.config_name,\n",
    "        trust_remote_code=args.trust_remote_code,\n",
    "    )\n",
    "elif args.model_name_or_path == \"custom_mistral\":\n",
    "    print(\"--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\")\n",
    "    print(\"Loading local custom config\")\n",
    "    D_emb = 128 #4*1024\n",
    "    Vocal = 30000\n",
    "    d_head = 128# + 64\n",
    "    d_FF = 128 # 14336\n",
    "    N_Layer = 1\n",
    "    N_Head = 32\n",
    "    KV_Head = 8\n",
    "    Window = 4096 #8192\n",
    "    # data_row = 100\n",
    "    value = [D_emb,Vocal,d_head,d_FF,N_Layer,N_Head,KV_Head,Window]\n",
    "    param = Parameter('MISTRAL', value)\n",
    "    config = MistralConfig(\n",
    "            vocab_size=param.Vocal,\n",
    "            hidden_size=param.D_emb,\n",
    "            intermediate_size=param.d_FF,\n",
    "            num_hidden_layers=param.N_Layer,\n",
    "            num_attention_heads=param.N_Head,\n",
    "            num_key_value_heads=param.KV_Head,\n",
    "            hidden_act=\"relu\",#\"silu\",\n",
    "            max_position_embeddings=4096 * 32,\n",
    "            initializer_range=0.02,\n",
    "            rms_norm_eps=1e-6,\n",
    "            use_cache=param.use_cache,\n",
    "            # use_reentrant=False,\n",
    "            pad_token_id=None,\n",
    "            bos_token_id=1,\n",
    "            eos_token_id=2,\n",
    "            tie_word_embeddings=False,\n",
    "            rope_theta=10000.0,\n",
    "            sliding_window=param.Window,\n",
    "            attention_dropout=0.0,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    print(\"CONFIG: \", config)\n",
    "    \n",
    "\n",
    "elif args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        trust_remote_code=args.trust_remote_code,\n",
    "    )\n",
    "else:\n",
    "    config = CONFIG_MAPPING[args.model_type]()\n",
    "    logger.warning(\"You are instantiating a new config instance from scratch.\")\n",
    "        \n",
    "if args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.tokenizer_name, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n",
    "    )\n",
    "elif args.model_name_or_path == \"custom_mistral\":\n",
    "    print(\"--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\")\n",
    "    print(\"Loading local custom tokenizer\")\n",
    "    TOKENIZER_HF_ST_PATH = '/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_1.0%_30000_new'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_HF_ST_PATH)\n",
    "    print(\"Len of tokenizer before adding special tokens\", len(tokenizer))\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>',\n",
    "                                        'cls_token': '<cls>',\n",
    "                                        'sep_token': '<sep>',\n",
    "                                        'mask_token': '<mask>',\n",
    "                                        'unk_token': '<unk>',\n",
    "                                        'bos_token': '<bos>',\n",
    "                                        'eos_token': '<eos>'\n",
    "                                    })\n",
    "    # print the vocab of the tokenizer\n",
    "    # print(\"Vocab of the tokenizer: \", self.tokenizer.get_vocab())\n",
    "    print(\"Len of tokenizer after adding special tokens\", len(tokenizer))\n",
    "\n",
    "elif args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path, use_fast=not args.use_slow_tokenizer, trust_remote_code=args.trust_remote_code\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "if args.model_name_or_path == \"custom_mistral\":\n",
    "    print(\"--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+\")\n",
    "    print(\"Loading local custom model\")\n",
    "    model = MistralForCausalLM(config)\n",
    "\n",
    "\n",
    "elif args.model_name_or_path:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "        low_cpu_mem_usage=args.low_cpu_mem_usage,\n",
    "        trust_remote_code=args.trust_remote_code,\n",
    "    )\n",
    "\n",
    "\n",
    "else:\n",
    "    logger.info(\"Training new model from scratch\")\n",
    "    model = AutoModelForCausalLM.from_config(config, trust_remote_code=args.trust_remote_code)\n",
    "\n",
    "\n",
    "if args.float16:\n",
    "    print(f'Model loaded in {model.dtype} with config')\n",
    "    model = model.half()\n",
    "    print(f'Model reduced to half precision in {model.dtype}')\n",
    "\n",
    "model_size_and_parameters(model, logger)\n",
    "model.to(0)\n",
    "##################################################################################################\n",
    "\n",
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the model types and precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(30000, 128)\n",
       "    (layers): ModuleList(\n",
       "      (0): MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (k_proj): Linear(in_features=128, out_features=32, bias=False)\n",
       "          (v_proj): Linear(in_features=128, out_features=32, bias=False)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (up_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (down_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (act_fn): ReLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=128, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 30000,\n",
       " 'max_position_embeddings': 131072,\n",
       " 'hidden_size': 128,\n",
       " 'intermediate_size': 128,\n",
       " 'num_hidden_layers': 1,\n",
       " 'num_attention_heads': 32,\n",
       " 'sliding_window': 4096,\n",
       " 'num_key_value_heads': 8,\n",
       " 'hidden_act': 'relu',\n",
       " 'initializer_range': 0.02,\n",
       " 'rms_norm_eps': 1e-06,\n",
       " 'use_cache': True,\n",
       " 'rope_theta': 10000.0,\n",
       " 'attention_dropout': 0.0,\n",
       " 'return_dict': True,\n",
       " 'output_hidden_states': False,\n",
       " 'output_attentions': False,\n",
       " 'torchscript': False,\n",
       " 'torch_dtype': 'float16',\n",
       " 'use_bfloat16': False,\n",
       " 'tf_legacy_loss': False,\n",
       " 'pruned_heads': {},\n",
       " 'tie_word_embeddings': False,\n",
       " 'chunk_size_feed_forward': 0,\n",
       " 'is_encoder_decoder': False,\n",
       " 'is_decoder': False,\n",
       " 'cross_attention_hidden_size': None,\n",
       " 'add_cross_attention': False,\n",
       " 'tie_encoder_decoder': False,\n",
       " 'max_length': 20,\n",
       " 'min_length': 0,\n",
       " 'do_sample': False,\n",
       " 'early_stopping': False,\n",
       " 'num_beams': 1,\n",
       " 'num_beam_groups': 1,\n",
       " 'diversity_penalty': 0.0,\n",
       " 'temperature': 1.0,\n",
       " 'top_k': 50,\n",
       " 'top_p': 1.0,\n",
       " 'typical_p': 1.0,\n",
       " 'repetition_penalty': 1.0,\n",
       " 'length_penalty': 1.0,\n",
       " 'no_repeat_ngram_size': 0,\n",
       " 'encoder_no_repeat_ngram_size': 0,\n",
       " 'bad_words_ids': None,\n",
       " 'num_return_sequences': 1,\n",
       " 'output_scores': False,\n",
       " 'return_dict_in_generate': False,\n",
       " 'forced_bos_token_id': None,\n",
       " 'forced_eos_token_id': None,\n",
       " 'remove_invalid_values': False,\n",
       " 'exponential_decay_length_penalty': None,\n",
       " 'suppress_tokens': None,\n",
       " 'begin_suppress_tokens': None,\n",
       " 'architectures': None,\n",
       " 'finetuning_task': None,\n",
       " 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       " 'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       " 'tokenizer_class': None,\n",
       " 'prefix': None,\n",
       " 'bos_token_id': 1,\n",
       " 'pad_token_id': None,\n",
       " 'eos_token_id': 2,\n",
       " 'sep_token_id': None,\n",
       " 'decoder_start_token_id': None,\n",
       " 'task_specific_params': None,\n",
       " 'problem_type': None,\n",
       " '_name_or_path': '',\n",
       " 'transformers_version': '4.40.0.dev0',\n",
       " 'model_type': 'mistral'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = Valkyria Chronicles III = \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name], return_token_type_ids=False)\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_datasets['train']['input_ids'][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/01/2024 15:23:06 - INFO - __main__ - Sample 90 of the training set: {'input_ids': [2862, 12412, 1054, 1890, 47, 1430, 2633, 5778, 74, 3955, 1012, 11141, 1025, 14808, 2730, 1026, 497, 74, 9359, 1054, 16516, 1046, 47, 35, 9008, 1525, 1191, 7921, 1015, 14808, 1025, 1351, 5749, 1061, 1901, 1061, 72, 7624, 1168, 85, 1514, 122, 45, 1015, 13042, 1596, 40, 9755, 13071, 1347, 10141, 45, 4090, 6103, 1054, 1015, 19132, 13058, 13574, 122, 1008, 1158, 1603, 47, 1111, 2285, 1178, 2302, 1191, 7921, 1233, 6633, 45, 1027, 1308, 98, 2065, 1451, 1777, 1313, 10577, 1038, 2452, 45, 76, 1558, 4877, 5683, 24938, 2379, 1855, 2862, 1015, 2065, 25451, 5157, 47, 26668, 7169, 1027, 8611, 1015, 71, 2558, 13574, 122, 1052, 1847, 2177, 45, 1027, 1858, 1591, 1009, 1015, 2490, 1596, 1027, 9407, 1025, 1015, 2298, 19920, 1039, 1032, 7322, 47, 1111], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2862, 12412, 1054, 1890, 47, 1430, 2633, 5778, 74, 3955, 1012, 11141, 1025, 14808, 2730, 1026, 497, 74, 9359, 1054, 16516, 1046, 47, 35, 9008, 1525, 1191, 7921, 1015, 14808, 1025, 1351, 5749, 1061, 1901, 1061, 72, 7624, 1168, 85, 1514, 122, 45, 1015, 13042, 1596, 40, 9755, 13071, 1347, 10141, 45, 4090, 6103, 1054, 1015, 19132, 13058, 13574, 122, 1008, 1158, 1603, 47, 1111, 2285, 1178, 2302, 1191, 7921, 1233, 6633, 45, 1027, 1308, 98, 2065, 1451, 1777, 1313, 10577, 1038, 2452, 45, 76, 1558, 4877, 5683, 24938, 2379, 1855, 2862, 1015, 2065, 25451, 5157, 47, 26668, 7169, 1027, 8611, 1015, 71, 2558, 13574, 122, 1052, 1847, 2177, 45, 1027, 1858, 1591, 1009, 1015, 2490, 1596, 1027, 9407, 1025, 1015, 2298, 19920, 1039, 1032, 7322, 47, 1111]}.\n",
      "04/01/2024 15:23:06 - INFO - __main__ - Sample 141 of the training set: {'input_ids': [18576, 1777, 15154, 10833, 45, 1015, 17354, 9566, 3427, 14935, 1039, 16470, 9050, 47, 1156, 1015, 102, 16873, 4936, 1025, 1015, 1408, 45, 2330, 3955, 98, 15466, 1039, 1080, 1015, 25919, 3783, 1410, 5638, 1015, 12029, 1033, 1490, 6298, 2369, 47, 20814, 8947, 15139, 98, 52, 497, 51, 3000, 1120, 1628, 98, 22575, 1039, 2560, 47, 17819, 4306, 11598, 25139, 1015, 4936, 1009, 98, 4018, 2476, 2560, 45, 1027, 1120, 3545, 53, 65, 47, 65, 56, 11144, 5511, 45, 5894, 1012, 9967, 23631, 1313, 6603, 1015, 18125, 5381, 1054, 1015, 6298, 2369, 47, 1111, 3100, 26537, 1264, 98, 20517, 1042, 1723, 10957, 1054, 5984, 7175, 1120, 98, 49, 497, 56, 497, 50, 10957, 1039, 3852, 98, 1030, 9673, 47, 92, 2010, 50, 94, 1111, 25919, 3783, 1410, 24294], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [18576, 1777, 15154, 10833, 45, 1015, 17354, 9566, 3427, 14935, 1039, 16470, 9050, 47, 1156, 1015, 102, 16873, 4936, 1025, 1015, 1408, 45, 2330, 3955, 98, 15466, 1039, 1080, 1015, 25919, 3783, 1410, 5638, 1015, 12029, 1033, 1490, 6298, 2369, 47, 20814, 8947, 15139, 98, 52, 497, 51, 3000, 1120, 1628, 98, 22575, 1039, 2560, 47, 17819, 4306, 11598, 25139, 1015, 4936, 1009, 98, 4018, 2476, 2560, 45, 1027, 1120, 3545, 53, 65, 47, 65, 56, 11144, 5511, 45, 5894, 1012, 9967, 23631, 1313, 6603, 1015, 18125, 5381, 1054, 1015, 6298, 2369, 47, 1111, 3100, 26537, 1264, 98, 20517, 1042, 1723, 10957, 1054, 5984, 7175, 1120, 98, 49, 497, 56, 497, 50, 10957, 1039, 3852, 98, 1030, 9673, 47, 92, 2010, 50, 94, 1111, 25919, 3783, 1410, 24294]}.\n",
      "04/01/2024 15:23:06 - INFO - __main__ - Sample 45 of the training set: {'input_ids': [1095, 5095, 6338, 24294, 1233, 1015, 10943, 11134, 26586, 1027, 28803, 98, 1031, 25763, 1298, 1842, 16584, 103, 5439, 1024, 1213, 6848, 1025, 9890, 1027, 12529, 4245, 27796, 1054, 20149, 3558, 1613, 45, 5406, 2662, 1495, 2302, 8470, 6951, 1095, 2330, 2302, 4216, 5673, 47, 1111, 1714, 12529, 1346, 6721, 1039, 5129, 1633, 15214, 15726, 1027, 78, 1270, 1023, 3955, 5627, 1954, 12277, 47, 6928, 1038, 3048, 5704, 4168, 78, 3726, 4081, 1777, 18910, 1039, 24128, 1015, 1714, 12529, 1346, 1213, 6848, 1078, 1233, 15897, 1348, 1061, 40, 116, 21281, 47, 1156, 5095, 8462, 45, 3726, 4081, 5048, 1083, 8825, 2662, 1495, 1095, 1648, 13929, 1014, 17875, 25728, 3955, 1954, 6190, 1039, 20149, 3558, 1015, 15214, 15726, 2439, 1900, 1014, 47, 1111, 10943, 11134, 17875, 25728, 1038, 1015], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1095, 5095, 6338, 24294, 1233, 1015, 10943, 11134, 26586, 1027, 28803, 98, 1031, 25763, 1298, 1842, 16584, 103, 5439, 1024, 1213, 6848, 1025, 9890, 1027, 12529, 4245, 27796, 1054, 20149, 3558, 1613, 45, 5406, 2662, 1495, 2302, 8470, 6951, 1095, 2330, 2302, 4216, 5673, 47, 1111, 1714, 12529, 1346, 6721, 1039, 5129, 1633, 15214, 15726, 1027, 78, 1270, 1023, 3955, 5627, 1954, 12277, 47, 6928, 1038, 3048, 5704, 4168, 78, 3726, 4081, 1777, 18910, 1039, 24128, 1015, 1714, 12529, 1346, 1213, 6848, 1078, 1233, 15897, 1348, 1061, 40, 116, 21281, 47, 1156, 5095, 8462, 45, 3726, 4081, 5048, 1083, 8825, 2662, 1495, 1095, 1648, 13929, 1014, 17875, 25728, 3955, 1954, 6190, 1039, 20149, 3558, 1015, 15214, 15726, 2439, 1900, 1014, 47, 1111, 10943, 11134, 17875, 25728, 1038, 1015]}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- the block size is set to:  128\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if args.block_size is None:\n",
    "    block_size = tokenizer.model_max_length\n",
    "    if block_size > config.max_position_embeddings:\n",
    "        logger.warning(\n",
    "            f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "            f\"Using block_size={min(1024, config.max_position_embeddings)} instead. You can change that default value by passing --block_size xxx.\"\n",
    "        )\n",
    "        block_size = min(1024, config.max_position_embeddings)\n",
    "else:\n",
    "    if args.block_size > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The block_size passed ({args.block_size}) is larger than the maximum length for the model \"\n",
    "            f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    block_size = min(args.block_size, tokenizer.model_max_length)\n",
    "    print(\"-------------------- the block size is set to: \", block_size)\n",
    "\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n",
    "# for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n",
    "# to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/process#map\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "    )\n",
    "\n",
    "\n",
    "# def convert_to_fp16(batch):\n",
    "#     # Convert tensors that are not used as indices for embeddings to torch.float16\n",
    "#     # Assuming 'input_ids', 'attention_mask', etc. are the keys for tensors that should be converted\n",
    "#     float_keys = ['input_ids']  # Add or remove keys as per your data structure\n",
    "#     return {k: torch.tensor(v, dtype=torch.float16) if k in float_keys else torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n",
    "\n",
    "# # Your mapping function remains the same\n",
    "# lm_datasets = lm_datasets.map(\n",
    "#     convert_to_fp16,\n",
    "#     batched=True,\n",
    "#     num_proc=args.preprocessing_num_workers,\n",
    "#     load_from_cache_file=not args.overwrite_cache,\n",
    "#     desc=\"Converting appropriate tensors to float16\"\n",
    "# )\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "eval_dataset = lm_datasets[\"validation\"]\n",
    "\n",
    "\n",
    "# Log a few random samples from the training set:\n",
    "for index in random.sample(range(len(train_dataset)), 3):\n",
    "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys:  dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "input_ids: torch.int64\n",
      "attention_mask: torch.int64\n",
      "labels: torch.int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Use the default data collator to process the batch\n",
    "    batch = default_data_collator(batch)\n",
    "    \n",
    "    # Convert all tensors in the batch to torch.float16\n",
    "    # batch = {key: value.to(torch.float16) if isinstance(value, torch.Tensor) else value for key, value in batch.items()}\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "# DataLoaders creation:\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=custom_collate_fn, batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=custom_collate_fn, batch_size=args.per_device_eval_batch_size\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(\"Batch keys: \", batch.keys())\n",
    "    # print dtype of all tensors in the batch\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key}: {value.dtype}\")\n",
    "\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys:  dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "input_ids: torch.int64\n",
      "attention_mask: torch.int64\n",
      "labels: torch.int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"layer_norm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps * accelerator.num_processes,\n",
    "    num_training_steps=args.max_train_steps\n",
    "    if overrode_max_train_steps\n",
    "    else args.max_train_steps * accelerator.num_processes,\n",
    ")\n",
    "\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")\n",
    "\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(\"Batch keys: \", batch.keys())\n",
    "    # print dtype of all tensors in the batch\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key}: {value.dtype}\")\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dosisiddhesh/miniconda3/envs/trainer_env/lib/python3.12/site-packages/accelerate/utils/dataclasses.py:301: FutureWarning: The `TPU` of `<enum 'DistributedType'>` is deprecated and will be removed in v1.0.0. Please use the `XLA` instead.\n",
      "  warnings.warn(\n",
      "04/01/2024 15:23:07 - INFO - __main__ - ***** Running training *****\n",
      "04/01/2024 15:23:07 - INFO - __main__ -   Num examples = 165\n",
      "04/01/2024 15:23:07 - INFO - __main__ -   Num Epochs = 1\n",
      "04/01/2024 15:23:07 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "04/01/2024 15:23:07 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "04/01/2024 15:23:07 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "04/01/2024 15:23:07 - INFO - __main__ -   Total optimization steps = 165\n",
      "  0%|          | 0/165 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# On TPU, the tie weights in our model have been disconnected, so we need to restore the ties.\n",
    "if accelerator.distributed_type == DistributedType.TPU:\n",
    "    model.tie_weights()\n",
    "\n",
    "# We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if overrode_max_train_steps:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "# Figure out how many steps we should save the Accelerator states\n",
    "checkpointing_steps = args.checkpointing_steps\n",
    "if checkpointing_steps is not None and checkpointing_steps.isdigit():\n",
    "    checkpointing_steps = int(checkpointing_steps)\n",
    "\n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "# The trackers initializes automatically on the main process.\n",
    "if args.with_tracking:\n",
    "    experiment_config = vars(args)\n",
    "    # TensorBoard cannot log Enums, need the raw value\n",
    "    experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n",
    "    accelerator.init_trackers(\"clm_no_trainer\", experiment_config)\n",
    "\n",
    "# Train!\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if args.resume_from_checkpoint:\n",
    "    if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n",
    "        checkpoint_path = args.resume_from_checkpoint\n",
    "        path = os.path.basename(args.resume_from_checkpoint)\n",
    "    else:\n",
    "        # Get the most recent checkpoint\n",
    "        dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n",
    "        dirs.sort(key=os.path.getctime)\n",
    "        path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n",
    "        checkpoint_path = path\n",
    "        path = os.path.basename(checkpoint_path)\n",
    "\n",
    "    accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n",
    "    accelerator.load_state(checkpoint_path)\n",
    "    # Extract `epoch_{i}` or `step_{i}`\n",
    "    training_difference = os.path.splitext(path)[0]\n",
    "\n",
    "    if \"epoch\" in training_difference:\n",
    "        starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n",
    "        resume_step = None\n",
    "        completed_steps = starting_epoch * num_update_steps_per_epoch\n",
    "    else:\n",
    "        # need to multiply `gradient_accumulation_steps` to reflect real steps\n",
    "        resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n",
    "        starting_epoch = resume_step // len(train_dataloader)\n",
    "        completed_steps = resume_step // args.gradient_accumulation_steps\n",
    "        resume_step -= starting_epoch * len(train_dataloader)\n",
    "\n",
    "# update the progress_bar if load from checkpoint\n",
    "progress_bar.update(completed_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### debug section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------ + ------------- + ---------------------------- + ------------------------------\n",
      "name                                             | dtype         | shape                        | requires_grad                 \n",
      "------------------------------------------------ + ------------- + ---------------------------- + ------------------------------\n",
      "model.embed_tokens.weight                          torch.float16   torch.Size([30000, 128])       requires_grad                 \n",
      "model.layers.0.self_attn.q_proj.weight             torch.float16   torch.Size([128, 128])         requires_grad                 \n",
      "model.layers.0.self_attn.k_proj.weight             torch.float16   torch.Size([32, 128])          requires_grad                 \n",
      "model.layers.0.self_attn.v_proj.weight             torch.float16   torch.Size([32, 128])          requires_grad                 \n",
      "model.layers.0.self_attn.o_proj.weight             torch.float16   torch.Size([128, 128])         requires_grad                 \n",
      "model.layers.0.mlp.gate_proj.weight                torch.float16   torch.Size([128, 128])         requires_grad                 \n",
      "model.layers.0.mlp.up_proj.weight                  torch.float16   torch.Size([128, 128])         requires_grad                 \n",
      "model.layers.0.mlp.down_proj.weight                torch.float16   torch.Size([128, 128])         requires_grad                 \n",
      "model.layers.0.input_layernorm.weight              torch.float16   torch.Size([128])              requires_grad                 \n",
      "model.layers.0.post_attention_layernorm.weight     torch.float16   torch.Size([128])              requires_grad                 \n",
      "model.norm.weight                                  torch.float16   torch.Size([128])              requires_grad                 \n",
      "lm_head.weight                                     torch.float16   torch.Size([30000, 128])       requires_grad                 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"{'-'*48} + {'-'*13} + {'-'*28} + {'-'*30}\")\n",
    "print(f\"{'name':49}| {'dtype':14}| {'shape':29}| {'requires_grad':30}\")\n",
    "print(f\"{'-'*48} + {'-'*13} + {'-'*28} + {'-'*30}\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{str(name):50} {str(param.dtype):15} {str(param.shape):30} {('requires_grad' if param.requires_grad else 'non grad'):30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of model.embed_tokens.weight: None\n",
      "Gradient of model.layers.0.self_attn.q_proj.weight: None\n",
      "Gradient of model.layers.0.self_attn.k_proj.weight: None\n",
      "Gradient of model.layers.0.self_attn.v_proj.weight: None\n",
      "Gradient of model.layers.0.self_attn.o_proj.weight: None\n",
      "Gradient of model.layers.0.mlp.gate_proj.weight: None\n",
      "Gradient of model.layers.0.mlp.up_proj.weight: None\n",
      "Gradient of model.layers.0.mlp.down_proj.weight: None\n",
      "Gradient of model.layers.0.input_layernorm.weight: None\n",
      "Gradient of model.layers.0.post_attention_layernorm.weight: None\n",
      "Gradient of model.norm.weight: None\n",
      "Gradient of lm_head.weight: None\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Gradient of {name}: {param.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_nan(tensor, name=\"Tensor\"):\n",
    "\n",
    "    print(f\"{name:50} Gradient isnan: {torch.isnan(tensor).any()}\")\n",
    "    print(f\"{name:50} Gradient isinf: {torch.isinf(tensor).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/01/2024 15:23:08 - INFO - __main__ - GPU memory usage before cleaning cache: 3270.31 MB\n",
      "04/01/2024 15:23:08 - INFO - __main__ - GPU memory usage after cleaning cache: 3270.31 MB\n",
      "04/01/2024 15:23:08 - INFO - __main__ - GPU memory occupied from nvmlInit: 32503 MB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+\n",
      "GPU memory usage before cleaning cache: 3270.31 MB\n",
      "GPU memory usage after cleaning cache: 3270.31 MB\n",
      "GPU memory occupied from nvmlInit: 32503 MB.\n",
      "+----------------------------------------------------------------------------------+\n",
      "=======================================================================\n",
      "logits type: torch.float16\n",
      "-----------------------------------------------------------------------\n",
      "Loss: 10.345694541931152\n",
      "-----------------------------------------------------------------------\n",
      "logits: tensor([[[-0.0424,  0.1405, -0.3186,  ..., -0.0536, -0.0767,  0.0967],\n",
      "         [-0.2224, -0.1342,  0.3086,  ..., -0.0615,  0.0247,  0.1748],\n",
      "         [-0.1973, -0.1361,  0.3901,  ...,  0.0219,  0.0798,  0.1747],\n",
      "         ...,\n",
      "         [ 0.0336, -0.1544,  0.4487,  ..., -0.1309, -0.0045,  0.1150],\n",
      "         [ 0.2319,  0.3330,  0.2759,  ...,  0.0732,  0.0803,  0.2098],\n",
      "         [ 0.1400, -0.0151, -0.1193,  ...,  0.4390, -0.2406, -0.0137]]],\n",
      "       device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "-----------------------------------------------------------------------\n",
      "past_key_values: ((tensor([[[[ 0.0193, -0.0650, -0.3340,  0.1405],\n",
      "          [-0.1149, -0.0267,  0.1692,  0.2947],\n",
      "          [-0.2043, -0.0296, -0.0054,  0.2944],\n",
      "          ...,\n",
      "          [ 0.0011, -0.1365, -0.2881,  0.3428],\n",
      "          [ 0.0157,  0.1426, -0.0537, -0.0415],\n",
      "          [-0.0756,  0.1343, -0.1263,  0.0013]],\n",
      "\n",
      "         [[-0.0915, -0.0069, -0.1586,  0.2190],\n",
      "          [ 0.1504, -0.2021, -0.3218, -0.0816],\n",
      "          [ 0.3518, -0.2014, -0.0471, -0.0837],\n",
      "          ...,\n",
      "          [ 0.0630, -0.4248,  0.2644, -0.0981],\n",
      "          [-0.1802, -0.3477,  0.0780, -0.0595],\n",
      "          [-0.5205, -0.2820, -0.0667, -0.3164]],\n",
      "\n",
      "         [[-0.0426,  0.2021,  0.0175,  0.0979],\n",
      "          [-0.1846, -0.0420, -0.0856,  0.2939],\n",
      "          [-0.0276, -0.0449, -0.2014,  0.2937],\n",
      "          ...,\n",
      "          [-0.2462, -0.2461, -0.3481, -0.1134],\n",
      "          [-0.2274, -0.1727, -0.0333, -0.0391],\n",
      "          [ 0.0224, -0.2920,  0.0316, -0.0464]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2115, -0.2161,  0.0035, -0.1049],\n",
      "          [-0.0816, -0.1364, -0.1105,  0.1790],\n",
      "          [ 0.0490, -0.1382, -0.1283,  0.1776],\n",
      "          ...,\n",
      "          [ 0.4155, -0.0758, -0.3247,  0.1073],\n",
      "          [ 0.1188, -0.0925, -0.6504, -0.3845],\n",
      "          [ 0.1287, -0.1238,  0.1882,  0.2428]],\n",
      "\n",
      "         [[-0.3813,  0.1693, -0.2412, -0.0920],\n",
      "          [ 0.0226, -0.1267,  0.1472, -0.2314],\n",
      "          [-0.1117, -0.1245,  0.0985, -0.2327],\n",
      "          ...,\n",
      "          [ 0.1637,  0.2820, -0.1897,  0.2026],\n",
      "          [-0.0742,  0.1174, -0.1530,  0.1594],\n",
      "          [-0.1047,  0.3525,  0.1719,  0.1066]],\n",
      "\n",
      "         [[ 0.0516,  0.4070, -0.0505,  0.1160],\n",
      "          [ 0.2524, -0.1523,  0.5796, -0.2220],\n",
      "          [-0.3516, -0.1501,  0.5254, -0.2235],\n",
      "          ...,\n",
      "          [-0.0125,  0.2593,  0.2961,  0.2476],\n",
      "          [ 0.0261,  0.0925,  0.0893, -0.1271],\n",
      "          [-0.0045,  0.0643,  0.2603, -0.2053]]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<AddBackward0>), tensor([[[[ 0.1604, -0.0873, -0.4744, -0.0951],\n",
      "          [ 0.6094,  0.2808, -0.1458,  0.2527],\n",
      "          [ 0.6094,  0.2808, -0.1458,  0.2527],\n",
      "          ...,\n",
      "          [ 0.3643,  0.0247,  0.3325, -0.0757],\n",
      "          [-0.2532,  0.4475,  0.1681,  0.0243],\n",
      "          [ 0.2230, -0.2981, -0.2600, -0.3667]],\n",
      "\n",
      "         [[ 0.0909, -0.1129, -0.2426, -0.0182],\n",
      "          [-0.1516,  0.0645, -0.0136, -0.2649],\n",
      "          [-0.1516,  0.0645, -0.0136, -0.2649],\n",
      "          ...,\n",
      "          [-0.0838,  0.0668,  0.2391, -0.4890],\n",
      "          [-0.1644,  0.3167,  0.0275,  0.2001],\n",
      "          [-0.1228, -0.2281,  0.2285,  0.2172]],\n",
      "\n",
      "         [[ 0.2549, -0.2732, -0.0636,  0.3303],\n",
      "          [-0.1592, -0.2272, -0.1227, -0.2505],\n",
      "          [-0.1592, -0.2272, -0.1227, -0.2505],\n",
      "          ...,\n",
      "          [-0.1636,  0.3159,  0.0754, -0.0070],\n",
      "          [ 0.0383,  0.2556, -0.0856,  0.0325],\n",
      "          [-0.0194, -0.0810,  0.1487,  0.0713]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2052, -0.0173, -0.2756,  0.1826],\n",
      "          [-0.0210, -0.3091,  0.1626,  0.1866],\n",
      "          [-0.0210, -0.3091,  0.1626,  0.1866],\n",
      "          ...,\n",
      "          [-0.2795, -0.0457, -0.3821, -0.1163],\n",
      "          [ 0.3054, -0.0626, -0.2218, -0.0912],\n",
      "          [-0.0658,  0.0771,  0.2622,  0.0510]],\n",
      "\n",
      "         [[-0.1942,  0.0897,  0.0268,  0.1576],\n",
      "          [ 0.0559,  0.0190, -0.1538, -0.0440],\n",
      "          [ 0.0559,  0.0190, -0.1538, -0.0440],\n",
      "          ...,\n",
      "          [ 0.0127,  0.0835, -0.4673, -0.0084],\n",
      "          [ 0.0830, -0.0657,  0.1564,  0.1963],\n",
      "          [-0.2627, -0.0622, -0.0249, -0.0779]],\n",
      "\n",
      "         [[-0.0849, -0.2771, -0.3228,  0.0203],\n",
      "          [ 0.0189,  0.3333,  0.0802,  0.0114],\n",
      "          [ 0.0189,  0.3333,  0.0802,  0.0114],\n",
      "          ...,\n",
      "          [-0.2312, -0.0295,  0.0604,  0.1243],\n",
      "          [ 0.1466, -0.1963, -0.1134, -0.1749],\n",
      "          [ 0.3850, -0.2340,  0.4321,  0.0344]]]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<TransposeBackward0>)),)\n",
      "-----------------------------------------------------------------------\n",
      "logits shape: torch.Size([1, 128, 30000])\n",
      "=======================================================================\n",
      "past_key_values length: 1\n",
      "     -----------------------------------------------------------------------\n",
      "     past_key_values[0] length (layers): 2\n",
      "          past_key_values[0][0].shape: torch.Size([1, 8, 128, 4])\n",
      "          past_key_values[0][1].shape: torch.Size([1, 8, 128, 4])\n",
      "No nan or inf values in logits\n",
      "Logits                                             Gradient isnan: False\n",
      "Logits                                             Gradient isinf: False\n",
      "Model type: torch.float16\n",
      "Outputs type: torch.float32\n",
      "Outputs logits torch.float32\n",
      "Outputs past_key_values torch.float16\n",
      "Outputs loss torch.float32\n",
      "##########################################################################################################\n",
      "Gradient of model.embed_tokens.weight: None\n",
      "Gradient of model.layers.0.self_attn.q_proj.weight: None\n",
      "Gradient of model.layers.0.self_attn.k_proj.weight: None\n",
      "Gradient of model.layers.0.self_attn.v_proj.weight: None\n",
      "Gradient of model.layers.0.self_attn.o_proj.weight: None\n",
      "Gradient of model.layers.0.mlp.gate_proj.weight: None\n",
      "Gradient of model.layers.0.mlp.up_proj.weight: None\n",
      "Gradient of model.layers.0.mlp.down_proj.weight: None\n",
      "Gradient of model.layers.0.input_layernorm.weight: None\n",
      "Gradient of model.layers.0.post_attention_layernorm.weight: None\n",
      "Gradient of model.norm.weight: None\n",
      "Gradient of lm_head.weight: None\n",
      "==========================================================================================================\n",
      "Data of model.embed_tokens.weight: tensor([[-0.0051, -0.0168,  0.0175,  ...,  0.0017,  0.0170, -0.0182],\n",
      "        [ 0.0116,  0.0215,  0.0152,  ...,  0.0160,  0.0173, -0.0175],\n",
      "        [ 0.0082, -0.0185, -0.0068,  ..., -0.0319, -0.0147, -0.0295],\n",
      "        ...,\n",
      "        [ 0.0195,  0.0100,  0.0262,  ...,  0.0116,  0.0055, -0.0080],\n",
      "        [ 0.0067,  0.0152,  0.0105,  ..., -0.0163,  0.0122, -0.0228],\n",
      "        [-0.0116,  0.0016, -0.0054,  ...,  0.0080,  0.0439, -0.0361]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.embed_tokens.weight                          Gradient isnan: False\n",
      "model.embed_tokens.weight                          Gradient isinf: False\n",
      "Data of model.layers.0.self_attn.q_proj.weight: tensor([[-6.3753e-04, -1.1854e-03,  7.5836e-03,  ..., -3.5095e-02,\n",
      "          2.3651e-02,  1.8356e-02],\n",
      "        [-9.0332e-03,  1.5640e-02,  3.7193e-05,  ..., -9.0561e-03,\n",
      "         -2.8427e-02, -1.1108e-02],\n",
      "        [-2.1866e-02, -3.0575e-03, -1.2459e-02,  ...,  1.9867e-02,\n",
      "         -1.8936e-02, -3.5095e-02],\n",
      "        ...,\n",
      "        [ 1.1841e-02,  5.7602e-03,  5.1346e-03,  ...,  2.5177e-02,\n",
      "          1.8814e-02, -6.6757e-03],\n",
      "        [-3.3966e-02, -3.0167e-02,  4.2191e-03,  ...,  5.3902e-03,\n",
      "          2.2297e-03,  3.4485e-02],\n",
      "        [ 1.7441e-02,  3.3630e-02,  1.0429e-02,  ...,  3.6125e-03,\n",
      "          3.2257e-02, -2.3041e-03]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isinf: False\n",
      "Data of model.layers.0.self_attn.k_proj.weight: tensor([[-0.0237,  0.0165, -0.0075,  ...,  0.0512, -0.0005, -0.0193],\n",
      "        [-0.0101,  0.0185,  0.0125,  ...,  0.0027,  0.0247, -0.0046],\n",
      "        [-0.0125,  0.0217,  0.0361,  ..., -0.0195, -0.0127, -0.0146],\n",
      "        ...,\n",
      "        [ 0.0003, -0.0242,  0.0101,  ..., -0.0277, -0.0422, -0.0003],\n",
      "        [-0.0217, -0.0437,  0.0367,  ..., -0.0186, -0.0352, -0.0180],\n",
      "        [-0.0184,  0.0011, -0.0119,  ...,  0.0223,  0.0067,  0.0242]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isinf: False\n",
      "Data of model.layers.0.self_attn.v_proj.weight: tensor([[ 0.0148, -0.0153, -0.0344,  ...,  0.0211, -0.0098,  0.0165],\n",
      "        [-0.0012, -0.0019, -0.0104,  ..., -0.0005, -0.0009, -0.0101],\n",
      "        [-0.0168, -0.0240, -0.0630,  ...,  0.0207, -0.0046,  0.0152],\n",
      "        ...,\n",
      "        [-0.0211, -0.0171, -0.0310,  ..., -0.0206,  0.0159,  0.0169],\n",
      "        [-0.0184,  0.0178, -0.0069,  ..., -0.0211,  0.0233, -0.0242],\n",
      "        [ 0.0028,  0.0027,  0.0224,  ...,  0.0031, -0.0215, -0.0089]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isinf: False\n",
      "Data of model.layers.0.self_attn.o_proj.weight: tensor([[-0.0045, -0.0160, -0.0172,  ...,  0.0364,  0.0104, -0.0036],\n",
      "        [-0.0041, -0.0263, -0.0057,  ..., -0.0044,  0.0258, -0.0348],\n",
      "        [ 0.0051,  0.0351, -0.0017,  ..., -0.0070,  0.0134, -0.0125],\n",
      "        ...,\n",
      "        [-0.0113, -0.0232, -0.0047,  ..., -0.0242,  0.0088,  0.0042],\n",
      "        [-0.0271,  0.0076, -0.0055,  ..., -0.0082, -0.0024,  0.0218],\n",
      "        [ 0.0232, -0.0019,  0.0462,  ...,  0.0017,  0.0161,  0.0128]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isinf: False\n",
      "Data of model.layers.0.mlp.gate_proj.weight: tensor([[ 1.7262e-03, -1.6357e-02, -5.4626e-03,  ...,  2.1027e-02,\n",
      "          7.7438e-03,  2.8641e-02],\n",
      "        [ 6.7413e-05,  1.7654e-02, -1.8875e-02,  ...,  4.5563e-02,\n",
      "          7.0381e-03,  2.5436e-02],\n",
      "        [ 1.3557e-02, -1.0078e-02, -2.1393e-02,  ..., -1.7227e-02,\n",
      "         -1.2875e-03,  7.6485e-03],\n",
      "        ...,\n",
      "        [-2.0096e-02, -1.9180e-02,  1.9333e-02,  ..., -1.9394e-02,\n",
      "          4.6425e-03, -1.7883e-02],\n",
      "        [-9.8343e-03,  8.9722e-03,  9.6054e-03,  ..., -7.5493e-03,\n",
      "          2.2720e-02, -1.2276e-02],\n",
      "        [-2.0325e-02,  8.5602e-03, -2.5391e-02,  ...,  8.3389e-03,\n",
      "         -1.7914e-02,  1.7914e-02]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isnan: False\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isinf: False\n",
      "Data of model.layers.0.mlp.up_proj.weight: tensor([[-0.0160,  0.0198,  0.0196,  ...,  0.0264, -0.0192,  0.0106],\n",
      "        [-0.0006, -0.0131,  0.0031,  ...,  0.0079,  0.0367,  0.0051],\n",
      "        [ 0.0090,  0.0378, -0.0209,  ...,  0.0242, -0.0308,  0.0178],\n",
      "        ...,\n",
      "        [-0.0176, -0.0014, -0.0217,  ...,  0.0196,  0.0225, -0.0160],\n",
      "        [ 0.0167, -0.0376,  0.0200,  ..., -0.0124, -0.0168,  0.0418],\n",
      "        [-0.0087,  0.0025,  0.0019,  ...,  0.0214, -0.0015,  0.0083]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isnan: False\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isinf: False\n",
      "Data of model.layers.0.mlp.down_proj.weight: tensor([[ 0.0048,  0.0111, -0.0287,  ..., -0.0199,  0.0141, -0.0165],\n",
      "        [-0.0027,  0.0020,  0.0043,  ..., -0.0308, -0.0122,  0.0119],\n",
      "        [ 0.0187,  0.0005, -0.0349,  ..., -0.0012, -0.0111,  0.0216],\n",
      "        ...,\n",
      "        [ 0.0395, -0.0174, -0.0219,  ..., -0.0107, -0.0077, -0.0323],\n",
      "        [ 0.0063,  0.0029,  0.0168,  ...,  0.0041,  0.0058, -0.0025],\n",
      "        [-0.0091,  0.0139,  0.0078,  ..., -0.0115,  0.0122,  0.0029]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isnan: False\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isinf: False\n",
      "Data of model.layers.0.input_layernorm.weight: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.input_layernorm.weight              Gradient isnan: False\n",
      "model.layers.0.input_layernorm.weight              Gradient isinf: False\n",
      "Data of model.layers.0.post_attention_layernorm.weight: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isnan: False\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isinf: False\n",
      "Data of model.norm.weight: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', dtype=torch.float16)\n",
      "model.norm.weight                                  Gradient isnan: False\n",
      "model.norm.weight                                  Gradient isinf: False\n",
      "Data of lm_head.weight: tensor([[ 0.0063, -0.0019,  0.0204,  ...,  0.0008,  0.0358, -0.0195],\n",
      "        [ 0.0053,  0.0071, -0.0259,  ...,  0.0351, -0.0576, -0.0518],\n",
      "        [ 0.0074, -0.0076,  0.0113,  ..., -0.0218, -0.0345,  0.0035],\n",
      "        ...,\n",
      "        [-0.0188, -0.0017, -0.0025,  ...,  0.0240, -0.0029,  0.0290],\n",
      "        [ 0.0034, -0.0046,  0.0001,  ...,  0.0004, -0.0192, -0.0113],\n",
      "        [ 0.0175, -0.0283,  0.0063,  ...,  0.0233,  0.0285,  0.0181]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "lm_head.weight                                     Gradient isnan: False\n",
      "lm_head.weight                                     Gradient isinf: False\n",
      "==========================================================================================================\n",
      "##########################################################################################################\n",
      "Gradient of model.embed_tokens.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "model.embed_tokens.weight                          Gradient isnan: False\n",
      "model.embed_tokens.weight                          Gradient isinf: False\n",
      "Gradient of model.layers.0.self_attn.q_proj.weight: tensor([[ 1.8692e-04,  8.1241e-05,  4.2140e-05,  ..., -2.3365e-05,\n",
      "         -1.0222e-04,  1.3351e-05],\n",
      "        [ 7.0930e-05,  7.3135e-05,  1.4460e-04,  ...,  1.0252e-05,\n",
      "          3.3331e-04,  2.7084e-04],\n",
      "        [ 4.5478e-05,  4.8399e-05,  6.3896e-05,  ..., -8.4281e-05,\n",
      "         -1.8787e-04, -1.0157e-04],\n",
      "        ...,\n",
      "        [-5.6267e-05, -3.0684e-04,  9.3400e-05,  ...,  7.7486e-07,\n",
      "          5.4240e-06, -1.1957e-04],\n",
      "        [-4.0579e-04,  1.2600e-04, -2.1636e-04,  ..., -5.9009e-05,\n",
      "         -3.9411e-04,  1.3614e-04],\n",
      "        [-2.8968e-04, -4.0233e-05,  6.9499e-05,  ...,  9.5367e-06,\n",
      "         -2.0385e-04,  4.4107e-06]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isinf: False\n",
      "Gradient of model.layers.0.self_attn.k_proj.weight: tensor([[-2.0516e-04,  1.8632e-04, -3.1948e-04,  ...,  5.6863e-05,\n",
      "         -5.6934e-04, -2.4915e-04],\n",
      "        [-1.6224e-04,  1.8132e-04,  2.1422e-04,  ..., -2.0695e-04,\n",
      "          3.9911e-04,  1.5438e-04],\n",
      "        [-5.0783e-04,  5.0664e-05, -1.4317e-04,  ...,  2.4700e-04,\n",
      "         -7.5579e-05,  5.2834e-04],\n",
      "        ...,\n",
      "        [ 1.2684e-04, -2.1660e-04, -1.3626e-04,  ..., -1.3506e-04,\n",
      "          3.3307e-04,  5.0306e-05],\n",
      "        [ 3.7503e-04, -1.1235e-04, -5.2035e-05,  ...,  4.9353e-04,\n",
      "          1.6689e-05,  6.9857e-04],\n",
      "        [ 1.3542e-04, -3.7098e-04, -2.7061e-05,  ..., -4.2260e-05,\n",
      "          2.0564e-04, -1.0312e-04]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isinf: False\n",
      "Gradient of model.layers.0.self_attn.v_proj.weight: tensor([[-1.0590e-02,  1.0780e-02,  9.5825e-03,  ...,  8.8348e-03,\n",
      "         -1.3916e-02, -8.5831e-03],\n",
      "        [ 2.0542e-03, -1.5762e-02, -6.2883e-05,  ..., -1.5808e-02,\n",
      "          2.4155e-02,  1.4587e-02],\n",
      "        [-8.4457e-03,  9.2163e-03,  3.4733e-03,  ...,  1.5850e-03,\n",
      "         -8.1100e-03, -9.9716e-03],\n",
      "        ...,\n",
      "        [-1.4977e-02,  1.3664e-02,  7.0724e-03,  ...,  1.0918e-02,\n",
      "         -1.8250e-02, -1.2909e-02],\n",
      "        [-4.2610e-03, -9.6283e-03, -3.7823e-03,  ..., -1.8066e-02,\n",
      "          1.9226e-02,  1.0696e-02],\n",
      "        [ 4.6921e-03, -1.1566e-02, -7.1812e-04,  ..., -2.6108e-02,\n",
      "          2.3041e-02,  1.6205e-02]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isinf: False\n",
      "Gradient of model.layers.0.self_attn.o_proj.weight: tensor([[ 1.2474e-03,  2.6798e-03,  1.4954e-03,  ...,  6.3372e-04,\n",
      "          7.4983e-05,  1.1463e-03],\n",
      "        [-2.8782e-03, -1.1272e-03, -2.7828e-03,  ..., -5.3310e-04,\n",
      "          4.9639e-04, -1.5173e-03],\n",
      "        [ 9.9869e-03,  6.8760e-04, -1.5335e-03,  ...,  3.6163e-03,\n",
      "         -3.8891e-03, -1.3885e-03],\n",
      "        ...,\n",
      "        [-1.1313e-04,  2.6417e-03, -8.5497e-04,  ..., -1.2517e-05,\n",
      "          1.0290e-03, -1.5078e-03],\n",
      "        [ 3.4981e-03, -1.5087e-03, -8.4829e-04,  ...,  1.7619e-04,\n",
      "         -3.3016e-03, -3.0785e-03],\n",
      "        [ 1.1650e-02,  2.8801e-03, -5.7640e-03,  ...,  2.5902e-03,\n",
      "         -2.5578e-03,  3.0537e-03]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isinf: False\n",
      "Gradient of model.layers.0.mlp.gate_proj.weight: tensor([[-0.0031,  0.0005, -0.0013,  ..., -0.0002,  0.0053,  0.0017],\n",
      "        [ 0.0003, -0.0046,  0.0019,  ..., -0.0083,  0.0104,  0.0001],\n",
      "        [-0.0006, -0.0015,  0.0008,  ..., -0.0003, -0.0012,  0.0003],\n",
      "        ...,\n",
      "        [ 0.0010,  0.0028, -0.0019,  ...,  0.0014, -0.0016, -0.0027],\n",
      "        [-0.0038,  0.0036, -0.0051,  ...,  0.0064, -0.0023,  0.0035],\n",
      "        [-0.0013, -0.0031, -0.0020,  ...,  0.0027,  0.0040, -0.0006]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isnan: False\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isinf: False\n",
      "Gradient of model.layers.0.mlp.up_proj.weight: tensor([[ 0.0002, -0.0013, -0.0007,  ..., -0.0011,  0.0030,  0.0008],\n",
      "        [ 0.0044, -0.0011,  0.0014,  ..., -0.0009,  0.0066, -0.0017],\n",
      "        [ 0.0014,  0.0009,  0.0012,  ...,  0.0005, -0.0002,  0.0014],\n",
      "        ...,\n",
      "        [ 0.0039,  0.0016, -0.0056,  ..., -0.0016, -0.0038, -0.0030],\n",
      "        [ 0.0051, -0.0072, -0.0008,  ..., -0.0052,  0.0037, -0.0016],\n",
      "        [ 0.0012, -0.0053, -0.0077,  ..., -0.0041,  0.0006,  0.0019]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isnan: False\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isinf: False\n",
      "Gradient of model.layers.0.mlp.down_proj.weight: tensor([[ 4.8714e-03,  8.6784e-04, -4.7946e-04,  ...,  5.2910e-03,\n",
      "         -9.1095e-03, -4.1122e-03],\n",
      "        [ 1.5192e-03,  4.9515e-03,  1.6689e-05,  ..., -1.3866e-03,\n",
      "         -6.7091e-04, -1.6766e-03],\n",
      "        [ 3.4504e-03,  8.3447e-07,  1.4086e-03,  ..., -2.8362e-03,\n",
      "         -3.7193e-03,  2.7885e-03],\n",
      "        ...,\n",
      "        [ 3.8266e-04, -2.0921e-04,  1.6136e-03,  ...,  1.7605e-03,\n",
      "          2.6226e-03,  7.3433e-04],\n",
      "        [ 5.1079e-03, -2.3537e-03,  3.5477e-04,  ...,  3.7193e-04,\n",
      "         -6.1417e-03, -1.4460e-04],\n",
      "        [ 1.5373e-03,  1.6069e-03, -1.6947e-03,  ...,  1.5821e-03,\n",
      "          2.5539e-03, -1.2932e-03]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isnan: False\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isinf: False\n",
      "Gradient of model.layers.0.input_layernorm.weight: tensor([-5.7840e-04, -1.0424e-03, -1.3552e-03,  2.9755e-04,  9.8610e-04,\n",
      "        -1.8072e-03,  7.2145e-04,  3.8266e-05, -2.4080e-04,  3.9411e-04,\n",
      "         1.4248e-03,  6.3038e-04, -4.6301e-04,  2.7084e-04, -4.1580e-04,\n",
      "         1.7643e-03,  1.3418e-03, -1.1671e-04, -9.9182e-04, -6.8998e-04,\n",
      "         7.4625e-04,  3.7146e-04, -1.2040e-04, -4.3488e-04, -3.5071e-04,\n",
      "         3.9577e-04,  8.4829e-04,  4.5347e-04, -4.5300e-06, -1.9252e-05,\n",
      "        -1.2398e-04, -6.8283e-04,  2.0466e-03,  1.2169e-03,  5.2404e-04,\n",
      "         3.9005e-03, -5.3883e-04,  1.3409e-03, -4.0913e-04, -4.2737e-05,\n",
      "         6.3753e-04, -7.5340e-05, -8.2636e-04,  5.8699e-04,  5.9903e-05,\n",
      "         6.7353e-06, -1.0073e-04,  8.2254e-04, -3.2949e-04, -1.3647e-03,\n",
      "         6.1512e-04, -6.4659e-04, -1.7166e-04, -5.3835e-04,  6.7234e-04,\n",
      "        -1.3189e-03,  3.3164e-04, -9.4700e-04, -1.3995e-04,  5.9223e-04,\n",
      "         3.8528e-04,  5.6124e-04,  2.1839e-04, -9.5844e-05,  4.5061e-04,\n",
      "        -1.7595e-04,  3.2091e-04,  6.7854e-04, -3.3855e-04, -1.2302e-03,\n",
      "         8.1301e-05,  8.2254e-04, -1.8644e-04,  2.1577e-04, -5.1117e-04,\n",
      "         1.2121e-03,  3.9434e-04,  3.7074e-04, -8.8978e-04, -1.5821e-03,\n",
      "         5.0116e-04,  1.1748e-04, -2.6894e-03,  1.9360e-04,  4.2987e-04,\n",
      "         2.6774e-04, -4.1103e-04,  4.9934e-03,  1.1044e-03, -6.6996e-04,\n",
      "        -1.0405e-03, -9.5463e-04, -1.0862e-03, -1.7047e-05,  3.4809e-04,\n",
      "         7.3814e-04, -6.6662e-04,  1.1234e-03,  1.1702e-03,  1.7786e-04,\n",
      "         1.6987e-04,  8.7643e-04, -2.2087e-03,  3.6836e-04, -6.5899e-04,\n",
      "         1.4079e-04, -1.3471e-04,  7.0190e-04, -2.8706e-04,  4.7326e-05,\n",
      "         8.5354e-04,  5.6934e-04, -5.6410e-04, -5.2118e-04, -5.1141e-05,\n",
      "         3.4118e-04, -2.9492e-04, -8.2493e-04, -5.1260e-04, -1.3745e-04,\n",
      "         5.8842e-04,  2.8896e-04,  3.9172e-04, -5.8031e-04,  1.1234e-03,\n",
      "         4.0245e-04,  2.7905e-03, -9.3699e-04], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.input_layernorm.weight              Gradient isnan: False\n",
      "model.layers.0.input_layernorm.weight              Gradient isinf: False\n",
      "Gradient of model.layers.0.post_attention_layernorm.weight: tensor([ 7.2575e-04, -4.1413e-04, -6.0081e-04,  7.3051e-04, -6.8665e-04,\n",
      "         2.4204e-03,  1.2569e-03, -1.1158e-03, -9.5701e-04, -3.1185e-04,\n",
      "         5.4693e-04, -4.1294e-04,  5.2989e-05, -1.3704e-03,  1.7977e-03,\n",
      "        -7.8630e-04,  1.8616e-03,  1.5366e-04, -8.4257e-04, -5.3358e-04,\n",
      "         9.4461e-04,  5.1355e-04,  1.4811e-03,  4.3344e-04,  8.2493e-04,\n",
      "         6.0654e-04,  7.6914e-04,  7.3051e-04, -1.0509e-03, -8.7357e-04,\n",
      "        -5.2547e-04,  1.3361e-03, -1.1692e-03,  3.5572e-04, -4.3035e-04,\n",
      "         7.2813e-04,  8.7547e-04,  4.2498e-05, -1.1021e-04, -7.8249e-04,\n",
      "         1.0338e-03, -3.2377e-04,  1.7767e-03,  8.2731e-04, -2.9993e-04,\n",
      "         3.4428e-04, -2.5964e-04, -1.2660e-04, -5.2392e-05, -1.0252e-03,\n",
      "        -1.1301e-03,  1.2379e-03, -5.9128e-04, -1.4009e-03,  4.1604e-04,\n",
      "        -1.5163e-03, -4.2295e-04, -2.2793e-03,  1.0567e-03, -1.1501e-03,\n",
      "         5.7757e-05, -2.2554e-04,  1.0424e-03,  1.7424e-03,  1.9848e-05,\n",
      "        -1.5843e-04, -7.1478e-04,  6.4802e-04,  7.3242e-04, -8.5974e-04,\n",
      "         1.5106e-03, -1.9836e-04, -7.3051e-04, -1.5402e-03, -3.0613e-04,\n",
      "        -2.0754e-04, -1.1988e-03,  3.5453e-04, -3.8099e-04,  1.2231e-04,\n",
      "        -9.4557e-04, -8.5354e-04, -3.9649e-04, -1.3523e-03,  3.0994e-04,\n",
      "        -1.0309e-03,  8.5652e-05,  7.3242e-04, -4.3678e-04,  4.4036e-04,\n",
      "        -5.3406e-04,  1.5039e-03,  9.8348e-06, -2.9945e-04,  2.5535e-04,\n",
      "        -2.3699e-04,  1.4627e-04, -6.4421e-04, -2.9159e-04, -3.5787e-04,\n",
      "        -2.5344e-04, -8.1444e-04, -5.8889e-04, -7.5340e-04, -3.6740e-04,\n",
      "         8.1348e-04, -2.2278e-03, -5.6648e-04,  1.0290e-03, -2.6536e-04,\n",
      "         1.9722e-03, -1.5755e-03, -1.2708e-04, -4.3631e-04,  1.4114e-03,\n",
      "         3.3855e-04, -3.4690e-05, -8.3876e-04,  7.7248e-04,  1.5795e-04,\n",
      "         2.0046e-03, -2.2621e-03, -4.3344e-04, -1.0961e-04, -2.3460e-03,\n",
      "        -1.3852e-04, -5.9557e-04,  6.3515e-04], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isnan: False\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isinf: False\n",
      "Gradient of model.norm.weight: tensor([-3.6621e-04,  2.7990e-04, -1.5392e-03,  7.4863e-04,  4.8180e-03,\n",
      "         2.1210e-03, -1.0738e-03,  2.0771e-03,  2.8992e-03, -4.0855e-03,\n",
      "        -1.9276e-04, -8.8072e-04,  7.0524e-04, -3.1090e-04,  8.4972e-04,\n",
      "         1.2980e-03,  6.7425e-04,  9.4557e-04,  1.4267e-03, -1.1911e-03,\n",
      "        -4.3440e-04,  2.5349e-03,  3.2544e-04, -1.3399e-03,  2.4910e-03,\n",
      "         2.6059e-04,  1.1587e-04,  1.1930e-03,  1.2331e-03,  1.1673e-03,\n",
      "         1.8311e-04,  9.0837e-04,  1.7996e-03,  7.9584e-04, -1.4019e-04,\n",
      "         4.7531e-03,  2.4853e-03,  1.7300e-03,  2.1420e-03, -3.2005e-03,\n",
      "        -3.7003e-04, -7.6723e-04,  2.5425e-03,  4.1866e-04,  2.2049e-03,\n",
      "        -2.4338e-03, -2.6741e-03, -1.8179e-04, -1.9035e-03,  8.9884e-05,\n",
      "         8.5211e-04,  5.8556e-04,  3.7041e-03, -1.9665e-03,  1.2808e-03,\n",
      "        -5.7840e-04,  1.8921e-03, -7.9870e-05,  7.8678e-06,  2.7065e-03,\n",
      "         3.2196e-03,  5.2214e-04,  3.9177e-03,  2.5213e-05,  4.9171e-03,\n",
      "         1.9760e-03, -1.8930e-03, -2.2316e-03, -1.9140e-03,  1.5516e-03,\n",
      "         8.2588e-04,  2.1534e-03,  2.0456e-04, -9.3651e-04,  6.8188e-04,\n",
      "        -2.5940e-03,  6.1941e-04,  1.5230e-03, -7.7438e-04,  2.0084e-03,\n",
      "        -1.5974e-03, -4.9162e-04,  1.0389e-04,  1.0185e-03, -1.4524e-03,\n",
      "         7.7820e-04,  4.9543e-04, -2.6560e-04, -1.0433e-03,  1.4219e-03,\n",
      "         1.2846e-03,  2.4681e-03,  3.0684e-04, -9.0408e-04,  1.5240e-03,\n",
      "         6.2466e-04, -1.8239e-04,  6.8045e-04,  7.8506e-03, -2.2583e-03,\n",
      "         3.7422e-03,  1.8196e-03,  1.7691e-03, -2.9993e-04,  1.9407e-03,\n",
      "        -4.1733e-03,  1.4906e-03,  1.0080e-03, -1.9016e-03, -4.8923e-04,\n",
      "        -2.5654e-03, -1.2608e-03, -1.3723e-03, -3.4981e-03,  7.8344e-04,\n",
      "         8.7023e-04,  1.6432e-03, -8.6594e-04, -2.7370e-03, -6.1989e-04,\n",
      "         3.2234e-03, -1.5392e-03,  3.9406e-03, -1.2312e-03,  3.2539e-03,\n",
      "        -4.3583e-04,  4.5753e-04,  1.5669e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.norm.weight                                  Gradient isnan: False\n",
      "model.norm.weight                                  Gradient isinf: False\n",
      "Gradient of lm_head.weight: tensor([[ 1.3709e-05, -2.1875e-05, -4.5300e-06,  ..., -1.1146e-05,\n",
      "          1.8895e-05, -9.5367e-06],\n",
      "        [ 1.1921e-05, -1.8358e-05, -6.7949e-06,  ..., -1.0073e-05,\n",
      "          1.0908e-05, -1.0908e-05],\n",
      "        [ 1.3649e-05, -2.1100e-05, -5.6028e-06,  ..., -1.2279e-05,\n",
      "          1.2457e-05, -7.6890e-06],\n",
      "        ...,\n",
      "        [ 1.2040e-05, -2.0444e-05, -4.1127e-06,  ..., -9.7752e-06,\n",
      "          1.3173e-05, -7.1526e-06],\n",
      "        [ 1.2338e-05, -1.8537e-05, -5.4836e-06,  ..., -1.0848e-05,\n",
      "          1.3590e-05, -9.4175e-06],\n",
      "        [ 1.4484e-05, -2.3484e-05, -5.6028e-06,  ..., -1.0788e-05,\n",
      "          1.7107e-05, -8.4639e-06]], device='cuda:0', dtype=torch.float16)\n",
      "lm_head.weight                                     Gradient isnan: False\n",
      "lm_head.weight                                     Gradient isinf: False\n",
      "==========================================================================================================\n",
      "Data of model.embed_tokens.weight: tensor([[-0.0051, -0.0168,  0.0175,  ...,  0.0017,  0.0170, -0.0182],\n",
      "        [ 0.0116,  0.0215,  0.0152,  ...,  0.0160,  0.0173, -0.0175],\n",
      "        [ 0.0082, -0.0185, -0.0068,  ..., -0.0319, -0.0147, -0.0295],\n",
      "        ...,\n",
      "        [ 0.0195,  0.0100,  0.0262,  ...,  0.0116,  0.0055, -0.0080],\n",
      "        [ 0.0067,  0.0152,  0.0105,  ..., -0.0163,  0.0122, -0.0228],\n",
      "        [-0.0116,  0.0016, -0.0054,  ...,  0.0080,  0.0439, -0.0361]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.embed_tokens.weight                          Gradient isnan: False\n",
      "model.embed_tokens.weight                          Gradient isinf: False\n",
      "Data of model.layers.0.self_attn.q_proj.weight: tensor([[-6.3753e-04, -1.1854e-03,  7.5836e-03,  ..., -3.5095e-02,\n",
      "          2.3651e-02,  1.8356e-02],\n",
      "        [-9.0332e-03,  1.5640e-02,  3.7193e-05,  ..., -9.0561e-03,\n",
      "         -2.8427e-02, -1.1108e-02],\n",
      "        [-2.1866e-02, -3.0575e-03, -1.2459e-02,  ...,  1.9867e-02,\n",
      "         -1.8936e-02, -3.5095e-02],\n",
      "        ...,\n",
      "        [ 1.1841e-02,  5.7602e-03,  5.1346e-03,  ...,  2.5177e-02,\n",
      "          1.8814e-02, -6.6757e-03],\n",
      "        [-3.3966e-02, -3.0167e-02,  4.2191e-03,  ...,  5.3902e-03,\n",
      "          2.2297e-03,  3.4485e-02],\n",
      "        [ 1.7441e-02,  3.3630e-02,  1.0429e-02,  ...,  3.6125e-03,\n",
      "          3.2257e-02, -2.3041e-03]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isinf: False\n",
      "Data of model.layers.0.self_attn.k_proj.weight: tensor([[-0.0237,  0.0165, -0.0075,  ...,  0.0512, -0.0005, -0.0193],\n",
      "        [-0.0101,  0.0185,  0.0125,  ...,  0.0027,  0.0247, -0.0046],\n",
      "        [-0.0125,  0.0217,  0.0361,  ..., -0.0195, -0.0127, -0.0146],\n",
      "        ...,\n",
      "        [ 0.0003, -0.0242,  0.0101,  ..., -0.0277, -0.0422, -0.0003],\n",
      "        [-0.0217, -0.0437,  0.0367,  ..., -0.0186, -0.0352, -0.0180],\n",
      "        [-0.0184,  0.0011, -0.0119,  ...,  0.0223,  0.0067,  0.0242]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isinf: False\n",
      "Data of model.layers.0.self_attn.v_proj.weight: tensor([[ 0.0148, -0.0153, -0.0344,  ...,  0.0211, -0.0098,  0.0165],\n",
      "        [-0.0012, -0.0019, -0.0104,  ..., -0.0005, -0.0009, -0.0101],\n",
      "        [-0.0168, -0.0240, -0.0630,  ...,  0.0207, -0.0046,  0.0152],\n",
      "        ...,\n",
      "        [-0.0211, -0.0171, -0.0310,  ..., -0.0206,  0.0159,  0.0169],\n",
      "        [-0.0184,  0.0178, -0.0069,  ..., -0.0211,  0.0233, -0.0242],\n",
      "        [ 0.0028,  0.0027,  0.0224,  ...,  0.0031, -0.0215, -0.0089]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isinf: False\n",
      "Data of model.layers.0.self_attn.o_proj.weight: tensor([[-0.0045, -0.0160, -0.0172,  ...,  0.0364,  0.0104, -0.0036],\n",
      "        [-0.0041, -0.0263, -0.0057,  ..., -0.0044,  0.0258, -0.0348],\n",
      "        [ 0.0051,  0.0351, -0.0017,  ..., -0.0070,  0.0134, -0.0125],\n",
      "        ...,\n",
      "        [-0.0113, -0.0232, -0.0047,  ..., -0.0242,  0.0088,  0.0042],\n",
      "        [-0.0271,  0.0076, -0.0055,  ..., -0.0082, -0.0024,  0.0218],\n",
      "        [ 0.0232, -0.0019,  0.0462,  ...,  0.0017,  0.0161,  0.0128]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isinf: False\n",
      "Data of model.layers.0.mlp.gate_proj.weight: tensor([[ 1.7262e-03, -1.6357e-02, -5.4626e-03,  ...,  2.1027e-02,\n",
      "          7.7438e-03,  2.8641e-02],\n",
      "        [ 6.7413e-05,  1.7654e-02, -1.8875e-02,  ...,  4.5563e-02,\n",
      "          7.0381e-03,  2.5436e-02],\n",
      "        [ 1.3557e-02, -1.0078e-02, -2.1393e-02,  ..., -1.7227e-02,\n",
      "         -1.2875e-03,  7.6485e-03],\n",
      "        ...,\n",
      "        [-2.0096e-02, -1.9180e-02,  1.9333e-02,  ..., -1.9394e-02,\n",
      "          4.6425e-03, -1.7883e-02],\n",
      "        [-9.8343e-03,  8.9722e-03,  9.6054e-03,  ..., -7.5493e-03,\n",
      "          2.2720e-02, -1.2276e-02],\n",
      "        [-2.0325e-02,  8.5602e-03, -2.5391e-02,  ...,  8.3389e-03,\n",
      "         -1.7914e-02,  1.7914e-02]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isnan: False\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isinf: False\n",
      "Data of model.layers.0.mlp.up_proj.weight: tensor([[-0.0160,  0.0198,  0.0196,  ...,  0.0264, -0.0192,  0.0106],\n",
      "        [-0.0006, -0.0131,  0.0031,  ...,  0.0079,  0.0367,  0.0051],\n",
      "        [ 0.0090,  0.0378, -0.0209,  ...,  0.0242, -0.0308,  0.0178],\n",
      "        ...,\n",
      "        [-0.0176, -0.0014, -0.0217,  ...,  0.0196,  0.0225, -0.0160],\n",
      "        [ 0.0167, -0.0376,  0.0200,  ..., -0.0124, -0.0168,  0.0418],\n",
      "        [-0.0087,  0.0025,  0.0019,  ...,  0.0214, -0.0015,  0.0083]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isnan: False\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isinf: False\n",
      "Data of model.layers.0.mlp.down_proj.weight: tensor([[ 0.0048,  0.0111, -0.0287,  ..., -0.0199,  0.0141, -0.0165],\n",
      "        [-0.0027,  0.0020,  0.0043,  ..., -0.0308, -0.0122,  0.0119],\n",
      "        [ 0.0187,  0.0005, -0.0349,  ..., -0.0012, -0.0111,  0.0216],\n",
      "        ...,\n",
      "        [ 0.0395, -0.0174, -0.0219,  ..., -0.0107, -0.0077, -0.0323],\n",
      "        [ 0.0063,  0.0029,  0.0168,  ...,  0.0041,  0.0058, -0.0025],\n",
      "        [-0.0091,  0.0139,  0.0078,  ..., -0.0115,  0.0122,  0.0029]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isnan: False\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isinf: False\n",
      "Data of model.layers.0.input_layernorm.weight: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.input_layernorm.weight              Gradient isnan: False\n",
      "model.layers.0.input_layernorm.weight              Gradient isinf: False\n",
      "Data of model.layers.0.post_attention_layernorm.weight: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isnan: False\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isinf: False\n",
      "Data of model.norm.weight: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', dtype=torch.float16)\n",
      "model.norm.weight                                  Gradient isnan: False\n",
      "model.norm.weight                                  Gradient isinf: False\n",
      "Data of lm_head.weight: tensor([[ 0.0063, -0.0019,  0.0204,  ...,  0.0008,  0.0358, -0.0195],\n",
      "        [ 0.0053,  0.0071, -0.0259,  ...,  0.0351, -0.0576, -0.0518],\n",
      "        [ 0.0074, -0.0076,  0.0113,  ..., -0.0218, -0.0345,  0.0035],\n",
      "        ...,\n",
      "        [-0.0188, -0.0017, -0.0025,  ...,  0.0240, -0.0029,  0.0290],\n",
      "        [ 0.0034, -0.0046,  0.0001,  ...,  0.0004, -0.0192, -0.0113],\n",
      "        [ 0.0175, -0.0283,  0.0063,  ...,  0.0233,  0.0285,  0.0181]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "lm_head.weight                                     Gradient isnan: False\n",
      "lm_head.weight                                     Gradient isinf: False\n",
      "==========================================================================================================\n",
      "Gradient norm: 1.8515625\n",
      "##########################################################################################################\n",
      "Gradient of model.embed_tokens.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "model.embed_tokens.weight                          Gradient isnan: False\n",
      "model.embed_tokens.weight                          Gradient isinf: False\n",
      "Gradient of model.layers.0.self_attn.q_proj.weight: tensor([[ 5.0485e-05,  2.1935e-05,  1.1384e-05,  ..., -6.3181e-06,\n",
      "         -2.7597e-05,  3.5763e-06],\n",
      "        [ 1.9133e-05,  1.9729e-05,  3.9041e-05,  ...,  2.7418e-06,\n",
      "          9.0003e-05,  7.3135e-05],\n",
      "        [ 1.2279e-05,  1.3053e-05,  1.7226e-05,  ..., -2.2769e-05,\n",
      "         -5.0724e-05, -2.7418e-05],\n",
      "        ...,\n",
      "        [-1.5199e-05, -8.2850e-05,  2.5213e-05,  ...,  2.3842e-07,\n",
      "          1.4901e-06, -3.2306e-05],\n",
      "        [-1.0955e-04,  3.4034e-05, -5.8413e-05,  ..., -1.5914e-05,\n",
      "         -1.0639e-04,  3.6776e-05],\n",
      "        [-7.8201e-05, -1.0848e-05,  1.8775e-05,  ...,  2.5630e-06,\n",
      "         -5.5015e-05,  1.1921e-06]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isinf: False\n",
      "Gradient of model.layers.0.self_attn.k_proj.weight: tensor([[-5.5373e-05,  5.0306e-05, -8.6248e-05,  ...,  1.5378e-05,\n",
      "         -1.5378e-04, -6.7294e-05],\n",
      "        [-4.3809e-05,  4.8935e-05,  5.7817e-05,  ..., -5.5909e-05,\n",
      "          1.0777e-04,  4.1664e-05],\n",
      "        [-1.3709e-04,  1.3709e-05, -3.8683e-05,  ...,  6.6698e-05,\n",
      "         -2.0385e-05,  1.4269e-04],\n",
      "        ...,\n",
      "        [ 3.4273e-05, -5.8472e-05, -3.6776e-05,  ..., -3.6478e-05,\n",
      "          8.9943e-05,  1.3590e-05],\n",
      "        [ 1.0127e-04, -3.0339e-05, -1.4067e-05,  ...,  1.3328e-04,\n",
      "          4.5300e-06,  1.8859e-04],\n",
      "        [ 3.6538e-05, -1.0020e-04, -7.3314e-06,  ..., -1.1384e-05,\n",
      "          5.5552e-05, -2.7835e-05]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isinf: False\n",
      "Gradient of model.layers.0.self_attn.v_proj.weight: tensor([[-2.8591e-03,  2.9106e-03,  2.5883e-03,  ...,  2.3861e-03,\n",
      "         -3.7575e-03, -2.3174e-03],\n",
      "        [ 5.5456e-04, -4.2572e-03, -1.6987e-05,  ..., -4.2686e-03,\n",
      "          6.5231e-03,  3.9406e-03],\n",
      "        [-2.2812e-03,  2.4891e-03,  9.3794e-04,  ...,  4.2796e-04,\n",
      "         -2.1896e-03, -2.6932e-03],\n",
      "        ...,\n",
      "        [-4.0436e-03,  3.6888e-03,  1.9093e-03,  ...,  2.9488e-03,\n",
      "         -4.9286e-03, -3.4847e-03],\n",
      "        [-1.1501e-03, -2.5997e-03, -1.0214e-03,  ..., -4.8790e-03,\n",
      "          5.1918e-03,  2.8877e-03],\n",
      "        [ 1.2665e-03, -3.1223e-03, -1.9395e-04,  ..., -7.0496e-03,\n",
      "          6.2218e-03,  4.3755e-03]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isinf: False\n",
      "Gradient of model.layers.0.self_attn.o_proj.weight: tensor([[ 3.3689e-04,  7.2384e-04,  4.0388e-04,  ...,  1.7107e-04,\n",
      "          2.0266e-05,  3.0947e-04],\n",
      "        [-7.7724e-04, -3.0446e-04, -7.5150e-04,  ..., -1.4400e-04,\n",
      "          1.3399e-04, -4.0960e-04],\n",
      "        [ 2.6970e-03,  1.8561e-04, -4.1413e-04,  ...,  9.7656e-04,\n",
      "         -1.0500e-03, -3.7503e-04],\n",
      "        ...,\n",
      "        [-3.0518e-05,  7.1335e-04, -2.3091e-04,  ..., -3.3975e-06,\n",
      "          2.7776e-04, -4.0722e-04],\n",
      "        [ 9.4461e-04, -4.0746e-04, -2.2900e-04,  ...,  4.7565e-05,\n",
      "         -8.9169e-04, -8.3113e-04],\n",
      "        [ 3.1452e-03,  7.7772e-04, -1.5564e-03,  ...,  6.9952e-04,\n",
      "         -6.9046e-04,  8.2445e-04]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isinf: False\n",
      "Gradient of model.layers.0.mlp.gate_proj.weight: tensor([[-8.4114e-04,  1.2290e-04, -3.5739e-04,  ..., -5.0783e-05,\n",
      "          1.4410e-03,  4.5323e-04],\n",
      "        [ 7.6175e-05, -1.2350e-03,  5.2357e-04,  ..., -2.2469e-03,\n",
      "          2.8076e-03,  3.1531e-05],\n",
      "        [-1.6630e-04, -4.0197e-04,  2.1601e-04,  ..., -7.4029e-05,\n",
      "         -3.3236e-04,  7.5519e-05],\n",
      "        ...,\n",
      "        [ 2.6321e-04,  7.5769e-04, -5.2118e-04,  ...,  3.6931e-04,\n",
      "         -4.3273e-04, -7.3957e-04],\n",
      "        [-1.0290e-03,  9.8324e-04, -1.3647e-03,  ...,  1.7195e-03,\n",
      "         -6.3086e-04,  9.5034e-04],\n",
      "        [-3.6073e-04, -8.3876e-04, -5.5075e-04,  ...,  7.1573e-04,\n",
      "          1.0929e-03, -1.7047e-04]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isnan: False\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isinf: False\n",
      "Gradient of model.layers.0.mlp.up_proj.weight: tensor([[ 5.5730e-05, -3.4475e-04, -1.7941e-04,  ..., -2.8729e-04,\n",
      "          8.1635e-04,  2.2030e-04],\n",
      "        [ 1.1892e-03, -3.0446e-04,  3.8528e-04,  ..., -2.3031e-04,\n",
      "          1.7958e-03, -4.5490e-04],\n",
      "        [ 3.8195e-04,  2.5034e-04,  3.3665e-04,  ...,  1.3661e-04,\n",
      "         -4.5776e-05,  3.6573e-04],\n",
      "        ...,\n",
      "        [ 1.0538e-03,  4.3917e-04, -1.4992e-03,  ..., -4.2725e-04,\n",
      "         -1.0366e-03, -8.1015e-04],\n",
      "        [ 1.3790e-03, -1.9445e-03, -2.2244e-04,  ..., -1.4029e-03,\n",
      "          9.9277e-04, -4.3464e-04],\n",
      "        [ 3.1090e-04, -1.4372e-03, -2.0809e-03,  ..., -1.1196e-03,\n",
      "          1.6582e-04,  5.0449e-04]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isnan: False\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isinf: False\n",
      "Gradient of model.layers.0.mlp.down_proj.weight: tensor([[ 1.3151e-03,  2.3437e-04, -1.2946e-04,  ...,  1.4286e-03,\n",
      "         -2.4605e-03, -1.1101e-03],\n",
      "        [ 4.1032e-04,  1.3371e-03,  4.5300e-06,  ..., -3.7432e-04,\n",
      "         -1.8120e-04, -4.5276e-04],\n",
      "        [ 9.3174e-04,  2.3842e-07,  3.8028e-04,  ..., -7.6580e-04,\n",
      "         -1.0042e-03,  7.5293e-04],\n",
      "        ...,\n",
      "        [ 1.0335e-04, -5.6505e-05,  4.3559e-04,  ...,  4.7541e-04,\n",
      "          7.0810e-04,  1.9825e-04],\n",
      "        [ 1.3790e-03, -6.3562e-04,  9.5785e-05,  ...,  1.0043e-04,\n",
      "         -1.6584e-03, -3.9041e-05],\n",
      "        [ 4.1509e-04,  4.3392e-04, -4.5753e-04,  ...,  4.2725e-04,\n",
      "          6.8951e-04, -3.4928e-04]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isnan: False\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isinf: False\n",
      "Gradient of model.layers.0.input_layernorm.weight: tensor([-1.5616e-04, -2.8157e-04, -3.6597e-04,  8.0347e-05,  2.6631e-04,\n",
      "        -4.8804e-04,  1.9479e-04,  1.0312e-05, -6.5029e-05,  1.0639e-04,\n",
      "         3.8481e-04,  1.7023e-04, -1.2505e-04,  7.3135e-05, -1.1230e-04,\n",
      "         4.7636e-04,  3.6240e-04, -3.1531e-05, -2.6774e-04, -1.8632e-04,\n",
      "         2.0146e-04,  1.0031e-04, -3.2485e-05, -1.1742e-04, -9.4712e-05,\n",
      "         1.0687e-04,  2.2900e-04,  1.2243e-04, -1.2517e-06, -5.1856e-06,\n",
      "        -3.3498e-05, -1.8442e-04,  5.5265e-04,  3.2854e-04,  1.4150e-04,\n",
      "         1.0529e-03, -1.4544e-04,  3.6216e-04, -1.1045e-04, -1.1563e-05,\n",
      "         1.7214e-04, -2.0325e-05, -2.2316e-04,  1.5855e-04,  1.6153e-05,\n",
      "         1.8477e-06, -2.7180e-05,  2.2209e-04, -8.8990e-05, -3.6860e-04,\n",
      "         1.6606e-04, -1.7464e-04, -4.6372e-05, -1.4532e-04,  1.8156e-04,\n",
      "        -3.5620e-04,  8.9526e-05, -2.5582e-04, -3.7789e-05,  1.5986e-04,\n",
      "         1.0401e-04,  1.5152e-04,  5.8949e-05, -2.5868e-05,  1.2165e-04,\n",
      "        -4.7505e-05,  8.6665e-05,  1.8322e-04, -9.1434e-05, -3.3212e-04,\n",
      "         2.1935e-05,  2.2209e-04, -5.0366e-05,  5.8234e-05, -1.3804e-04,\n",
      "         3.2735e-04,  1.0645e-04,  1.0014e-04, -2.4021e-04, -4.2725e-04,\n",
      "         1.3530e-04,  3.1710e-05, -7.2622e-04,  5.2273e-05,  1.1605e-04,\n",
      "         7.2300e-05, -1.1098e-04,  1.3485e-03,  2.9826e-04, -1.8096e-04,\n",
      "        -2.8086e-04, -2.5773e-04, -2.9325e-04, -4.5896e-06,  9.3997e-05,\n",
      "         1.9932e-04, -1.8001e-04,  3.0327e-04,  3.1590e-04,  4.8041e-05,\n",
      "         4.5896e-05,  2.3663e-04, -5.9652e-04,  9.9480e-05, -1.7798e-04,\n",
      "         3.8028e-05, -3.6359e-05,  1.8954e-04, -7.7486e-05,  1.2755e-05,\n",
      "         2.3043e-04,  1.5378e-04, -1.5235e-04, -1.4079e-04, -1.3828e-05,\n",
      "         9.2149e-05, -7.9632e-05, -2.2280e-04, -1.3840e-04, -3.7134e-05,\n",
      "         1.5891e-04,  7.8022e-05,  1.0580e-04, -1.5664e-04,  3.0327e-04,\n",
      "         1.0866e-04,  7.5340e-04, -2.5296e-04], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.input_layernorm.weight              Gradient isnan: False\n",
      "model.layers.0.input_layernorm.weight              Gradient isinf: False\n",
      "Gradient of model.layers.0.post_attention_layernorm.weight: tensor([ 1.9598e-04, -1.1182e-04, -1.6224e-04,  1.9729e-04, -1.8537e-04,\n",
      "         6.5374e-04,  3.3951e-04, -3.0136e-04, -2.5845e-04, -8.4221e-05,\n",
      "         1.4770e-04, -1.1152e-04,  1.4305e-05, -3.7003e-04,  4.8542e-04,\n",
      "        -2.1231e-04,  5.0259e-04,  4.1485e-05, -2.2745e-04, -1.4412e-04,\n",
      "         2.5511e-04,  1.3864e-04,  3.9983e-04,  1.1706e-04,  2.2280e-04,\n",
      "         1.6379e-04,  2.0766e-04,  1.9729e-04, -2.8372e-04, -2.3592e-04,\n",
      "        -1.4186e-04,  3.6073e-04, -3.1567e-04,  9.6023e-05, -1.1623e-04,\n",
      "         1.9658e-04,  2.3639e-04,  1.1504e-05, -2.9743e-05, -2.1124e-04,\n",
      "         2.7919e-04, -8.7440e-05,  4.7970e-04,  2.2340e-04, -8.1003e-05,\n",
      "         9.2983e-05, -7.0095e-05, -3.4213e-05, -1.4126e-05, -2.7680e-04,\n",
      "        -3.0518e-04,  3.3426e-04, -1.5962e-04, -3.7837e-04,  1.1235e-04,\n",
      "        -4.0936e-04, -1.1420e-04, -6.1560e-04,  2.8539e-04, -3.1066e-04,\n",
      "         1.5616e-05, -6.0916e-05,  2.8157e-04,  4.7040e-04,  5.3644e-06,\n",
      "        -4.2796e-05, -1.9300e-04,  1.7500e-04,  1.9777e-04, -2.3210e-04,\n",
      "         4.0793e-04, -5.3585e-05, -1.9729e-04, -4.1580e-04, -8.2672e-05,\n",
      "        -5.6028e-05, -3.2377e-04,  9.5725e-05, -1.0288e-04,  3.3021e-05,\n",
      "        -2.5535e-04, -2.3043e-04, -1.0705e-04, -3.6526e-04,  8.3685e-05,\n",
      "        -2.7847e-04,  2.3127e-05,  1.9777e-04, -1.1796e-04,  1.1891e-04,\n",
      "        -1.4424e-04,  4.0603e-04,  2.6822e-06, -8.0884e-05,  6.8963e-05,\n",
      "        -6.4015e-05,  3.9518e-05, -1.7393e-04, -7.8738e-05, -9.6619e-05,\n",
      "        -6.8426e-05, -2.1994e-04, -1.5903e-04, -2.0349e-04, -9.9182e-05,\n",
      "         2.1970e-04, -6.0177e-04, -1.5295e-04,  2.7776e-04, -7.1645e-05,\n",
      "         5.3263e-04, -4.2534e-04, -3.4332e-05, -1.1784e-04,  3.8123e-04,\n",
      "         9.1434e-05, -9.3579e-06, -2.2650e-04,  2.0862e-04,  4.2677e-05,\n",
      "         5.4121e-04, -6.1083e-04, -1.1706e-04, -2.9624e-05, -6.3324e-04,\n",
      "        -3.7432e-05, -1.6081e-04,  1.7154e-04], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isnan: False\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isinf: False\n",
      "Gradient of model.norm.weight: tensor([-9.8884e-05,  7.5579e-05, -4.1556e-04,  2.0218e-04,  1.3008e-03,\n",
      "         5.7268e-04, -2.8992e-04,  5.6076e-04,  7.8297e-04, -1.1034e-03,\n",
      "        -5.2035e-05, -2.3782e-04,  1.9038e-04, -8.3923e-05,  2.2948e-04,\n",
      "         3.5048e-04,  1.8203e-04,  2.5535e-04,  3.8528e-04, -3.2163e-04,\n",
      "        -1.1730e-04,  6.8426e-04,  8.7857e-05, -3.6192e-04,  6.7282e-04,\n",
      "         7.0393e-05,  3.1292e-05,  3.2210e-04,  3.3307e-04,  3.1519e-04,\n",
      "         4.9472e-05,  2.4533e-04,  4.8590e-04,  2.1493e-04, -3.7849e-05,\n",
      "         1.2836e-03,  6.7091e-04,  4.6706e-04,  5.7840e-04, -8.6403e-04,\n",
      "        -9.9897e-05, -2.0719e-04,  6.8665e-04,  1.1307e-04,  5.9557e-04,\n",
      "        -6.5708e-04, -7.2193e-04, -4.9114e-05, -5.1403e-04,  2.4259e-05,\n",
      "         2.3007e-04,  1.5807e-04,  1.0004e-03, -5.3120e-04,  3.4595e-04,\n",
      "        -1.5616e-04,  5.1069e-04, -2.1577e-05,  2.1458e-06,  7.3099e-04,\n",
      "         8.6927e-04,  1.4102e-04,  1.0576e-03,  6.7949e-06,  1.3275e-03,\n",
      "         5.3358e-04, -5.1117e-04, -6.0272e-04, -5.1689e-04,  4.1890e-04,\n",
      "         2.2304e-04,  5.8126e-04,  5.5254e-05, -2.5296e-04,  1.8418e-04,\n",
      "        -7.0047e-04,  1.6725e-04,  4.1127e-04, -2.0909e-04,  5.4216e-04,\n",
      "        -4.3130e-04, -1.3280e-04,  2.8074e-05,  2.7514e-04, -3.9220e-04,\n",
      "         2.1017e-04,  1.3375e-04, -7.1704e-05, -2.8181e-04,  3.8385e-04,\n",
      "         3.4690e-04,  6.6662e-04,  8.2850e-05, -2.4414e-04,  4.1151e-04,\n",
      "         1.6868e-04, -4.9233e-05,  1.8370e-04,  2.1191e-03, -6.0987e-04,\n",
      "         1.0109e-03,  4.9114e-04,  4.7779e-04, -8.1003e-05,  5.2404e-04,\n",
      "        -1.1272e-03,  4.0245e-04,  2.7227e-04, -5.1355e-04, -1.3208e-04,\n",
      "        -6.9284e-04, -3.4046e-04, -3.7050e-04, -9.4461e-04,  2.1160e-04,\n",
      "         2.3496e-04,  4.4370e-04, -2.3377e-04, -7.3910e-04, -1.6737e-04,\n",
      "         8.7023e-04, -4.1556e-04,  1.0643e-03, -3.3236e-04,  8.7881e-04,\n",
      "        -1.1766e-04,  1.2350e-04,  4.2319e-04], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.norm.weight                                  Gradient isnan: False\n",
      "model.norm.weight                                  Gradient isinf: False\n",
      "Gradient of lm_head.weight: tensor([[ 3.6955e-06, -5.9009e-06, -1.2517e-06,  ..., -2.9802e-06,\n",
      "          5.1260e-06, -2.5630e-06],\n",
      "        [ 3.2187e-06, -4.9472e-06, -1.8477e-06,  ..., -2.7418e-06,\n",
      "          2.9206e-06, -2.9206e-06],\n",
      "        [ 3.6955e-06, -5.7220e-06, -1.4901e-06,  ..., -3.3379e-06,\n",
      "          3.3379e-06, -2.0862e-06],\n",
      "        ...,\n",
      "        [ 3.2783e-06, -5.5432e-06, -1.1325e-06,  ..., -2.6226e-06,\n",
      "          3.5763e-06, -1.9073e-06],\n",
      "        [ 3.3379e-06, -5.0068e-06, -1.4901e-06,  ..., -2.9206e-06,\n",
      "          3.6955e-06, -2.5630e-06],\n",
      "        [ 3.9339e-06, -6.3181e-06, -1.4901e-06,  ..., -2.9206e-06,\n",
      "          4.5896e-06, -2.2650e-06]], device='cuda:0', dtype=torch.float16)\n",
      "lm_head.weight                                     Gradient isnan: False\n",
      "lm_head.weight                                     Gradient isinf: False\n",
      "==========================================================================================================\n",
      "Data of model.embed_tokens.weight: tensor([[-0.0051, -0.0168,  0.0175,  ...,  0.0017,  0.0170, -0.0182],\n",
      "        [ 0.0116,  0.0215,  0.0152,  ...,  0.0160,  0.0173, -0.0175],\n",
      "        [ 0.0082, -0.0185, -0.0068,  ..., -0.0319, -0.0147, -0.0295],\n",
      "        ...,\n",
      "        [ 0.0195,  0.0100,  0.0262,  ...,  0.0116,  0.0055, -0.0080],\n",
      "        [ 0.0067,  0.0152,  0.0105,  ..., -0.0163,  0.0122, -0.0228],\n",
      "        [-0.0116,  0.0016, -0.0054,  ...,  0.0080,  0.0439, -0.0361]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.embed_tokens.weight                          Gradient isnan: False\n",
      "model.embed_tokens.weight                          Gradient isinf: False\n",
      "Data of model.layers.0.self_attn.q_proj.weight: tensor([[-6.3753e-04, -1.1854e-03,  7.5836e-03,  ..., -3.5095e-02,\n",
      "          2.3651e-02,  1.8356e-02],\n",
      "        [-9.0332e-03,  1.5640e-02,  3.7193e-05,  ..., -9.0561e-03,\n",
      "         -2.8427e-02, -1.1108e-02],\n",
      "        [-2.1866e-02, -3.0575e-03, -1.2459e-02,  ...,  1.9867e-02,\n",
      "         -1.8936e-02, -3.5095e-02],\n",
      "        ...,\n",
      "        [ 1.1841e-02,  5.7602e-03,  5.1346e-03,  ...,  2.5177e-02,\n",
      "          1.8814e-02, -6.6757e-03],\n",
      "        [-3.3966e-02, -3.0167e-02,  4.2191e-03,  ...,  5.3902e-03,\n",
      "          2.2297e-03,  3.4485e-02],\n",
      "        [ 1.7441e-02,  3.3630e-02,  1.0429e-02,  ...,  3.6125e-03,\n",
      "          3.2257e-02, -2.3041e-03]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isinf: False\n",
      "Data of model.layers.0.self_attn.k_proj.weight: tensor([[-0.0237,  0.0165, -0.0075,  ...,  0.0512, -0.0005, -0.0193],\n",
      "        [-0.0101,  0.0185,  0.0125,  ...,  0.0027,  0.0247, -0.0046],\n",
      "        [-0.0125,  0.0217,  0.0361,  ..., -0.0195, -0.0127, -0.0146],\n",
      "        ...,\n",
      "        [ 0.0003, -0.0242,  0.0101,  ..., -0.0277, -0.0422, -0.0003],\n",
      "        [-0.0217, -0.0437,  0.0367,  ..., -0.0186, -0.0352, -0.0180],\n",
      "        [-0.0184,  0.0011, -0.0119,  ...,  0.0223,  0.0067,  0.0242]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isinf: False\n",
      "Data of model.layers.0.self_attn.v_proj.weight: tensor([[ 0.0148, -0.0153, -0.0344,  ...,  0.0211, -0.0098,  0.0165],\n",
      "        [-0.0012, -0.0019, -0.0104,  ..., -0.0005, -0.0009, -0.0101],\n",
      "        [-0.0168, -0.0240, -0.0630,  ...,  0.0207, -0.0046,  0.0152],\n",
      "        ...,\n",
      "        [-0.0211, -0.0171, -0.0310,  ..., -0.0206,  0.0159,  0.0169],\n",
      "        [-0.0184,  0.0178, -0.0069,  ..., -0.0211,  0.0233, -0.0242],\n",
      "        [ 0.0028,  0.0027,  0.0224,  ...,  0.0031, -0.0215, -0.0089]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isinf: False\n",
      "Data of model.layers.0.self_attn.o_proj.weight: tensor([[-0.0045, -0.0160, -0.0172,  ...,  0.0364,  0.0104, -0.0036],\n",
      "        [-0.0041, -0.0263, -0.0057,  ..., -0.0044,  0.0258, -0.0348],\n",
      "        [ 0.0051,  0.0351, -0.0017,  ..., -0.0070,  0.0134, -0.0125],\n",
      "        ...,\n",
      "        [-0.0113, -0.0232, -0.0047,  ..., -0.0242,  0.0088,  0.0042],\n",
      "        [-0.0271,  0.0076, -0.0055,  ..., -0.0082, -0.0024,  0.0218],\n",
      "        [ 0.0232, -0.0019,  0.0462,  ...,  0.0017,  0.0161,  0.0128]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isinf: False\n",
      "Data of model.layers.0.mlp.gate_proj.weight: tensor([[ 1.7262e-03, -1.6357e-02, -5.4626e-03,  ...,  2.1027e-02,\n",
      "          7.7438e-03,  2.8641e-02],\n",
      "        [ 6.7413e-05,  1.7654e-02, -1.8875e-02,  ...,  4.5563e-02,\n",
      "          7.0381e-03,  2.5436e-02],\n",
      "        [ 1.3557e-02, -1.0078e-02, -2.1393e-02,  ..., -1.7227e-02,\n",
      "         -1.2875e-03,  7.6485e-03],\n",
      "        ...,\n",
      "        [-2.0096e-02, -1.9180e-02,  1.9333e-02,  ..., -1.9394e-02,\n",
      "          4.6425e-03, -1.7883e-02],\n",
      "        [-9.8343e-03,  8.9722e-03,  9.6054e-03,  ..., -7.5493e-03,\n",
      "          2.2720e-02, -1.2276e-02],\n",
      "        [-2.0325e-02,  8.5602e-03, -2.5391e-02,  ...,  8.3389e-03,\n",
      "         -1.7914e-02,  1.7914e-02]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isnan: False\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isinf: False\n",
      "Data of model.layers.0.mlp.up_proj.weight: tensor([[-0.0160,  0.0198,  0.0196,  ...,  0.0264, -0.0192,  0.0106],\n",
      "        [-0.0006, -0.0131,  0.0031,  ...,  0.0079,  0.0367,  0.0051],\n",
      "        [ 0.0090,  0.0378, -0.0209,  ...,  0.0242, -0.0308,  0.0178],\n",
      "        ...,\n",
      "        [-0.0176, -0.0014, -0.0217,  ...,  0.0196,  0.0225, -0.0160],\n",
      "        [ 0.0167, -0.0376,  0.0200,  ..., -0.0124, -0.0168,  0.0418],\n",
      "        [-0.0087,  0.0025,  0.0019,  ...,  0.0214, -0.0015,  0.0083]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isnan: False\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isinf: False\n",
      "Data of model.layers.0.mlp.down_proj.weight: tensor([[ 0.0048,  0.0111, -0.0287,  ..., -0.0199,  0.0141, -0.0165],\n",
      "        [-0.0027,  0.0020,  0.0043,  ..., -0.0308, -0.0122,  0.0119],\n",
      "        [ 0.0187,  0.0005, -0.0349,  ..., -0.0012, -0.0111,  0.0216],\n",
      "        ...,\n",
      "        [ 0.0395, -0.0174, -0.0219,  ..., -0.0107, -0.0077, -0.0323],\n",
      "        [ 0.0063,  0.0029,  0.0168,  ...,  0.0041,  0.0058, -0.0025],\n",
      "        [-0.0091,  0.0139,  0.0078,  ..., -0.0115,  0.0122,  0.0029]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isnan: False\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isinf: False\n",
      "Data of model.layers.0.input_layernorm.weight: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.input_layernorm.weight              Gradient isnan: False\n",
      "model.layers.0.input_layernorm.weight              Gradient isinf: False\n",
      "Data of model.layers.0.post_attention_layernorm.weight: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isnan: False\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isinf: False\n",
      "Data of model.norm.weight: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0', dtype=torch.float16)\n",
      "model.norm.weight                                  Gradient isnan: False\n",
      "model.norm.weight                                  Gradient isinf: False\n",
      "Data of lm_head.weight: tensor([[ 0.0063, -0.0019,  0.0204,  ...,  0.0008,  0.0358, -0.0195],\n",
      "        [ 0.0053,  0.0071, -0.0259,  ...,  0.0351, -0.0576, -0.0518],\n",
      "        [ 0.0074, -0.0076,  0.0113,  ..., -0.0218, -0.0345,  0.0035],\n",
      "        ...,\n",
      "        [-0.0188, -0.0017, -0.0025,  ...,  0.0240, -0.0029,  0.0290],\n",
      "        [ 0.0034, -0.0046,  0.0001,  ...,  0.0004, -0.0192, -0.0113],\n",
      "        [ 0.0175, -0.0283,  0.0063,  ...,  0.0233,  0.0285,  0.0181]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "lm_head.weight                                     Gradient isnan: False\n",
      "lm_head.weight                                     Gradient isinf: False\n",
      "==========================================================================================================\n",
      "##########################################################################################################\n",
      "Gradient of model.embed_tokens.weight: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
      "model.embed_tokens.weight                          Gradient isnan: False\n",
      "model.embed_tokens.weight                          Gradient isinf: False\n",
      "Gradient of model.layers.0.self_attn.q_proj.weight: tensor([[ 5.0485e-05,  2.1935e-05,  1.1384e-05,  ..., -6.3181e-06,\n",
      "         -2.7597e-05,  3.5763e-06],\n",
      "        [ 1.9133e-05,  1.9729e-05,  3.9041e-05,  ...,  2.7418e-06,\n",
      "          9.0003e-05,  7.3135e-05],\n",
      "        [ 1.2279e-05,  1.3053e-05,  1.7226e-05,  ..., -2.2769e-05,\n",
      "         -5.0724e-05, -2.7418e-05],\n",
      "        ...,\n",
      "        [-1.5199e-05, -8.2850e-05,  2.5213e-05,  ...,  2.3842e-07,\n",
      "          1.4901e-06, -3.2306e-05],\n",
      "        [-1.0955e-04,  3.4034e-05, -5.8413e-05,  ..., -1.5914e-05,\n",
      "         -1.0639e-04,  3.6776e-05],\n",
      "        [-7.8201e-05, -1.0848e-05,  1.8775e-05,  ...,  2.5630e-06,\n",
      "         -5.5015e-05,  1.1921e-06]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isinf: False\n",
      "Gradient of model.layers.0.self_attn.k_proj.weight: tensor([[-5.5373e-05,  5.0306e-05, -8.6248e-05,  ...,  1.5378e-05,\n",
      "         -1.5378e-04, -6.7294e-05],\n",
      "        [-4.3809e-05,  4.8935e-05,  5.7817e-05,  ..., -5.5909e-05,\n",
      "          1.0777e-04,  4.1664e-05],\n",
      "        [-1.3709e-04,  1.3709e-05, -3.8683e-05,  ...,  6.6698e-05,\n",
      "         -2.0385e-05,  1.4269e-04],\n",
      "        ...,\n",
      "        [ 3.4273e-05, -5.8472e-05, -3.6776e-05,  ..., -3.6478e-05,\n",
      "          8.9943e-05,  1.3590e-05],\n",
      "        [ 1.0127e-04, -3.0339e-05, -1.4067e-05,  ...,  1.3328e-04,\n",
      "          4.5300e-06,  1.8859e-04],\n",
      "        [ 3.6538e-05, -1.0020e-04, -7.3314e-06,  ..., -1.1384e-05,\n",
      "          5.5552e-05, -2.7835e-05]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isinf: False\n",
      "Gradient of model.layers.0.self_attn.v_proj.weight: tensor([[-2.8591e-03,  2.9106e-03,  2.5883e-03,  ...,  2.3861e-03,\n",
      "         -3.7575e-03, -2.3174e-03],\n",
      "        [ 5.5456e-04, -4.2572e-03, -1.6987e-05,  ..., -4.2686e-03,\n",
      "          6.5231e-03,  3.9406e-03],\n",
      "        [-2.2812e-03,  2.4891e-03,  9.3794e-04,  ...,  4.2796e-04,\n",
      "         -2.1896e-03, -2.6932e-03],\n",
      "        ...,\n",
      "        [-4.0436e-03,  3.6888e-03,  1.9093e-03,  ...,  2.9488e-03,\n",
      "         -4.9286e-03, -3.4847e-03],\n",
      "        [-1.1501e-03, -2.5997e-03, -1.0214e-03,  ..., -4.8790e-03,\n",
      "          5.1918e-03,  2.8877e-03],\n",
      "        [ 1.2665e-03, -3.1223e-03, -1.9395e-04,  ..., -7.0496e-03,\n",
      "          6.2218e-03,  4.3755e-03]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isinf: False\n",
      "Gradient of model.layers.0.self_attn.o_proj.weight: tensor([[ 3.3689e-04,  7.2384e-04,  4.0388e-04,  ...,  1.7107e-04,\n",
      "          2.0266e-05,  3.0947e-04],\n",
      "        [-7.7724e-04, -3.0446e-04, -7.5150e-04,  ..., -1.4400e-04,\n",
      "          1.3399e-04, -4.0960e-04],\n",
      "        [ 2.6970e-03,  1.8561e-04, -4.1413e-04,  ...,  9.7656e-04,\n",
      "         -1.0500e-03, -3.7503e-04],\n",
      "        ...,\n",
      "        [-3.0518e-05,  7.1335e-04, -2.3091e-04,  ..., -3.3975e-06,\n",
      "          2.7776e-04, -4.0722e-04],\n",
      "        [ 9.4461e-04, -4.0746e-04, -2.2900e-04,  ...,  4.7565e-05,\n",
      "         -8.9169e-04, -8.3113e-04],\n",
      "        [ 3.1452e-03,  7.7772e-04, -1.5564e-03,  ...,  6.9952e-04,\n",
      "         -6.9046e-04,  8.2445e-04]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isnan: False\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isinf: False\n",
      "Gradient of model.layers.0.mlp.gate_proj.weight: tensor([[-8.4114e-04,  1.2290e-04, -3.5739e-04,  ..., -5.0783e-05,\n",
      "          1.4410e-03,  4.5323e-04],\n",
      "        [ 7.6175e-05, -1.2350e-03,  5.2357e-04,  ..., -2.2469e-03,\n",
      "          2.8076e-03,  3.1531e-05],\n",
      "        [-1.6630e-04, -4.0197e-04,  2.1601e-04,  ..., -7.4029e-05,\n",
      "         -3.3236e-04,  7.5519e-05],\n",
      "        ...,\n",
      "        [ 2.6321e-04,  7.5769e-04, -5.2118e-04,  ...,  3.6931e-04,\n",
      "         -4.3273e-04, -7.3957e-04],\n",
      "        [-1.0290e-03,  9.8324e-04, -1.3647e-03,  ...,  1.7195e-03,\n",
      "         -6.3086e-04,  9.5034e-04],\n",
      "        [-3.6073e-04, -8.3876e-04, -5.5075e-04,  ...,  7.1573e-04,\n",
      "          1.0929e-03, -1.7047e-04]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isnan: False\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isinf: False\n",
      "Gradient of model.layers.0.mlp.up_proj.weight: tensor([[ 5.5730e-05, -3.4475e-04, -1.7941e-04,  ..., -2.8729e-04,\n",
      "          8.1635e-04,  2.2030e-04],\n",
      "        [ 1.1892e-03, -3.0446e-04,  3.8528e-04,  ..., -2.3031e-04,\n",
      "          1.7958e-03, -4.5490e-04],\n",
      "        [ 3.8195e-04,  2.5034e-04,  3.3665e-04,  ...,  1.3661e-04,\n",
      "         -4.5776e-05,  3.6573e-04],\n",
      "        ...,\n",
      "        [ 1.0538e-03,  4.3917e-04, -1.4992e-03,  ..., -4.2725e-04,\n",
      "         -1.0366e-03, -8.1015e-04],\n",
      "        [ 1.3790e-03, -1.9445e-03, -2.2244e-04,  ..., -1.4029e-03,\n",
      "          9.9277e-04, -4.3464e-04],\n",
      "        [ 3.1090e-04, -1.4372e-03, -2.0809e-03,  ..., -1.1196e-03,\n",
      "          1.6582e-04,  5.0449e-04]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isnan: False\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isinf: False\n",
      "Gradient of model.layers.0.mlp.down_proj.weight: tensor([[ 1.3151e-03,  2.3437e-04, -1.2946e-04,  ...,  1.4286e-03,\n",
      "         -2.4605e-03, -1.1101e-03],\n",
      "        [ 4.1032e-04,  1.3371e-03,  4.5300e-06,  ..., -3.7432e-04,\n",
      "         -1.8120e-04, -4.5276e-04],\n",
      "        [ 9.3174e-04,  2.3842e-07,  3.8028e-04,  ..., -7.6580e-04,\n",
      "         -1.0042e-03,  7.5293e-04],\n",
      "        ...,\n",
      "        [ 1.0335e-04, -5.6505e-05,  4.3559e-04,  ...,  4.7541e-04,\n",
      "          7.0810e-04,  1.9825e-04],\n",
      "        [ 1.3790e-03, -6.3562e-04,  9.5785e-05,  ...,  1.0043e-04,\n",
      "         -1.6584e-03, -3.9041e-05],\n",
      "        [ 4.1509e-04,  4.3392e-04, -4.5753e-04,  ...,  4.2725e-04,\n",
      "          6.8951e-04, -3.4928e-04]], device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isnan: False\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isinf: False\n",
      "Gradient of model.layers.0.input_layernorm.weight: tensor([-1.5616e-04, -2.8157e-04, -3.6597e-04,  8.0347e-05,  2.6631e-04,\n",
      "        -4.8804e-04,  1.9479e-04,  1.0312e-05, -6.5029e-05,  1.0639e-04,\n",
      "         3.8481e-04,  1.7023e-04, -1.2505e-04,  7.3135e-05, -1.1230e-04,\n",
      "         4.7636e-04,  3.6240e-04, -3.1531e-05, -2.6774e-04, -1.8632e-04,\n",
      "         2.0146e-04,  1.0031e-04, -3.2485e-05, -1.1742e-04, -9.4712e-05,\n",
      "         1.0687e-04,  2.2900e-04,  1.2243e-04, -1.2517e-06, -5.1856e-06,\n",
      "        -3.3498e-05, -1.8442e-04,  5.5265e-04,  3.2854e-04,  1.4150e-04,\n",
      "         1.0529e-03, -1.4544e-04,  3.6216e-04, -1.1045e-04, -1.1563e-05,\n",
      "         1.7214e-04, -2.0325e-05, -2.2316e-04,  1.5855e-04,  1.6153e-05,\n",
      "         1.8477e-06, -2.7180e-05,  2.2209e-04, -8.8990e-05, -3.6860e-04,\n",
      "         1.6606e-04, -1.7464e-04, -4.6372e-05, -1.4532e-04,  1.8156e-04,\n",
      "        -3.5620e-04,  8.9526e-05, -2.5582e-04, -3.7789e-05,  1.5986e-04,\n",
      "         1.0401e-04,  1.5152e-04,  5.8949e-05, -2.5868e-05,  1.2165e-04,\n",
      "        -4.7505e-05,  8.6665e-05,  1.8322e-04, -9.1434e-05, -3.3212e-04,\n",
      "         2.1935e-05,  2.2209e-04, -5.0366e-05,  5.8234e-05, -1.3804e-04,\n",
      "         3.2735e-04,  1.0645e-04,  1.0014e-04, -2.4021e-04, -4.2725e-04,\n",
      "         1.3530e-04,  3.1710e-05, -7.2622e-04,  5.2273e-05,  1.1605e-04,\n",
      "         7.2300e-05, -1.1098e-04,  1.3485e-03,  2.9826e-04, -1.8096e-04,\n",
      "        -2.8086e-04, -2.5773e-04, -2.9325e-04, -4.5896e-06,  9.3997e-05,\n",
      "         1.9932e-04, -1.8001e-04,  3.0327e-04,  3.1590e-04,  4.8041e-05,\n",
      "         4.5896e-05,  2.3663e-04, -5.9652e-04,  9.9480e-05, -1.7798e-04,\n",
      "         3.8028e-05, -3.6359e-05,  1.8954e-04, -7.7486e-05,  1.2755e-05,\n",
      "         2.3043e-04,  1.5378e-04, -1.5235e-04, -1.4079e-04, -1.3828e-05,\n",
      "         9.2149e-05, -7.9632e-05, -2.2280e-04, -1.3840e-04, -3.7134e-05,\n",
      "         1.5891e-04,  7.8022e-05,  1.0580e-04, -1.5664e-04,  3.0327e-04,\n",
      "         1.0866e-04,  7.5340e-04, -2.5296e-04], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.input_layernorm.weight              Gradient isnan: False\n",
      "model.layers.0.input_layernorm.weight              Gradient isinf: False\n",
      "Gradient of model.layers.0.post_attention_layernorm.weight: tensor([ 1.9598e-04, -1.1182e-04, -1.6224e-04,  1.9729e-04, -1.8537e-04,\n",
      "         6.5374e-04,  3.3951e-04, -3.0136e-04, -2.5845e-04, -8.4221e-05,\n",
      "         1.4770e-04, -1.1152e-04,  1.4305e-05, -3.7003e-04,  4.8542e-04,\n",
      "        -2.1231e-04,  5.0259e-04,  4.1485e-05, -2.2745e-04, -1.4412e-04,\n",
      "         2.5511e-04,  1.3864e-04,  3.9983e-04,  1.1706e-04,  2.2280e-04,\n",
      "         1.6379e-04,  2.0766e-04,  1.9729e-04, -2.8372e-04, -2.3592e-04,\n",
      "        -1.4186e-04,  3.6073e-04, -3.1567e-04,  9.6023e-05, -1.1623e-04,\n",
      "         1.9658e-04,  2.3639e-04,  1.1504e-05, -2.9743e-05, -2.1124e-04,\n",
      "         2.7919e-04, -8.7440e-05,  4.7970e-04,  2.2340e-04, -8.1003e-05,\n",
      "         9.2983e-05, -7.0095e-05, -3.4213e-05, -1.4126e-05, -2.7680e-04,\n",
      "        -3.0518e-04,  3.3426e-04, -1.5962e-04, -3.7837e-04,  1.1235e-04,\n",
      "        -4.0936e-04, -1.1420e-04, -6.1560e-04,  2.8539e-04, -3.1066e-04,\n",
      "         1.5616e-05, -6.0916e-05,  2.8157e-04,  4.7040e-04,  5.3644e-06,\n",
      "        -4.2796e-05, -1.9300e-04,  1.7500e-04,  1.9777e-04, -2.3210e-04,\n",
      "         4.0793e-04, -5.3585e-05, -1.9729e-04, -4.1580e-04, -8.2672e-05,\n",
      "        -5.6028e-05, -3.2377e-04,  9.5725e-05, -1.0288e-04,  3.3021e-05,\n",
      "        -2.5535e-04, -2.3043e-04, -1.0705e-04, -3.6526e-04,  8.3685e-05,\n",
      "        -2.7847e-04,  2.3127e-05,  1.9777e-04, -1.1796e-04,  1.1891e-04,\n",
      "        -1.4424e-04,  4.0603e-04,  2.6822e-06, -8.0884e-05,  6.8963e-05,\n",
      "        -6.4015e-05,  3.9518e-05, -1.7393e-04, -7.8738e-05, -9.6619e-05,\n",
      "        -6.8426e-05, -2.1994e-04, -1.5903e-04, -2.0349e-04, -9.9182e-05,\n",
      "         2.1970e-04, -6.0177e-04, -1.5295e-04,  2.7776e-04, -7.1645e-05,\n",
      "         5.3263e-04, -4.2534e-04, -3.4332e-05, -1.1784e-04,  3.8123e-04,\n",
      "         9.1434e-05, -9.3579e-06, -2.2650e-04,  2.0862e-04,  4.2677e-05,\n",
      "         5.4121e-04, -6.1083e-04, -1.1706e-04, -2.9624e-05, -6.3324e-04,\n",
      "        -3.7432e-05, -1.6081e-04,  1.7154e-04], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isnan: False\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isinf: False\n",
      "Gradient of model.norm.weight: tensor([-9.8884e-05,  7.5579e-05, -4.1556e-04,  2.0218e-04,  1.3008e-03,\n",
      "         5.7268e-04, -2.8992e-04,  5.6076e-04,  7.8297e-04, -1.1034e-03,\n",
      "        -5.2035e-05, -2.3782e-04,  1.9038e-04, -8.3923e-05,  2.2948e-04,\n",
      "         3.5048e-04,  1.8203e-04,  2.5535e-04,  3.8528e-04, -3.2163e-04,\n",
      "        -1.1730e-04,  6.8426e-04,  8.7857e-05, -3.6192e-04,  6.7282e-04,\n",
      "         7.0393e-05,  3.1292e-05,  3.2210e-04,  3.3307e-04,  3.1519e-04,\n",
      "         4.9472e-05,  2.4533e-04,  4.8590e-04,  2.1493e-04, -3.7849e-05,\n",
      "         1.2836e-03,  6.7091e-04,  4.6706e-04,  5.7840e-04, -8.6403e-04,\n",
      "        -9.9897e-05, -2.0719e-04,  6.8665e-04,  1.1307e-04,  5.9557e-04,\n",
      "        -6.5708e-04, -7.2193e-04, -4.9114e-05, -5.1403e-04,  2.4259e-05,\n",
      "         2.3007e-04,  1.5807e-04,  1.0004e-03, -5.3120e-04,  3.4595e-04,\n",
      "        -1.5616e-04,  5.1069e-04, -2.1577e-05,  2.1458e-06,  7.3099e-04,\n",
      "         8.6927e-04,  1.4102e-04,  1.0576e-03,  6.7949e-06,  1.3275e-03,\n",
      "         5.3358e-04, -5.1117e-04, -6.0272e-04, -5.1689e-04,  4.1890e-04,\n",
      "         2.2304e-04,  5.8126e-04,  5.5254e-05, -2.5296e-04,  1.8418e-04,\n",
      "        -7.0047e-04,  1.6725e-04,  4.1127e-04, -2.0909e-04,  5.4216e-04,\n",
      "        -4.3130e-04, -1.3280e-04,  2.8074e-05,  2.7514e-04, -3.9220e-04,\n",
      "         2.1017e-04,  1.3375e-04, -7.1704e-05, -2.8181e-04,  3.8385e-04,\n",
      "         3.4690e-04,  6.6662e-04,  8.2850e-05, -2.4414e-04,  4.1151e-04,\n",
      "         1.6868e-04, -4.9233e-05,  1.8370e-04,  2.1191e-03, -6.0987e-04,\n",
      "         1.0109e-03,  4.9114e-04,  4.7779e-04, -8.1003e-05,  5.2404e-04,\n",
      "        -1.1272e-03,  4.0245e-04,  2.7227e-04, -5.1355e-04, -1.3208e-04,\n",
      "        -6.9284e-04, -3.4046e-04, -3.7050e-04, -9.4461e-04,  2.1160e-04,\n",
      "         2.3496e-04,  4.4370e-04, -2.3377e-04, -7.3910e-04, -1.6737e-04,\n",
      "         8.7023e-04, -4.1556e-04,  1.0643e-03, -3.3236e-04,  8.7881e-04,\n",
      "        -1.1766e-04,  1.2350e-04,  4.2319e-04], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.norm.weight                                  Gradient isnan: False\n",
      "model.norm.weight                                  Gradient isinf: False\n",
      "Gradient of lm_head.weight: tensor([[ 3.6955e-06, -5.9009e-06, -1.2517e-06,  ..., -2.9802e-06,\n",
      "          5.1260e-06, -2.5630e-06],\n",
      "        [ 3.2187e-06, -4.9472e-06, -1.8477e-06,  ..., -2.7418e-06,\n",
      "          2.9206e-06, -2.9206e-06],\n",
      "        [ 3.6955e-06, -5.7220e-06, -1.4901e-06,  ..., -3.3379e-06,\n",
      "          3.3379e-06, -2.0862e-06],\n",
      "        ...,\n",
      "        [ 3.2783e-06, -5.5432e-06, -1.1325e-06,  ..., -2.6226e-06,\n",
      "          3.5763e-06, -1.9073e-06],\n",
      "        [ 3.3379e-06, -5.0068e-06, -1.4901e-06,  ..., -2.9206e-06,\n",
      "          3.6955e-06, -2.5630e-06],\n",
      "        [ 3.9339e-06, -6.3181e-06, -1.4901e-06,  ..., -2.9206e-06,\n",
      "          4.5896e-06, -2.2650e-06]], device='cuda:0', dtype=torch.float16)\n",
      "lm_head.weight                                     Gradient isnan: False\n",
      "lm_head.weight                                     Gradient isinf: False\n",
      "==========================================================================================================\n",
      "Data of model.embed_tokens.weight: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.embed_tokens.weight                          Gradient isnan: True\n",
      "model.embed_tokens.weight                          Gradient isinf: True\n",
      "Data of model.layers.0.self_attn.q_proj.weight: tensor([[-inf, -inf, -inf,  ..., inf, inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., inf, inf, inf],\n",
      "        ...,\n",
      "        [inf, inf, -inf,  ..., nan, -inf, inf],\n",
      "        [inf, -inf, inf,  ..., inf, inf, -inf],\n",
      "        [inf, inf, -inf,  ..., -inf, inf, -inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isnan: True\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isinf: True\n",
      "Data of model.layers.0.self_attn.k_proj.weight: tensor([[inf, -inf, inf,  ..., -inf, inf, inf],\n",
      "        [inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
      "        [inf, -inf, inf,  ..., -inf, inf, -inf],\n",
      "        ...,\n",
      "        [-inf, inf, inf,  ..., inf, -inf, -inf],\n",
      "        [-inf, inf, inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isnan: True\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isinf: True\n",
      "Data of model.layers.0.self_attn.v_proj.weight: tensor([[    inf,    -inf,    -inf,  ...,    -inf,     inf,     inf],\n",
      "        [   -inf,     inf,     inf,  ...,     inf, -0.0010,    -inf],\n",
      "        [    inf,    -inf,    -inf,  ...,    -inf,     inf,     inf],\n",
      "        ...,\n",
      "        [    inf,    -inf,    -inf,  ...,    -inf,     inf,     inf],\n",
      "        [    inf,     inf,     inf,  ...,     inf,    -inf,    -inf],\n",
      "        [   -inf,     inf,     inf,  ...,  0.0031, -0.0216,    -inf]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isnan: True\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isinf: True\n",
      "Data of model.layers.0.self_attn.o_proj.weight: tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, -inf, inf,  ..., -inf, inf, inf],\n",
      "        ...,\n",
      "        [inf, -inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, inf, inf,  ..., -inf, inf, inf],\n",
      "        [-inf, -inf, inf,  ..., -inf, inf, -inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isnan: True\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isinf: True\n",
      "Data of model.layers.0.mlp.gate_proj.weight: tensor([[inf, -inf, inf,  ..., inf, -inf, -inf],\n",
      "        [-inf, inf, -inf,  ..., inf, -inf, -inf],\n",
      "        [inf, inf, -inf,  ..., inf, inf, -inf],\n",
      "        ...,\n",
      "        [-inf, -inf, inf,  ..., -inf, inf, inf],\n",
      "        [inf, -inf, inf,  ..., -inf, inf, -inf],\n",
      "        [inf, inf, inf,  ..., -inf, -inf, inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isnan: True\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isinf: True\n",
      "Data of model.layers.0.mlp.up_proj.weight: tensor([[-inf, inf, inf,  ..., inf, -inf, -inf],\n",
      "        [-inf, inf, -inf,  ..., inf, -inf, inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, inf, -inf],\n",
      "        ...,\n",
      "        [-inf, -inf, inf,  ..., inf, inf, inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, -inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isnan: True\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isinf: True\n",
      "Data of model.layers.0.mlp.down_proj.weight: tensor([[-inf, -inf, inf,  ..., -inf, inf, inf],\n",
      "        [-inf, -inf, -inf,  ..., inf, inf, inf],\n",
      "        [-inf, nan, -inf,  ..., inf, inf, -inf],\n",
      "        ...,\n",
      "        [-inf, inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, inf, -inf,  ..., -inf, inf, inf],\n",
      "        [-inf, -inf, inf,  ..., -inf, -inf, inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isnan: True\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isinf: True\n",
      "Data of model.layers.0.input_layernorm.weight: tensor([inf, inf, inf, -inf, -inf, inf, -inf, -inf, inf, -inf, -inf, -inf, inf, -inf, inf, -inf, -inf, inf, inf, inf, -inf, -inf, inf, inf,\n",
      "        inf, -inf, -inf, -inf, inf, inf, inf, inf, -inf, -inf, -inf, -inf, inf, -inf, inf, inf, -inf, inf, inf, -inf, -inf, -inf, inf, -inf,\n",
      "        inf, inf, -inf, inf, inf, inf, -inf, inf, -inf, inf, inf, -inf, -inf, -inf, -inf, inf, -inf, inf, -inf, -inf, inf, inf, -inf, -inf,\n",
      "        inf, -inf, inf, -inf, -inf, -inf, inf, inf, -inf, -inf, inf, -inf, -inf, -inf, inf, -inf, -inf, inf, inf, inf, inf, inf, -inf, -inf,\n",
      "        inf, -inf, -inf, -inf, -inf, -inf, inf, -inf, inf, -inf, inf, -inf, inf, -inf, -inf, -inf, inf, inf, inf, -inf, inf, inf, inf, inf,\n",
      "        -inf, -inf, -inf, inf, -inf, -inf, -inf, inf], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.input_layernorm.weight              Gradient isnan: False\n",
      "model.layers.0.input_layernorm.weight              Gradient isinf: True\n",
      "Data of model.layers.0.post_attention_layernorm.weight: tensor([-inf, inf, inf, -inf, inf, -inf, -inf, inf, inf, inf, -inf, inf, -inf, inf, -inf, inf, -inf, -inf, inf, inf, -inf, -inf, -inf, -inf,\n",
      "        -inf, -inf, -inf, -inf, inf, inf, inf, -inf, inf, -inf, inf, -inf, -inf, -inf, inf, inf, -inf, inf, -inf, -inf, inf, -inf, inf, inf,\n",
      "        inf, inf, inf, -inf, inf, inf, -inf, inf, inf, inf, -inf, inf, -inf, inf, -inf, -inf, -inf, inf, inf, -inf, -inf, inf, -inf, inf,\n",
      "        inf, inf, inf, inf, inf, -inf, inf, -inf, inf, inf, inf, inf, -inf, inf, -inf, -inf, inf, -inf, inf, -inf, -inf, inf, -inf, inf,\n",
      "        -inf, inf, inf, inf, inf, inf, inf, inf, inf, -inf, inf, inf, -inf, inf, -inf, inf, inf, inf, -inf, -inf, inf, inf, -inf, -inf,\n",
      "        -inf, inf, inf, inf, inf, inf, inf, -inf], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isnan: False\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isinf: True\n",
      "Data of model.norm.weight: tensor([inf, -inf, inf, -inf, -inf, -inf, inf, -inf, -inf, inf, inf, inf, -inf, inf, -inf, -inf, -inf, -inf, -inf, inf, inf, -inf, -inf, inf,\n",
      "        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, -inf, -inf, -inf, -inf, inf, inf, inf, -inf, -inf, -inf, inf, inf, inf,\n",
      "        inf, -inf, -inf, -inf, -inf, inf, -inf, inf, -inf, inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, -inf, -inf, -inf,\n",
      "        -inf, inf, -inf, inf, -inf, -inf, inf, -inf, inf, inf, -inf, -inf, inf, -inf, -inf, inf, inf, -inf, -inf, -inf, -inf, inf, -inf, -inf,\n",
      "        inf, -inf, -inf, inf, -inf, -inf, -inf, inf, -inf, inf, -inf, -inf, inf, inf, inf, inf, inf, inf, -inf, -inf, -inf, inf, inf, inf,\n",
      "        -inf, inf, -inf, inf, -inf, inf, -inf, -inf], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.norm.weight                                  Gradient isnan: False\n",
      "model.norm.weight                                  Gradient isinf: True\n",
      "Data of lm_head.weight: tensor([[-inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        ...,\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "lm_head.weight                                     Gradient isnan: True\n",
      "lm_head.weight                                     Gradient isinf: True\n",
      "==========================================================================================================\n",
      "##########################################################################################################\n",
      "Gradient of model.embed_tokens.weight: None\n",
      "Gradient of model.layers.0.self_attn.q_proj.weight: None\n",
      "Gradient of model.layers.0.self_attn.k_proj.weight: None\n",
      "Gradient of model.layers.0.self_attn.v_proj.weight: None\n",
      "Gradient of model.layers.0.self_attn.o_proj.weight: None\n",
      "Gradient of model.layers.0.mlp.gate_proj.weight: None\n",
      "Gradient of model.layers.0.mlp.up_proj.weight: None\n",
      "Gradient of model.layers.0.mlp.down_proj.weight: None\n",
      "Gradient of model.layers.0.input_layernorm.weight: None\n",
      "Gradient of model.layers.0.post_attention_layernorm.weight: None\n",
      "Gradient of model.norm.weight: None\n",
      "Gradient of lm_head.weight: None\n",
      "==========================================================================================================\n",
      "Data of model.embed_tokens.weight: tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.embed_tokens.weight                          Gradient isnan: True\n",
      "model.embed_tokens.weight                          Gradient isinf: True\n",
      "Data of model.layers.0.self_attn.q_proj.weight: tensor([[-inf, -inf, -inf,  ..., inf, inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, -inf, -inf,  ..., inf, inf, inf],\n",
      "        ...,\n",
      "        [inf, inf, -inf,  ..., nan, -inf, inf],\n",
      "        [inf, -inf, inf,  ..., inf, inf, -inf],\n",
      "        [inf, inf, -inf,  ..., -inf, inf, -inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isnan: True\n",
      "model.layers.0.self_attn.q_proj.weight             Gradient isinf: True\n",
      "Data of model.layers.0.self_attn.k_proj.weight: tensor([[inf, -inf, inf,  ..., -inf, inf, inf],\n",
      "        [inf, -inf, -inf,  ..., inf, -inf, -inf],\n",
      "        [inf, -inf, inf,  ..., -inf, inf, -inf],\n",
      "        ...,\n",
      "        [-inf, inf, inf,  ..., inf, -inf, -inf],\n",
      "        [-inf, inf, inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isnan: True\n",
      "model.layers.0.self_attn.k_proj.weight             Gradient isinf: True\n",
      "Data of model.layers.0.self_attn.v_proj.weight: tensor([[    inf,    -inf,    -inf,  ...,    -inf,     inf,     inf],\n",
      "        [   -inf,     inf,     inf,  ...,     inf, -0.0010,    -inf],\n",
      "        [    inf,    -inf,    -inf,  ...,    -inf,     inf,     inf],\n",
      "        ...,\n",
      "        [    inf,    -inf,    -inf,  ...,    -inf,     inf,     inf],\n",
      "        [    inf,     inf,     inf,  ...,     inf,    -inf,    -inf],\n",
      "        [   -inf,     inf,     inf,  ...,  0.0031, -0.0216,    -inf]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isnan: True\n",
      "model.layers.0.self_attn.v_proj.weight             Gradient isinf: True\n",
      "Data of model.layers.0.self_attn.o_proj.weight: tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, -inf, inf,  ..., -inf, inf, inf],\n",
      "        ...,\n",
      "        [inf, -inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, inf, inf,  ..., -inf, inf, inf],\n",
      "        [-inf, -inf, inf,  ..., -inf, inf, -inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isnan: True\n",
      "model.layers.0.self_attn.o_proj.weight             Gradient isinf: True\n",
      "Data of model.layers.0.mlp.gate_proj.weight: tensor([[inf, -inf, inf,  ..., inf, -inf, -inf],\n",
      "        [-inf, inf, -inf,  ..., inf, -inf, -inf],\n",
      "        [inf, inf, -inf,  ..., inf, inf, -inf],\n",
      "        ...,\n",
      "        [-inf, -inf, inf,  ..., -inf, inf, inf],\n",
      "        [inf, -inf, inf,  ..., -inf, inf, -inf],\n",
      "        [inf, inf, inf,  ..., -inf, -inf, inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isnan: True\n",
      "model.layers.0.mlp.gate_proj.weight                Gradient isinf: True\n",
      "Data of model.layers.0.mlp.up_proj.weight: tensor([[-inf, inf, inf,  ..., inf, -inf, -inf],\n",
      "        [-inf, inf, -inf,  ..., inf, -inf, inf],\n",
      "        [-inf, -inf, -inf,  ..., -inf, inf, -inf],\n",
      "        ...,\n",
      "        [-inf, -inf, inf,  ..., inf, inf, inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, -inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isnan: True\n",
      "model.layers.0.mlp.up_proj.weight                  Gradient isinf: True\n",
      "Data of model.layers.0.mlp.down_proj.weight: tensor([[-inf, -inf, inf,  ..., -inf, inf, inf],\n",
      "        [-inf, -inf, -inf,  ..., inf, inf, inf],\n",
      "        [-inf, nan, -inf,  ..., inf, inf, -inf],\n",
      "        ...,\n",
      "        [-inf, inf, -inf,  ..., -inf, -inf, -inf],\n",
      "        [-inf, inf, -inf,  ..., -inf, inf, inf],\n",
      "        [-inf, -inf, inf,  ..., -inf, -inf, inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isnan: True\n",
      "model.layers.0.mlp.down_proj.weight                Gradient isinf: True\n",
      "Data of model.layers.0.input_layernorm.weight: tensor([inf, inf, inf, -inf, -inf, inf, -inf, -inf, inf, -inf, -inf, -inf, inf, -inf, inf, -inf, -inf, inf, inf, inf, -inf, -inf, inf, inf,\n",
      "        inf, -inf, -inf, -inf, inf, inf, inf, inf, -inf, -inf, -inf, -inf, inf, -inf, inf, inf, -inf, inf, inf, -inf, -inf, -inf, inf, -inf,\n",
      "        inf, inf, -inf, inf, inf, inf, -inf, inf, -inf, inf, inf, -inf, -inf, -inf, -inf, inf, -inf, inf, -inf, -inf, inf, inf, -inf, -inf,\n",
      "        inf, -inf, inf, -inf, -inf, -inf, inf, inf, -inf, -inf, inf, -inf, -inf, -inf, inf, -inf, -inf, inf, inf, inf, inf, inf, -inf, -inf,\n",
      "        inf, -inf, -inf, -inf, -inf, -inf, inf, -inf, inf, -inf, inf, -inf, inf, -inf, -inf, -inf, inf, inf, inf, -inf, inf, inf, inf, inf,\n",
      "        -inf, -inf, -inf, inf, -inf, -inf, -inf, inf], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.input_layernorm.weight              Gradient isnan: False\n",
      "model.layers.0.input_layernorm.weight              Gradient isinf: True\n",
      "Data of model.layers.0.post_attention_layernorm.weight: tensor([-inf, inf, inf, -inf, inf, -inf, -inf, inf, inf, inf, -inf, inf, -inf, inf, -inf, inf, -inf, -inf, inf, inf, -inf, -inf, -inf, -inf,\n",
      "        -inf, -inf, -inf, -inf, inf, inf, inf, -inf, inf, -inf, inf, -inf, -inf, -inf, inf, inf, -inf, inf, -inf, -inf, inf, -inf, inf, inf,\n",
      "        inf, inf, inf, -inf, inf, inf, -inf, inf, inf, inf, -inf, inf, -inf, inf, -inf, -inf, -inf, inf, inf, -inf, -inf, inf, -inf, inf,\n",
      "        inf, inf, inf, inf, inf, -inf, inf, -inf, inf, inf, inf, inf, -inf, inf, -inf, -inf, inf, -inf, inf, -inf, -inf, inf, -inf, inf,\n",
      "        -inf, inf, inf, inf, inf, inf, inf, inf, inf, -inf, inf, inf, -inf, inf, -inf, inf, inf, inf, -inf, -inf, inf, inf, -inf, -inf,\n",
      "        -inf, inf, inf, inf, inf, inf, inf, -inf], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isnan: False\n",
      "model.layers.0.post_attention_layernorm.weight     Gradient isinf: True\n",
      "Data of model.norm.weight: tensor([inf, -inf, inf, -inf, -inf, -inf, inf, -inf, -inf, inf, inf, inf, -inf, inf, -inf, -inf, -inf, -inf, -inf, inf, inf, -inf, -inf, inf,\n",
      "        -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, -inf, -inf, -inf, -inf, inf, inf, inf, -inf, -inf, -inf, inf, inf, inf,\n",
      "        inf, -inf, -inf, -inf, -inf, inf, -inf, inf, -inf, inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, inf, inf, inf, -inf, -inf, -inf,\n",
      "        -inf, inf, -inf, inf, -inf, -inf, inf, -inf, inf, inf, -inf, -inf, inf, -inf, -inf, inf, inf, -inf, -inf, -inf, -inf, inf, -inf, -inf,\n",
      "        inf, -inf, -inf, inf, -inf, -inf, -inf, inf, -inf, inf, -inf, -inf, inf, inf, inf, inf, inf, inf, -inf, -inf, -inf, inf, inf, inf,\n",
      "        -inf, inf, -inf, inf, -inf, inf, -inf, -inf], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "model.norm.weight                                  Gradient isnan: False\n",
      "model.norm.weight                                  Gradient isinf: True\n",
      "Data of lm_head.weight: tensor([[-inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        ...,\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf],\n",
      "        [-inf, inf, inf,  ..., inf, -inf, inf]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "lm_head.weight                                     Gradient isnan: True\n",
      "lm_head.weight                                     Gradient isinf: True\n",
      "==========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "gpu_usage()\n",
    "from torch.cuda import amp\n",
    "epoch = 1\n",
    "\n",
    "model.train()\n",
    "if args.with_tracking:\n",
    "    total_loss = 0\n",
    "if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n",
    "    # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n",
    "    active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n",
    "else:\n",
    "    active_dataloader = train_dataloader\n",
    "\n",
    "losses = []\n",
    "for step, batch in enumerate(active_dataloader):\n",
    "    with accelerator.accumulate(model):\n",
    "        # outputs = model(input_ids=input_ids.to(torch.float16))\n",
    "        # batch = {k: v.to(torch.float16) for k, v in batch.items()}\n",
    "        with amp.autocast():\n",
    "            outputs = model(**batch)\n",
    "        logits = outputs.logits.to(torch.float16)\n",
    "        print(\"=======================================================================\")\n",
    "        print(f'logits type: {logits.dtype}')\n",
    "        print(\"-----------------------------------------------------------------------\")\n",
    "\n",
    "        print(f'Loss: {outputs.loss}')\n",
    "        print(\"-----------------------------------------------------------------------\")\n",
    "        print(f'logits: {outputs.logits}')\n",
    "        print(\"-----------------------------------------------------------------------\")\n",
    "        print(f'past_key_values: {outputs.past_key_values}')\n",
    "        print(\"-----------------------------------------------------------------------\")\n",
    "        print(f'logits shape: {outputs.logits.shape}')  \n",
    "        print(\"=======================================================================\")\n",
    "        print(f'past_key_values length: {len(outputs.past_key_values)}')\n",
    "        for j in range(len(outputs.past_key_values)):\n",
    "            print(f'{\" \"*5}-----------------------------------------------------------------------')\n",
    "            print(f'{\" \"*5}past_key_values[{j}] length (layers): {len(outputs.past_key_values[j])}')\n",
    "            for i in range(len(outputs.past_key_values[j])):\n",
    "                print(f'{\" \"*10}past_key_values[{j}][{i}].shape: {outputs.past_key_values[j][i].shape}')\n",
    "\n",
    "        # check if any nan of inf values in the logits or past_key_values using torch.isnan() and torch.isinf()\n",
    "        if torch.isnan(outputs.logits).any() or torch.isinf(outputs.logits).any():\n",
    "            print(\"Found nan or inf values in logits\")\n",
    "        else:\n",
    "            print(\"No nan or inf values in logits\")\n",
    "        \n",
    "\n",
    "        detect_nan(outputs.logits, \"Logits\")\n",
    "\n",
    "        # print the float type of the model\n",
    "        print(f\"Model type: {model.dtype}\")\n",
    "        # print the float type of the outputs\n",
    "        print(f\"Outputs type: {outputs.loss.dtype}\")\n",
    "        print(f'Outputs logits {outputs.logits.dtype}')\n",
    "        print(f'Outputs past_key_values {outputs.past_key_values[0][0].dtype}')\n",
    "        print(f'Outputs loss {outputs.loss.dtype}')\n",
    "\n",
    "        print(\"##########################################################################################################\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"Gradient of {name}: {param.grad}\")\n",
    "                if param.grad is not None:\n",
    "                    detect_nan(param.grad, name)\n",
    "        print(\"==========================================================================================================\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"Data of {name}: {param.data}\")\n",
    "                if param.data is not None:\n",
    "                    detect_nan(param.data, name)\n",
    "        print(\"==========================================================================================================\")\n",
    "        input(\"Enter to continue: ...\")\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.to(torch.float16)\n",
    "        losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_train_batch_size)))\n",
    "\n",
    "        # We keep track of the loss at each epoch\n",
    "        if args.with_tracking:\n",
    "            total_loss += loss.detach().float()\n",
    "\n",
    "        # computes the gradients of the loss with respect to the model parameters\n",
    "        accelerator.backward(loss) #  The accelerator.backward method is used instead of the usual loss.backward() to potentially handle distributed training specifics.\n",
    "        # loss.backward() #  The accelerator.backward method is used instead of the usual loss.backward() to potentially handle distributed training specifics.\n",
    "        print(\"##########################################################################################################\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"Gradient of {name}: {param.grad}\")\n",
    "                if param.grad is not None:\n",
    "                    detect_nan(param.grad, name)\n",
    "        print(\"==========================================================================================================\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"Data of {name}: {param.data}\")\n",
    "                if param.data is not None:\n",
    "                    detect_nan(param.data, name)\n",
    "        print(\"==========================================================================================================\")\n",
    "        input(\"Enter to continue: ...\")\n",
    "\n",
    "        #_______________________________________________________________________________________________________________\n",
    "        # -------------------------------------------------------------- adding grading clipping -----------------------\n",
    "        max_grad_norm = 0.5\n",
    "\n",
    "        grad_norm = accelerator.clip_grad_norm_(\n",
    "            model.parameters(),\n",
    "            max_grad_norm,\n",
    "        )\n",
    "\n",
    "        print(f\"Gradient norm: {grad_norm}\")\n",
    "        \n",
    "        print(\"##########################################################################################################\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"Gradient of {name}: {param.grad}\")\n",
    "                if param.grad is not None:\n",
    "                    detect_nan(param.grad, name)\n",
    "        print(\"==========================================================================================================\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"Data of {name}: {param.data}\")\n",
    "                if param.data is not None:\n",
    "                    detect_nan(param.data, name)\n",
    "        print(\"==========================================================================================================\")\n",
    "        input(\"Enter to continue: ...\")\n",
    "\n",
    "        #_______________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "        # updates the model parameters based on the computed gradients.\n",
    "        optimizer.step()\n",
    "        print(\"##########################################################################################################\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"Gradient of {name}: {param.grad}\")\n",
    "                if param.grad is not None:\n",
    "                    detect_nan(param.grad, name)\n",
    "        print(\"==========================================================================================================\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"Data of {name}: {param.data}\")\n",
    "                if param.data is not None:\n",
    "                    detect_nan(param.data, name)\n",
    "        print(\"==========================================================================================================\")\n",
    "        input(\"Enter to continue: ...\")\n",
    "\n",
    "        # updates the learning rate according to the scheduler's policy\n",
    "        lr_scheduler.step()\n",
    "        # clears the old gradients to prevent them from accumulating with the gradients of the next batch.\n",
    "        optimizer.zero_grad()\n",
    "        print(\"##########################################################################################################\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"Gradient of {name}: {param.grad}\")\n",
    "                if param.grad is not None:\n",
    "                    detect_nan(param.grad, name)\n",
    "        print(\"==========================================================================================================\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"Data of {name}: {param.data}\")\n",
    "                if param.data is not None:\n",
    "                    detect_nan(param.data, name)\n",
    "        print(\"==========================================================================================================\")\n",
    "        input(\"Enter to continue: ...\")\n",
    "        break\n",
    "    # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "    if accelerator.sync_gradients:\n",
    "        progress_bar.update(1)\n",
    "        completed_steps += 1\n",
    "\n",
    "    if isinstance(checkpointing_steps, int):\n",
    "        if completed_steps % checkpointing_steps == 0:\n",
    "            output_dir = f\"step_{completed_steps}\"\n",
    "            if args.output_dir is not None:\n",
    "                output_dir = os.path.join(args.output_dir, output_dir)\n",
    "            accelerator.save_state(output_dir)\n",
    "    if completed_steps >= args.max_train_steps:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "losses = torch.cat(losses)\n",
    "try:\n",
    "    train_loss = torch.mean(losses)\n",
    "    perplexity = math.exp(train_loss)\n",
    "except OverflowError:\n",
    "    perplexity = float(\"inf\")\n",
    "logger.info(f\"epoch {epoch}: perplexity: {perplexity} train_loss: {train_loss}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "losses = []\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))\n",
    "\n",
    "losses = torch.cat(losses)\n",
    "try:\n",
    "    eval_loss = torch.mean(losses)\n",
    "    perplexity = math.exp(eval_loss)\n",
    "except OverflowError:\n",
    "    perplexity = float(\"inf\")\n",
    "\n",
    "logger.info(f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\")\n",
    "\n",
    "if args.with_tracking:\n",
    "    accelerator.log(\n",
    "        {\n",
    "            \"perplexity\": perplexity,\n",
    "            \"eval_loss\": eval_loss,\n",
    "            \"train_loss\": total_loss.item() / len(train_dataloader),\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": completed_steps,\n",
    "        },\n",
    "        step=completed_steps,\n",
    "    )\n",
    "\n",
    "if args.push_to_hub and epoch < args.num_train_epochs - 1:\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "        api.upload_folder(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\",\n",
    "            folder_path=args.output_dir,\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"model\",\n",
    "            token=args.hub_token,\n",
    "        )\n",
    "\n",
    "if args.checkpointing_steps == \"epoch\":\n",
    "    output_dir = f\"epoch_{epoch}\"\n",
    "    if args.output_dir is not None:\n",
    "        output_dir = os.path.join(args.output_dir, output_dir)\n",
    "    accelerator.save_state(output_dir)\n",
    "\n",
    "\n",
    "if args.with_tracking:\n",
    "    accelerator.end_training()\n",
    "\n",
    "if args.output_dir is not None:\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "        if args.push_to_hub:\n",
    "            api.upload_folder(\n",
    "                commit_message=\"End of training\",\n",
    "                folder_path=args.output_dir,\n",
    "                repo_id=repo_id,\n",
    "                repo_type=\"model\",\n",
    "                token=args.hub_token,\n",
    "            )\n",
    "        with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n",
    "            json.dump({\"perplexity\": perplexity}, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mist3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
